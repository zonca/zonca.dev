<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Your Name">
<meta name="dcterms.date" content="2025-09-18">

<title>Deploying a Larger LLM on Jetstream’s g3.xl Instance – Andrea Zonca</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ffaf9999eff50501d3d250dab41715b7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Andrea Zonca</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../consult.html"> 
<span class="menu-text">Consulting</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zonca"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/andreazonca"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mastodon.online/@zonca"> <i class="bi bi-mastodon" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deploying a Larger LLM on Jetstream’s g3.xl Instance</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">cloud</div>
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">Jetstream</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Your Name </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 18, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div data-md-component="skip">
<p><a href="#deploy-a-chatgptlike-llm-service-on-jetstream" class="md-skip">Skip to content</a></p>
</div>
<div data-md-component="announce">

</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../.." class="md-header__button md-logo" title="Jetstream2 Documentation"><img src="../../images/jetstream2-logo-white.svg" class="img-fluid figure-img" alt="logo"></a></p>
<figcaption>logo</figcaption>
</figure>
</div>
<div class="md-header__title" data-md-component="header-title">
<div class="{.md-header__ellipsis&quot;}">
<div class="{.md-header__topic&quot;}">
<p><span class="md-ellipsis">Jetstream2 Documentation</span></p>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<p><span class="md-ellipsis">Deploying a Larger LLM on Jetstream’s g3.xl Instance</span></p>
</div>
</div>
</div>
<div class="md-search" data-md-component="search" role="dialog">
<div class="md-search__inner" role="search">
<div class="md-search__suggest" data-md-component="search-suggest">

</div>
</div>
<div class="{.md-search__output&quot;}">
<div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="{.md-search-result__meta&quot;}">
<p>Initializing search</p>
</div>
</div>
</div>
</div>
</div>
<p>:::</p>
<div class="{.md-header__source&quot;}">
<p><a href="https://gitlab.com/jetstream-cloud/docs" class="md-source" title="Go to repository"></a></p>
<p>::: {.md-source__icon .md-icon”}</p>
</div>
<div class="{.md-source__repository&quot;}">
<p>jetstream-cloud/docs</p>
</div>
<p>:::</p>
<div class="md-container" data-md-component="container">
<div class="md-main" role="main" data-md-component="main">
<p>::: {.md-main__inner .md-grid”} ::: {.md-sidebar .md-sidebar–primary md-component=“sidebar” md-type=“navigation”} ::: {.md-sidebar__scrollwrap”} ::: {.md-sidebar__inner”} <a href="../.." class="md-nav__button md-logo" title="Jetstream2 Documentation"><img src="../../images/jetstream2-logo-white.svg" class="img-fluid" alt="logo"></a> Jetstream2 Documentation</p>
<div class="{.md-nav__source&quot;}">
<p><a href="https://gitlab.com/jetstream-cloud/docs" class="md-source" title="Go to repository"></a></p>
<p>::: {.md-source__icon .md-icon”}</p>
</div>
<div class="{.md-source__repository&quot;}">
<p>jetstream-cloud/docs</p>
</div>
</div>
<ul>
<li><span class="md-ellipsis">Home</span></li>
<li><span class="md-ellipsis">Status</span></li>
<li><span class="md-ellipsis">Support and News</span></li>
<li><span class="md-ellipsis">Getting Started</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Getting Started
<ul>
<li><a href="../../getting-started/overview/" class="md-nav__link"><span class="md-ellipsis">Overview</span></a></li>
<li><a href="../../getting-started/login/" class="md-nav__link"><span class="md-ellipsis">Logging in to Jetstream2</span></a></li>
<li><a href="../../getting-started/first-instance/" class="md-nav__link"><span class="md-ellipsis">Creating your First Instance</span></a></li>
<li><a href="../../getting-started/access-instance/" class="md-nav__link"><span class="md-ellipsis">Accessing your Instance</span></a></li>
<li><a href="../../getting-started/volumes/" class="md-nav__link"><span class="md-ellipsis">Volume Management</span></a></li>
<li><a href="../../getting-started/software/" class="md-nav__link"><span class="md-ellipsis">Installing and Running Software</span></a></li>
<li><a href="../../getting-started/instance-management/" class="md-nav__link"><span class="md-ellipsis">Instance Management</span></a></li>
<li><a href="../../getting-started/snapshots/" class="md-nav__link"><span class="md-ellipsis">Snapshots and Images</span></a></li>
<li><a href="../../getting-started/next-steps/" class="md-nav__link"><span class="md-ellipsis">Next Steps</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Jetstream2 Info</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Jetstream2 Info
<ul>
<li><a href="../../overview/overview-doc/" class="md-nav__link"><span class="md-ellipsis">System Overview</span></a></li>
<li><a href="../../overview/architecture/" class="md-nav__link"><span class="md-ellipsis">Architecture and Capabilities of Jetstream2</span></a></li>
<li><a href="../../overview/config/" class="md-nav__link"><span class="md-ellipsis">Configuration and specifications</span></a></li>
<li><a href="../../overview/network/" class="md-nav__link"><span class="md-ellipsis">Network configuration and considerations</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Frequently Asked Questions</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Frequently Asked Questions
<ul>
<li><a href="../../faq/general-faq/" class="md-nav__link"><span class="md-ellipsis">General FAQs</span></a></li>
<li><a href="../../faq/trouble/" class="md-nav__link"><span class="md-ellipsis">Troubleshooting</span></a></li>
<li><a href="../../faq/alloc/" class="md-nav__link"><span class="md-ellipsis">Allocations</span></a></li>
<li><a href="../../faq/gpu/" class="md-nav__link"><span class="md-ellipsis">GPU FAQs</span></a></li>
<li><a href="../../faq/security/" class="md-nav__link"><span class="md-ellipsis">Security</span></a></li>
<li><a href="../../faq/software/" class="md-nav__link"><span class="md-ellipsis">Software</span></a></li>
<li><a href="../../faq/gateways/" class="md-nav__link"><span class="md-ellipsis">Gateways</span></a></li>
</ul></li>
<li><span class="md-ellipsis">General Usage Information</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> General Usage Information
<ul>
<li><a href="../resources/" class="md-nav__link"><span class="md-ellipsis">Jetstream2 Resources</span></a></li>
<li><a href="../access/" class="md-nav__link"><span class="md-ellipsis">ACCESS Credits and Jetstream2</span></a></li>
<li><a href="../instance-flavors/" class="md-nav__link"><span class="md-ellipsis">Instance Flavors</span></a></li>
<li><a href="../quotas/" class="md-nav__link"><span class="md-ellipsis">Quotas</span></a></li>
<li><a href="../instancemgt/" class="md-nav__link"><span class="md-ellipsis">Instance Management Actions</span></a></li>
<li><a href="../featured/" class="md-nav__link"><span class="md-ellipsis">Featured Images</span></a></li>
<li><a href="../windows/" class="md-nav__link"><span class="md-ellipsis">Microsoft Windows on Jetstream2</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Allocations</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Allocations
<ul>
<li><a href="../../alloc/overview/" class="md-nav__link"><span class="md-ellipsis">Allocations Overview</span></a></li>
<li><a href="../../alloc/trial/" class="md-nav__link"><span class="md-ellipsis">Trial Allocation</span></a></li>
<li><a href="../../alloc/general-allocations/" class="md-nav__link"><span class="md-ellipsis">General Allocations</span></a></li>
<li><a href="../../alloc/education/" class="md-nav__link"><span class="md-ellipsis">Education Allocations</span></a></li>
<li><a href="../../alloc/research/" class="md-nav__link"><span class="md-ellipsis">Research (Maximize ACCESS) Allocations</span></a></li>
<li><a href="../../alloc/supplement/" class="md-nav__link"><span class="md-ellipsis">Supplements (Storage/SUs)</span></a></li>
<li><a href="../../alloc/renew-extend/" class="md-nav__link"><span class="md-ellipsis">Extensions &amp; Renewals</span></a></li>
<li><a href="../../alloc/faq/" class="md-nav__link"><span class="md-ellipsis">Frequently Asked Questions</span></a></li>
<li><a href="../../alloc/budgeting/" class="md-nav__link"><span class="md-ellipsis">Budgeting for Common Usage Scenarios</span></a></li>
<li><a href="../../alloc/estimator/" class="md-nav__link"><span class="md-ellipsis">Usage Estimation Calculator</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Storage</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Storage
<ul>
<li><a href="../storage/" class="md-nav__link"><span class="md-ellipsis">Storage Overview</span></a></li>
<li><a href="../volume/" class="md-nav__link"><span class="md-ellipsis">Volumes</span></a></li>
<li><a href="../manila/" class="md-nav__link"><span class="md-ellipsis">Manila Shares</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../object/" class="md-nav__link"><span class="md-ellipsis">Object Store</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../filetransfer/" class="md-nav__link"><span class="md-ellipsis">File Transfer</span></a></li>
</ul></li>
<li><span class="md-ellipsis">User Interfaces</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> User Interfaces
<ul>
<li><a href="../../ui/" class="md-nav__link"><span class="md-ellipsis">Overview</span></a></li>
<li><a href="../../ui/exo/exo/" class="md-nav__link"><span class="md-ellipsis">Exosphere</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../../ui/horizon/intro/" class="md-nav__link"><span class="md-ellipsis">Horizon</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../../ui/cli/overview/" class="md-nav__link"><span class="md-ellipsis">CLI</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../../ui/cacao/overview/" class="md-nav__link"><span class="md-ellipsis">CACAO</span> <span class="md-nav__icon md-icon"></span></a></li>
</ul></li>
<li><span class="md-ellipsis">General VM Operations</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> General VM Operations
<ul>
<li><a href="../firewalls/" class="md-nav__link"><span class="md-ellipsis">Security</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../adduser/" class="md-nav__link"><span class="md-ellipsis">Maintenance and Administration</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../jupyter/" class="md-nav__link"><span class="md-ellipsis">Research Software</span> <span class="md-nav__icon md-icon"></span></a></li>
</ul></li>
<li><span class="md-ellipsis">Software Collection</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Software Collection
<ul>
<li><a href="../software/" class="md-nav__link"><span class="md-ellipsis">Jetstream2 Software Collection</span></a></li>
<li><a href="../usingsoftware-cli/" class="md-nav__link"><span class="md-ellipsis">Using the JS2 Software Collection - Command Line</span></a></li>
<li><a href="../usingsoftware-desktop/" class="md-nav__link"><span class="md-ellipsis">Using the JS2 Software Collection - Web Desktop</span></a></li>
<li><a href="../licenses/" class="md-nav__link"><span class="md-ellipsis">Software Licenses</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Policies and Best Practices</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Policies and Best Practices
<ul>
<li><a href="../policies/" class="md-nav__link"><span class="md-ellipsis">Acceptable Usage Policies</span></a></li>
<li><a href="../export/" class="md-nav__link"><span class="md-ellipsis">Export Control Guidance</span></a></li>
<li><a href="../gateways/" class="md-nav__link"><span class="md-ellipsis">AUPs for Jetstream2 Hosted Gateways</span></a></li>
<li><a href="../bestpractice/" class="md-nav__link"><span class="md-ellipsis">Best Practices</span></a></li>
</ul></li>
<li><span class="md-ellipsis">Special Topics</span> <span class="md-nav__icon md-icon"></span> <span class="md-nav__icon md-icon"></span> Special Topics
<ul>
<li><a href="../docker/" class="md-nav__link"><span class="md-ellipsis">Containers on Jetstream2</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../terraform/" class="md-nav__link"><span class="md-ellipsis">Terraform</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../virtualclusters/" class="md-nav__link"><span class="md-ellipsis">Virtual Clusters on Jetstream2</span></a></li>
<li><a href="../federating/" class="md-nav__link"><span class="md-ellipsis">Federating Gateways on Jetstream2</span></a></li>
<li><a href="../octavia/" class="md-nav__link"><span class="md-ellipsis">Load Balancing with OpenStack Octavia</span></a></li>
<li><a href="../../inference-service/overview/" class="md-nav__link"><span class="md-ellipsis">LLM Inference Service</span> <span class="md-nav__icon md-icon"></span></a></li>
<li><a href="../running-llm/" class="md-nav__link"><span class="md-ellipsis">Orientation to Running LLMs</span></a></li>
<li><span class="md-ellipsis">LLM and Chat Interface Deployment Tutorial</span> <span class="md-nav__icon md-icon"></span> <a href="./" class="md-nav__link md-nav__link--active"><span class="md-ellipsis">LLM and Chat Interface Deployment Tutorial</span></a> <span class="md-nav__icon md-icon"></span> Table of contents
<ul>
<li><a href="#model-choice-sizing" class="md-nav__link"><span class="md-ellipsis">Model choice &amp; sizing</span></a></li>
<li><a href="#create-a-jetstream-instance" class="md-nav__link"><span class="md-ellipsis">Create a Jetstream instance</span></a></li>
<li><a href="#load-miniforge" class="md-nav__link"><span class="md-ellipsis">Load Miniforge</span></a></li>
<li><a href="#serve-the-model-with-llamacpp-openaicompatible-server" class="md-nav__link"><span class="md-ellipsis">Serve the model with llama.cpp (OpenAI‑compatible server)</span></a></li>
<li><a href="#configure-the-chat-interface" class="md-nav__link"><span class="md-ellipsis">Configure the chat interface</span></a>
<ul>
<li><a href="#optional-one-liner-to-create-both-services" class="md-nav__link"><span class="md-ellipsis">(Optional) One-liner to create both services</span></a></li>
</ul></li>
<li><a href="#configure-web-server-for-https" class="md-nav__link"><span class="md-ellipsis">Configure web server for HTTPS</span></a></li>
<li><a href="#connect-the-model-and-test-the-chat-interface" class="md-nav__link"><span class="md-ellipsis">Connect the model and test the chat interface</span></a></li>
<li><a href="#scaling-up-or-changing-models" class="md-nav__link"><span class="md-ellipsis">Scaling up or changing models</span></a></li>
</ul></li>
</ul></li>
</ul>
</div>
<p>::: :::</p>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="{.md-sidebar__scrollwrap&quot;}">
<div class="{.md-sidebar__inner&quot;}">
<p><span class="md-nav__icon md-icon"></span> Table of contents</p>
<ul>
<li><a href="#model-choice-sizing" class="md-nav__link"><span class="md-ellipsis">Model choice &amp; sizing</span></a></li>
<li><a href="#create-a-jetstream-instance" class="md-nav__link"><span class="md-ellipsis">Create a Jetstream instance</span></a></li>
<li><a href="#load-miniforge" class="md-nav__link"><span class="md-ellipsis">Load Miniforge</span></a></li>
<li><a href="#serve-the-model-with-llamacpp-openaicompatible-server" class="md-nav__link"><span class="md-ellipsis">Serve the model with llama.cpp (OpenAI‑compatible server)</span></a></li>
<li><a href="#configure-the-chat-interface" class="md-nav__link"><span class="md-ellipsis">Configure the chat interface</span></a>
<ul>
<li><a href="#optional-one-liner-to-create-both-services" class="md-nav__link"><span class="md-ellipsis">(Optional) One-liner to create both services</span></a></li>
</ul></li>
<li><a href="#configure-web-server-for-https" class="md-nav__link"><span class="md-ellipsis">Configure web server for HTTPS</span></a></li>
<li><a href="#connect-the-model-and-test-the-chat-interface" class="md-nav__link"><span class="md-ellipsis">Connect the model and test the chat interface</span></a></li>
<li><a href="#scaling-up-or-changing-models" class="md-nav__link"><span class="md-ellipsis">Scaling up or changing models</span></a></li>
</ul>
</div>
</div>
</div>
<section id="deploying-a-larger-llm-on-jetstreams-g3.xl-instance" class="level1 md-content" data-md-component="content">
<h1>Deploying a Larger LLM on Jetstream’s g3.xl Instance<a href="#deploy-a-chatgptlike-llm-service-on-jetstream" class="headerlink" title="Permanent link">¶</a></h1>
<p>Tutorial last updated on September 18, 2025</p>
<p>In this tutorial we deploy a Large Language Model (LLM) on Jetstream, run inference locally on a powerful GPU node (<code>g3.xl</code>, 40 GB VRAM), then install a web chat interface (Open WebUI) and serve it with HTTPS using Caddy.</p>
<p>Before spinning up your own GPU, consider the managed <a href="https://docs.jetstream-cloud.org/inference-service/overview/">Jetstream LLM inference service</a>. It may be more cost- and time-effective if you just need API access to standard models.</p>
<p>We will deploy a larger quantized model: <strong>Meta Llama 3.1 70B Instruct Q3_K_L (GGUF)</strong>. This quantized 70B model requires approximately 35-40 GB of GPU memory, making it suitable for the <code>g3.xl</code> instance (40 GB VRAM). Note that this is a tight fit, and you might need to adjust <code>n_ctx</code> or consider a slightly lower quantization if you encounter out-of-memory errors.</p>
<p>This tutorial is adapted from work by <a href="https://www2.kek.jp/qup/en/member/dehaan.html">Tijmen de Haan</a>, the author of <a href="https://cosmosage.online/">Cosmosage</a>.</p>
<section id="model-choice-sizing" class="level2">
<h2 class="anchored" data-anchor-id="model-choice-sizing">Model choice &amp; sizing<a href="#model-choice-sizing" class="headerlink" title="Permanent link">¶</a></h2>
<p>Jetstream GPU flavors (current key options):</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Instance Type</th>
<th style="text-align: left;">Approx. GPU Memory (GB)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">g3.xl</td>
<td style="text-align: left;">40 (full A100)</td>
</tr>
</tbody>
</table>
<p>We pick the quantized <strong>Llama 3.1 70B Instruct Q3_K_L</strong> variant (GGUF format). Its VRAM residency during inference is about ~35-40 GB with default context settings, leaving very little margin on <code>g3.xl</code>. Always keep a couple of GB free to avoid OOM errors when increasing context length or concurrency.</p>
<p>Ensure the model is an Instruct fine-tuned variant (it is) so it responds well to chat prompts.</p>
</section>
<section id="create-a-jetstream-instance" class="level2">
<h2 class="anchored" data-anchor-id="create-a-jetstream-instance">Create a Jetstream instance<a href="#create-a-jetstream-instance" class="headerlink" title="Permanent link">¶</a></h2>
<p>Log in to Exosphere, request an Ubuntu 24 <strong><code>g3.xl</code></strong> instance (name it <code>chat</code>) and SSH into it using either your SSH key or the passphrase generated by Exosphere.</p>
</section>
<section id="load-miniforge" class="level2">
<h2 class="anchored" data-anchor-id="load-miniforge">Load Miniforge<a href="#load-miniforge" class="headerlink" title="Permanent link">¶</a></h2>
<p>A centrally provided Miniforge module is available on Jetstream images. Load it (each new shell) and then create the two Conda environments used below (one for the model server, one for the web UI).</p>
<div class="highlight">
<pre><code>module load miniforge
conda init</code></pre>
</div>
<blockquote class="blockquote">
<p>After running <code>conda init</code>, reload your shell so <code>conda</code> is available: run <code>exec bash -l</code> (avoids logging out and back in).</p>
</blockquote>
</section>
<section id="serve-the-model-with-llama.cpp-openai-compatible-server" class="level2">
<h2 class="anchored" data-anchor-id="serve-the-model-with-llama.cpp-openai-compatible-server">Serve the model with <code>llama.cpp</code> (OpenAI-compatible server)<a href="#serve-the-model-with-llamacpp-openaicompatible-server" class="headerlink" title="Permanent link">¶</a></h2>
<p>We use <code>llama.cpp</code> via the <code>llama-cpp-python</code> package, which provides an OpenAI-style HTTP API (default port 8000) that Open WebUI can connect to.</p>
<p>Create an environment and install (remember to <code>module load miniforge</code> first in any new shell).</p>
<p>The last <code>pip install</code> step may take several minutes to compile lama.cpp from source, so please be patient.</p>
<div class="highlight">
<pre><code>conda create -y -n llama python=3.11
conda activate llama
conda install -y cmake ninja scikit-build-core huggingface_hub
module load nvhpc/24.7/nvhpc
# Enable CUDA acceleration with explicit compilers, arch, release build
CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_COMPILER=$(which nvcc) -DCMAKE_C_COMPILER=$(which gcc) -DCMAKE_CXX_COMPILER=$(which g++) -DCMAKE_CUDA_ARCHITECTURES=80 -DCMAKE_BUILD_TYPE=Release" \
    pip install --no-cache-dir --no-build-isolation --force-reinstall "llama-cpp-python[server]==0.3.16"</code></pre>
</div>
<p>Download the quantized GGUF file (Q4_K_M variant) from Hugging Face: <code>aiqtech/Meta-Llama-3-70B-Instruct-Q4_K_M-GGUF</code></p>
<div class="highlight">
<pre><code>mkdir -p ~/models
huggingface-cli download bartowski/Meta-Llama-3.1-70B-Instruct-GGUF \
    Meta-Llama-3.1-70B-Instruct-Q3_K_L.gguf \
    --local-dir ~/models \
    --local-dir-use-symlinks False</code></pre>
</div>
<p>Test run (Ctrl-C to stop):</p>
<div class="highlight">
<pre><code>python -m llama_cpp.server \
    --model /home/exouser/models/Meta-Llama-3.1-70B-Instruct-Q3_K_L.gguf \
    --host 0.0.0.0 --port 8080 \
    --slots 2 \
    --n-gpu-layers -1 \
    --ctx-size 4096 \
    --batch-size 64 \
    --cache-type-k q4_0 --cache-type-v q4_0</code></pre>
</div>
<p><code>--n_gpu_layers -1</code> tells <code>llama.cpp</code> to offload <strong>all model layers</strong> to the GPU (full GPU inference). Without this flag the default is CPU layers (<code>n_gpu_layers=0</code>), which results in significantly slower generation. Full offload of this 70B Q3_K_L model plus context buffers should occupy roughly 35-40 GB VRAM at <code>--ctx-size 4096</code> on first real requests. Given the <code>g3.xl</code> has 40 GB VRAM, this is a tight fit. If it fails to start with an out-of-memory (OOM) error you have a few mitigation options (apply one, then retry):</p>
<ul>
<li>Lower context length: e.g.&nbsp;<code>--ctx-size 2048</code> (largest single lever; roughly linear VRAM impact for KV cache).</li>
<li>Partially offload: replace <code>--n_gpu_layers -1</code> with a number (e.g. <code>--n_gpu_layers 20</code>). Remaining layers will run on CPU (slower, but reduces VRAM need).</li>
<li>Use a lower-bit quantization (e.g.&nbsp;Q2_K) or a smaller model.</li>
</ul>
<p>You can inspect VRAM usage with <code>watch -n 2 nvidia-smi</code> after starting the server.</p>
<p>Quick note on the “KV cache”: During generation the model reuses previously computed attention Key and Value tensors (instead of recalculating them each new token). These tensors are stored per layer and per processed token; as your prompt and conversation grow, the cache grows linearly with the number of tokens kept in context. That’s why idle VRAM (~weights only) is lower (~35 GB) and rises toward the higher number (up to ~40 GB here) only after longer prompts / chats. Reducing <code>--ctx-size</code> caps the maximum KV cache size; clearing history or restarting frees it.</p>
<p>If it starts without errors, create a systemd service so it restarts automatically.</p>
<blockquote class="blockquote">
<p>Quick option: If you prefer a single copy/paste that creates <strong>both</strong> the <code>llama</code> and <code>open-webui</code> systemd services at once, skip the next two manual unit file sections and jump ahead to the subsection titled “(Optional) One-liner to create both services” below. You can always come back here for the longer, step-by-step version and troubleshooting notes.</p>
</blockquote>
<p>Using <code>sudo</code> to run your preferred text editor, create <code>/etc/systemd/system/llama.service</code> with the following contents:</p>
<div class="highlight">
<pre><code>[Unit]
Description=Llama.cpp OpenAI-compatible server
After=network.target

[Service]
User=exouser
Group=exouser
WorkingDirectory=/home/exouser
ExecStart=/bin/bash -lc "module load nvhpc/24.7/nvhpc miniforge &amp;&amp; conda run -n llama python -m llama_cpp.server --model /home/exouser/models/Meta-Llama-3.1-70B-Instruct-Q3_K_L.gguf --host 0.0.0.0 --port 8080 --slots 2 --n-gpu-layers -1 --ctx-size 4096 --batch-size 64 --cache-type-k q4_0 --cache-type-v q4_0"
Restart=always

[Install]
WantedBy=multi-user.target</code></pre>
</div>
<p>Enable and start:</p>
<div class="highlight">
<pre><code>sudo systemctl enable llama
sudo systemctl start llama</code></pre>
</div>
<p>Troubleshooting:</p>
<ul>
<li>Logs: <code>sudo journalctl -u llama -f</code></li>
<li>Status: <code>sudo systemctl status llama</code></li>
<li>GPU usage: <code>nvidia-smi</code> (~35-40 GB idle right after start with full offload; can grow toward ~40 GB under long prompts/conversations as KV cache fills, potentially leading to OOM if not managed).</li>
</ul>
</section>
<section id="configure-the-chat-interface" class="level2">
<h2 class="anchored" data-anchor-id="configure-the-chat-interface">Configure the chat interface<a href="#configure-the-chat-interface" class="headerlink" title="Permanent link">¶</a></h2>
<p>The chat interface is provided by <a href="https://openwebui.com/">Open Web UI</a>.</p>
<p>Create the environment (in a new shell remember to <code>module load miniforge</code> first):</p>
<div class="highlight">
<pre><code>module load miniforge
conda create -y -n open-webui python=3.11
conda activate open-webui
pip install open-webui
open-webui serve</code></pre>
</div>
<p>If this starts with no error, we can kill it with <code>Ctrl-C</code> and create a service for it.</p>
<p>Using <code>sudo</code> to run your preferred text editor, create <code>/etc/systemd/system/webui.service</code> with the following contents:</p>
<div class="highlight">
<pre><code>[Unit]
Description=Open Web UI serving
After=network.target

[Service]
User=exouser
Group=exouser
WorkingDirectory=/home/exouser

# Activating the conda environment and starting the service
ExecStart=/bin/bash -lc "module load miniforge &amp;&amp; conda run -n open-webui open-webui serve"
Restart=always
# PATH managed by module + conda

[Install]
WantedBy=multi-user.target</code></pre>
</div>
<p>Then enable and start the service:</p>
<div class="highlight">
<pre><code>sudo systemctl enable webui
sudo systemctl start webui</code></pre>
</div>
<section id="optional-one-liner-to-create-both-services" class="level3">
<h3 class="anchored" data-anchor-id="optional-one-liner-to-create-both-services">(Optional) One-liner to create both services<a href="#optional-one-liner-to-create-both-services" class="headerlink" title="Permanent link">¶</a></h3>
<p>If you already created the Conda environments (<code>llama</code> and <code>open-webui</code>) and downloaded the model, you can create, enable, and start both systemd services (model server + Open WebUI) in a single copy/paste. Adjust <code>MODEL</code>, <code>N_CTX</code>, <code>USER</code>, and <code>NVHPC_MOD</code> if needed before running:</p>
<div class="highlight">
<pre><code>MODEL=/home/exouser/models/Meta-Llama-3.1-70B-Instruct-Q3_K_L.gguf N_CTX=4096 USER=exouser NVHPC_MOD=nvhpc/24.7/nvhpc ; sudo tee /etc/systemd/system/llama.service &gt;/dev/null &lt;&lt;EOF &amp;&amp; sudo tee /etc/systemd/system/webui.service &gt;/dev/null &lt;&lt;EOF2 &amp;&amp; sudo systemctl daemon-reload &amp;&amp; sudo systemctl enable --now llama webui
[Unit]
Description=Llama.cpp OpenAI-compatible server
After=network.target

[Service]
User=$USER
Group=$USER
WorkingDirectory=/home/$USER
ExecStart=/bin/bash -lc "module load $NVHPC_MOD miniforge &amp;&amp; conda run -n llama python -m llama_cpp.server --model $MODEL --host 0.0.0.0 --port 8080 --slots 2 --n-gpu-layers -1 --ctx-size $N_CTX --batch-size 64 --cache-type-k q4_0 --cache-type-v q4_0"
Restart=always

[Install]
WantedBy=multi-user.target
EOF
[Unit]
Description=Open Web UI serving
After=network.target

[Service]
User=$USER
Group=$USER
WorkingDirectory=/home/$USER
ExecStart=/bin/bash -lc "module load miniforge &amp;&amp; conda run -n open-webui open-webui serve"
Restart=always

[Install]
WantedBy=multi-user.target
EOF2</code></pre>
</div>
<p>To later change context length: edit <code>/etc/systemd/system/llama.service</code>, modify <code>--n_ctx</code>, then run:</p>
<div class="highlight">
<pre><code>sudo systemctl daemon-reload
sudo systemctl restart llama</code></pre>
</div>
</section>
</section>
<section id="configure-web-server-for-https" class="level2">
<h2 class="anchored" data-anchor-id="configure-web-server-for-https">Configure web server for HTTPS<a href="#configure-web-server-for-https" class="headerlink" title="Permanent link">¶</a></h2>
<p>Finally we can use Caddy to serve the web interface with HTTPS.</p>
<p>Install <a href="https://caddyserver.com/">Caddy</a>. Note that the version of Caddy available in the Ubuntu APT repositories is often outdated. Follow <a href="https://caddyserver.com/docs/install#debian-ubuntu-raspbian">the instructions to install Caddy on Ubuntu</a>. You can copy-paste all the lines at once.</p>
<p>Modify the Caddyfile to serve the web interface. (Note that <code>sensible-editor</code> will prompt you to choose a text editor; select the number for <code>/bin/nano</code> if you aren’t sure what else to pick.)</p>
<div class="highlight">
<pre><code>sudo sensible-editor /etc/caddy/Caddyfile</code></pre>
</div>
<p>to:</p>
<div class="highlight">
<pre><code>chat.xxx000000.projects.jetstream-cloud.org {

        reverse_proxy localhost:8080
}</code></pre>
</div>
<p>Where <code>chat</code> is the name of your instance, and <code>xxx000000</code> is the allocation code. You can find the full hostname (e.g. <code>chat.xxx000000.projects.jetstream-cloud.org</code>) in Exosphere: open your instance’s details page, scroll to the Credentials section, and copy the value shown under Hostname.</p>
<p>Then reload Caddy:</p>
<div class="highlight">
<pre><code>sudo systemctl reload caddy</code></pre>
</div>
</section>
<section id="connect-the-model-and-test-the-chat-interface" class="level2">
<h2 class="anchored" data-anchor-id="connect-the-model-and-test-the-chat-interface">Connect the model and test the chat interface<a href="#connect-the-model-and-test-the-chat-interface" class="headerlink" title="Permanent link">¶</a></h2>
<p>Point your browser to <code>https://chat.xxx000000.projects.jetstream-cloud.org</code> and you should see the chat interface.</p>
<p>Create an account, click on the profile icon on the top right and enter the “Admin panel” section, open “Settings” then “Connections”. Once you create the first account, that will become admin, if anyone else creates an account they will be a regular user and need to be approved by the admin user. This is the only protection available in this setup, an attacker could still leverage vulnerabilities on Open WebUI to gain access. If you require more security the easiest way is to just open the firewall (using <code>ufw</code>) to only allow connections from your IP.</p>
<p>Under “OpenAI API” enter the URL <code>http://localhost:8080/v1</code> and leave the API key empty (the local llama.cpp server is unsecured by default on localhost).</p>
<p>Click on the “Verify connection” button, then to “Save” on the bottom.</p>
<p>Finally you can start chatting with the model!</p>
<p>If you change context length (<code>--ctx-size</code>) or increase concurrent users you may quickly approach the 40 GB limit of the <code>g3.xl</code> instance. Reduce <code>--ctx-size</code> (e.g.&nbsp;2048 or lower) if you encounter out-of-memory errors. For even larger models or higher concurrency, consider specialized solutions or future Jetstream instances with more GPU memory.</p>
</section>
<section id="scaling-up-or-changing-models" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up-or-changing-models">Scaling up or changing models<a href="#scaling-up-or-changing-models" class="headerlink" title="Permanent link">¶</a></h2>
<p>Want a larger model or higher quality? Options:</p>
<ul>
<li>Use a higher-bit quantization (e.g., Q4_K_M or Q5_K_M) for better quality (needs more VRAM, potentially exceeding <code>g3.xl</code> capacity).</li>
<li>For unquantized models or even larger models, you would need an instance with more GPU memory than <code>g3.xl</code>.</li>
<li>Increase context length (each 1k tokens adds memory usage). If you see OOM, lower <code>--ctx-size</code>.</li>
</ul>
<p>For production workloads, consider the managed Jetstream inference service or frameworks like <code>vllm</code> on larger GPUs for higher throughput.</p>
<p><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="September 18, 2025 12:00:00 UTC">September 18, 2025</span></p>
</section>
</section>
<p>:::</p>
<p>Back to top :::</p>
<p><a href="../running-llm/" class="md-footer__link md-footer__link--prev"></a></p>
<p>::: {.md-footer__button .md-icon”} :::</p>
<div class="{.md-footer__title&quot;}">
<p><span class="md-footer__direction">Previous</span></p>
<div class="{.md-ellipsis&quot;}">
<p>Orientation to Running LLMs</p>
</div>
</div>
<p>::: {.md-footer__button .md-icon”} :::</p>
<p><a href="../../tutorial/classroom/overview/" class="md-footer__link md-footer__link--next"></a></p>
<div class="{.md-footer__title&quot;}">
<p><span class="md-footer__direction">Next</span></p>
<div class="{.md-ellipsis&quot;}">
<p>Overview</p>
</div>
</div>
<p>::: {.md-footer__button .md-icon”} :::</p>
<p>::: {.md-footer-meta .md-typeset”} ::: {.md-footer-meta__inner .md-grid”} ::: {.md-copyright”} ::: {.md-copyright__highlight”} Copyright © 2025 The Trustees of Indiana University :::</p>
<p>Made with <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a> ::: ::: ::: :::</p>
<div class="md-dialog" data-md-component="dialog">
<p>::: {.md-dialog__inner .md-typeset”}</p>
</div>
<p>:::</p>



</main> <!-- /main -->
<div>
    <hr>
</div>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.zonca\.dev");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>