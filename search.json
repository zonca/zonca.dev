[
  {
    "objectID": "consult.html",
    "href": "consult.html",
    "title": "Consulting",
    "section": "",
    "text": "I have a large expertise ranging from scientific computing to web development and devops.\nI am available to be hired as a consultant in:\n\nAI: Custom development of AI workflows based on Convolutional Neural Networks, Large Language Models and more\nScientific computing: development of high performance and parallel Python software for data analysis, code-review, software design\nWeb: hosting of websites, development of custom Python web apps using Flask and Django\nKubernetes: deployment of Kubernetes to Openstack, deployment on top of Kubernetes of JupyterHub, Dask or custom microservices.\n\nSee my AI consulting services locally in San Diego through San Diego Data Science LLC"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Mastodon\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPySM 3.4.1 Released\n\n\n\n\n\n\nopenscience\n\n\npysm\n\n\npython\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nhealpy 1.18.1 released\n\n\n\n\n\n\nhealpy\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew Simulations of the Panexp Model Suite for Planck and WMAP\n\n\n\n\n\n\npysm\n\n\ncmb\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nJetstream2 Usage Monitoring\n\n\n\n\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment of BinderHub by Project Pythia on Jetstream 2\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGateways 2024 - Tutorial on Creating a Serverless Research Data Repository based on Globus\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew Paper on Arxiv about Models of Galactic Emission in the Microwaves for CMB Experiments\n\n\n\n\n\n\nopenscience\n\n\npysm\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUse Jetstream’s DeepSeek R1 as a code assistant on JupyterAI\n\n\n\n\n\n\njetstream\n\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAuthenticate to GitHub in the Browser with the Device Flow\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSave Jupyterlite Notebooks to GitHub\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUsing GitHub Releases to keep track of Large Artifacts\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nItalian and chess classes in San Diego\n\n\n\n\n\n\nitalian\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProposal for of Simons Observatory Data Products Attribution\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRun Windows and WSL on Jetstream\n\n\n\n\n\n\njetstream\n\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAllow users to self-invite to a Google Calendar event\n\n\n\n\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy a LLM Chat-GPT like service on Jetstream\n\n\n\n\n\n\njetstream\n\n\nllm\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes monitoring with Prometheus and Grafana\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate script to mount a Manila share\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nOct 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure ssh-agent on Linux\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial on Python for HPC at the SDSC Summer Institute 2024\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Anki flashcards to help kids learn spelling\n\n\n\n\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate sample Cosmology data for Cheap and FAIR data portal with PySM\n\n\n\n\n\n\ncosmology\n\n\nhealpy\n\n\npysm\n\n\n\n\n\n\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCosmic Microwave Background data licensing\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExport a Confluence page to Markdown\n\n\n\n\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUbuntu 22.04 Minimal on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPEARC24 Paper Cheap and FAIR Data Portal\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate point source maps with pixell and convert to HEALPix\n\n\n\n\n\n\ncosmology\n\n\npysm\n\n\n\n\n\n\n\n\n\nJul 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRun (part of) healpy in the browser with pyodide\n\n\n\n\n\n\npython\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nConditional authentication on a single endpoint with FastAPI\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy a web server with automatic HTTPS for static website and Django app on Jestream 2\n\n\n\n\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAuthenticate FastAPI endpoints with a Github organization\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Notebook frontmatter for Quarto to download source notebook\n\n\n\n\n\n\npython\n\n\njupyter\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAI Code Assistant at NERSC\n\n\n\n\n\n\npython\n\n\nnersc\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive 3D plot of a Planck map with Matplotlib\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHEALPix in Megapixels\n\n\n\n\n\n\ncosmology\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLoad data from Globus in the browser and plot with pyscript\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUbuntu 22.04 Minimal on Jetstream with GPU Support\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNews item on SDSC website about Advanced Simons Observatory and PySM\n\n\n\n\n\n\npython\n\n\npysm\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nItalian classes in San Diego\n\n\n\n\n\n\nitalian\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate point source maps with pixell\n\n\n\n\n\n\ncosmology\n\n\npysm\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSetup a conda environment at NERSC\n\n\n\n\n\n\npython\n\n\nnersc\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFlatcar Container Linux on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAutomate sending polls via WhatsApp\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n2-way Synchronization of Overleaf and Github via a Github Action\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate figures for PySM emissions\n\n\n\n\n\n\ncosmology\n\n\nhealpy\n\n\npysm\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nRun PyTorch with GPU support on Expanse\n\n\n\n\n\n\nhpc\n\n\njupyter\n\n\n\n\n\n\n\n\n\nApr 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream 2 with GPU support\n\n\n\n\n\n\njetstream2\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSoft Scaling Kubernetes on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\nana-espinoza\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMake a PDF from a series of screenshots\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReceived DeSouza award for work on deploying software infrastructure on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBuy US Treasury bonds on Fidelity\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGateways 2023 tutorial about Dask and JupyterHub on Kubernetes on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Dask Gateway with JupyterHub on Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\njupyterhub\n\n\ndask\n\n\n\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Github Authenticator in JupyterHub\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSetup HTTPS on Kubernetes with cert-manager\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial on Python for HPC at the SDSC Summer Institute\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream 2 with Kubespray 2.21.0\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHiring a Computational Scientist at the San Diego Supercomputer Center\n\n\n\n\n\n\nhpc\n\n\n\n\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCosmology webinar Hyper Supreme Cam\n\n\n\n\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGet a Letsencrypt certificate for Dreamhost with certbot\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate Openstack credentials in Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWork on a Latex Document in Github Codespaces\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\n\n\n\n\n\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGzipping or not Gzipping HEALPix maps\n\n\n\n\n\n\nhealpy\n\n\nnersc\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShare data from NERSC publicly via web\n\n\n\n\n\n\nnersc\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShowyourwork! Globus demo\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy a NFS server to share data between JupyterHub users on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\nlinux\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWork on a Latex Document with Github and Overleaf\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream 2 with GPU support\n\n\n\n\n\n\njetstream2\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCompare WebSky Radio Galaxies maps from PySM to Planck\n\n\n\n\n\n\nhealpy\n\n\npysm\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nShare data between JupyterHub users with Manila shares\n\n\n\n\n\n\njetstream2\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy BinderHub on top of Kubernetes on Jetstream 2\n\n\n\n\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSetup a conda environment at NERSC\n\n\n\n\n\n\npython\n\n\nnersc\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBuild and host Singularity containers on Github\n\n\n\n\n\n\nsingularity\n\n\n\n\n\n\n\n\n\nNov 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Click command line options dynamically from class arguments\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nClosures in Numba\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the JupyROOT Python kernel in JupyterHub\n\n\n\n\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSingularity on Expanse tutorial\n\n\n\n\n\n\nsingularity\n\n\nhpc\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMigrate from fastpages to quarto preserving git history\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy MariaDB on Jetstream 2 on top of Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nJun 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRemove unique cell id from Jupyter Notebooks\n\n\n\n\n\n\ngit\n\n\njupyter\n\n\n\n\n\n\n\n\n\nMay 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAccess running Github action with SSH\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMonitor Restic backups on Kubernetes\n\n\n\n\n\n\njetstream2\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nApr 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nJetstream2 SU calculator\n\n\n\n\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCustos authentication for JupyterHub\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nApr 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nScience Gateway with Dask and Zarr\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\njupyterhub\n\n\ndask\n\n\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUse the distributed file format Zarr on Jetstream 2 object storage\n\n\n\n\n\n\ndask\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Dask Gateway with JupyterHub on Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\njupyterhub\n\n\ndask\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy JupyterHub on Jetstream 2 on top of Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njupyterhub\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nMar 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream 2 with Kubespray 2.18.0\n\n\n\n\n\n\nkubernetes\n\n\njetstream2\n\n\n\n\n\n\n\n\n\nMar 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow for Jupyter Notebooks under version control\n\n\n\n\n\n\njupyter\n\n\ngit\n\n\n\n\n\n\n\n\n\nMar 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Dask Gateway with JupyterHub on Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\ndask\n\n\n\n\n\n\n\n\n\nJan 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMake for repeated commands\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJan 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFund healpy via Github Sponsors\n\n\n\n\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nNov 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes certifications CKA and CKAD\n\n\n\n\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nNov 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCompare HEALPix and Gauss-Legendre pixelizations for sky emission modelling\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\npysm\n\n\n\n\n\n\n\n\n\nNov 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Hadoop on Kubernetes on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPlot HEALPix pixel boundaries\n\n\n\n\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nOct 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMonitor traffic on Github repositories\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nOct 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nManage Globus groups with the Python SDK\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper about PySM 3\n\n\n\n\n\n\nopenscience\n\n\npython\n\n\npysm\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUpload a JOSS paper to Arxiv\n\n\n\n\n\n\ngithub\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure dreamhost email\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nhealpy 1.15.0 released\n\n\n\n\n\n\nhealpy\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigate rotation of masks in healpy\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nJun 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLearn Italian in San Diego\n\n\n\n\n\n\nitalian\n\n\n\n\n\n\n\n\n\nJun 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMigrate from Google Docs to Overleaf and Github\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nMay 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCorrect a power spectrum by the instrument beam with healpy\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBackup Kubernetes volumes to OpenStorageNetwork object store\n\n\n\n\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nApr 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nScience Gateways Tech blog - Kubernetes and JupyterHub on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nMar 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nStream video from object store on Jetstream\n\n\n\n\n\n\njetstream\n\n\n\n\n\n\n\n\n\nMar 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTutorial on how to rotate maps in healpy\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the Planck CMB temperature power spectrum with healpy anafast\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nFeb 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinate a large Latex document with multiple Overleaf projects and Github\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJan 28, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAutoscaling script for JupyterHub on top of Kubespray\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nJan 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream with Kubespray 2.15.0\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nJan 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCFITSIO file writing with checkpointing\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPaper review workflow with Overleaf, git and Google Docs\n\n\n\n\n\n\nopenscience\n\n\ngit\n\n\n\n\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nInstall HEALPix and PolSpice in a conda environment\n\n\n\n\n\n\nhealpy\n\n\nhpc\n\n\nnersc\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nExample of the healpy query disc function\n\n\n\n\n\n\npython\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nOct 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRead and process Planck CMB power spectra with healpy\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nEdit video recordings with ffmpeg\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow to share Jupyter Notebooks\n\n\n\n\n\n\njupyter\n\n\ngithub\n\n\n\n\n\n\n\n\n\nSep 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSimple WCS with astropy modeling and gwcs\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGateways 2020 paper about Kubernetes and JupyterHub on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUnit conversion with broadband detectors looking at the CMB\n\n\n\n\n\n\nhealpy\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUnit conversion with broadband detectors looking at foregrounds\n\n\n\n\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMigrate from Travis-CI and Readthedocs to Github actions\n\n\n\n\n\n\npython\n\n\ngithub\n\n\n\n\n\n\n\n\n\nSep 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRedirect readthedocs documentation to another website\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nSep 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRenew letsencrypt certificate with NGINX\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nAug 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigate broken implementation of bandpass unit conversion in PySM 3\n\n\n\n\n\n\ncosmology\n\n\npysm\n\n\n\n\n\n\n\n\n\nAug 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Dask Gateway with JupyterHub on Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\ncloudcomputing\n\n\njetstream\n\n\njupyterhub\n\n\ndask\n\n\n\n\n\n\n\n\n\nAug 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDask array rounding\n\n\n\n\n\n\npython\n\n\ndask\n\n\n\nDask array rounding float32\n\n\n\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy a NFS server to share data between JupyterHub users on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHandle white noise with healpy 3 not-uniform and partial sky coverage\n\n\n\n\n\n\ncosmology\n\n\npython\n\n\nhealpy\n\n\n\nSimulate white noise maps and use hitmaps\n\n\n\n\n\nJun 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHandle white noise with healpy 2 partial sky coverage\n\n\n\n\n\n\ncosmology\n\n\npython\n\n\nhealpy\n\n\n\nSimulate white noise maps and use hitmaps\n\n\n\n\n\nJun 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHandle white noise with healpy 1 Full sky coverage\n\n\n\n\n\n\ncosmology\n\n\npython\n\n\nhealpy\n\n\n\nSimulate white noise maps and use hitmaps\n\n\n\n\n\nJun 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream with Kubespray\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nJun 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMy own git cheatsheet\n\n\n\n\n\n\ngit\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nImport and export list of blocked users on Twitter\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMinimal example of readthedocs configuration for conda\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes and JupyterHub on Jetstream with Magnum\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWhite noise NET in Radio-astronomy and Cosmology\n\n\n\n\n\n\ncosmology\n\n\npython\n\n\nhealpy\n\n\n\nCreate a white noise map and compare with power spectrum expected from the NET\n\n\n\n\n\nApr 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nECSS Symposium introduction to cloud computing with Jetstream and deployment of Kubernetes\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUse the Jetstream object store\n\n\n\n\n\n\njetstream\n\n\n\n\n\n\n\n\n\nApr 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRaise and check a flag array with numpy\n\n\n\n\n\n\npython\n\n\n\nHandle flag array bits with numpy bitwise operations\n\n\n\n\n\nMar 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSetup HTTPS on Kubernetes with Letsencrypt\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nMar 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nKill Jupyter Notebook servers\n\n\n\n\n\n\npython\n\n\njupyter\n\n\n\n\n\n\n\n\n\nMar 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nMigrate from Pelican to Fastpages\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy CVMFS on Kubernetes\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nFeb 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nOrganize calendars for a large scientific collaboration\n\n\n\n\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nDec 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSimulate users on JupyterHub\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nOct 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nExecute Jupyter Notebooks not interactively\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nSep 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Cluster Autoscaler for Kubernetes on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Github account for your research group with free private repositories\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nAug 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nShip large files with Python packages\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes and JupyterHub on Jetstream with Magnum\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nJun 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWebinar about distributed computing with Python\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nMay 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes monitoring with Prometheus and Grafana\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nApr 20, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nInherit group permission in folder\n\n\n\n\n\n\nhpc\n\n\nlinux\n\n\n\n\n\n\n\n\n\nMar 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes monitoring with Dashboard, Prometheus, and Grafana\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nScale Kubernetes manually on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes with Kubespray 2.8.2 and JupyterHub with helm recipe 0.8 on Jetstream\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nFeb 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nUse the distributed file format Zarr on Jetstream Swift object storage, 2019\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\ndask\n\n\n\n\n\n\n\n\n\nJan 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Pangeo on Kubernetes deployment on Jetstream created with Kubespray\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nDec 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSetup two factor authentication for UCSD, and Lastpass\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition\n\n\n\n\n\n\njupyterhub\n\n\nhpc\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition\n\n\n\n\n\n\njupyterhub\n\n\nhpc\n\n\n\n\n\n\n\n\n\nNov 7, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced pandas with Astrophysics example Notebook\n\n\n\n\n\n\njupyter\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nOct 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBring your computing to the San Diego Supercomputer Center\n\n\n\n\n\n\nhpc\n\n\n\n\n\n\n\n\n\nOct 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nSep 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nExplore a Kubernetes deployment on Jetstream with Kubespray 2/3\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nSep 23, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Kubernetes on Jetstream with Kubespray 1/3\n\n\n\n\n\n\nkubernetes\n\n\njetstream\n\n\n\n\n\n\n\n\n\nSep 23, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPEARC18 Paper on Deploying Jupyterhub at scale on XSEDE\n\n\n\n\n\n\nsingularity\n\n\njetstream\n\n\njupyterhub\n\n\nkubernetes\n\n\n\n\n\n\n\n\n\nJul 23, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Singularity images for Comet\n\n\n\n\n\n\nsingularity\n\n\nhpc\n\n\n\n\n\n\n\n\n\nJul 22, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCreate DockerHub auto build\n\n\n\n\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJul 19, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow to organize code and data for simulations at NERSC\n\n\n\n\n\n\nhpc\n\n\npython\n\n\nnersc\n\n\n\n\n\n\n\n\n\nJun 20, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSetup private dask clusters in Kubernetes alongside JupyterHub on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\nkubernetes\n\n\njetstream\n\n\ndask\n\n\n\n\n\n\n\n\n\nJun 7, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nHow to post a PEARC18 paper pre-print to Arxiv\n\n\n\n\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nMay 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLaunch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\ndask\n\n\n\n\n\n\n\n\n\nMay 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nInstall a BOINC server on Jetstream\n\n\n\n\n\n\njetstream\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nMar 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUse the distributed file format Zarr on Jetstream Swift object storage\n\n\n\n\n\n\njupyter\n\n\njetstream\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nMar 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUse the distributed file format Zarr on Jetstream Swift object storage\n\n\n\n\n\n\njupyter\n\n\njetstream\n\n\n\n\n\n\n\n\n\nMar 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nInstall custom Python environment on Jupyter Notebooks at NERSC\n\n\n\n\n\n\njupyterhub\n\n\npython\n\n\nhpc\n\n\nnersc\n\n\n\n\n\n\n\n\n\nDec 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nECSS Symposium about Jupyterhub deployments on XSEDE\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nDec 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy scalable Jupyterhub with Kubernetes on Jetstream\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nDec 5, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nStore a conda environment inside a Notebook\n\n\n\n\n\n\njupyter\n\n\npython\n\n\n\n\n\n\n\n\n\nDec 4, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to modify Singularity images on a Supercomputer\n\n\n\n\n\n\nsingularity\n\n\nhpc\n\n\n\n\n\n\n\n\n\nNov 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy scalable Jupyterhub on Docker Swarm mode\n\n\n\n\n\n\njupyterhub\n\n\njetstream\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nOct 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSetup automated testing on a Github repository with Travis-ci\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\n\n\n\n\n\n\n\nOct 25, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSetup automated testing on a Github repository with Travis-ci\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\n\n\n\n\n\n\n\nSep 6, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers\n\n\n\n\n\n\njupyterhub\n\n\nsingularity\n\n\nhpc\n\n\n\n\n\n\n\n\n\nAug 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create pull requests on Github\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\n\n\n\n\n\n\n\nJun 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to create pull requests on Github\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nJun 30, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Jupyterhub on a Supercomputer with SSH Authentication\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\nhpc\n\n\n\n\n\n\n\n\n\nMay 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure Globus on your local machine for GridFTP with XSEDE authentication\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSample deployment of Jupyterhub in HPC on SDSC Comet\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\nhpc\n\n\n\n\n\n\n\n\n\nFeb 26, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize your Python environment in Jupyterhub\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nFeb 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated deployment of Jupyterhub with Ansible\n\n\n\n\n\n\njupyterhub\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nFeb 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to publish your research software to Github\n\n\n\n\n\n\ngit\n\n\ngithub\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nFeb 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nRun Ubuntu in HPC with Singularity\n\n\n\n\n\n\nsingularity\n\n\nhpc\n\n\n\n\n\n\n\n\n\nJan 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nJupyterhub Docker Spawner with GPU support\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nJupyterhub deployment on multiple nodes with Docker Swarm\n\n\n\n\n\n\njupyterhub\n\n\ncloudcomputing\n\n\n\n\n\n\n\n\n\nMay 24, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Jupyterhub deployment for workshops with pre-built image\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nApr 28, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Jupyterhub on a Virtual Machine for a Workshop\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nApr 16, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUse your own Python installation (kernel) in Jupyterhub\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\n\n\n\n\n\n\n\nOct 5, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nIPython/Jupyter notebook setup on NERSC Edison\n\n\n\n\n\n\njupyter\n\n\npython\n\n\nhpc\n\n\nnersc\n\n\n\n\n\n\n\n\n\nSep 24, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nIPython/Jupyter notebook setup on SDSC Comet\n\n\n\n\n\n\npython\n\n\nhpc\n\n\njupyter\n\n\n\n\n\n\n\n\n\nSep 17, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nRun Jupyterhub on a Supercomputer\n\n\n\n\n\n\npython\n\n\njupyterhub\n\n\nhpc\n\n\n\n\n\n\n\n\n\nApr 2, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nAccelerate groupby operation on pixels with Numba\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMar 24, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware Carpentry setup for Chromebook\n\n\n\n\n\n\nlinux\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 10, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nZero based indexing\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 22, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nWrite unit tests as cells of IPython notebooks\n\n\n\n\n\n\npython\n\n\njupyter\n\n\n\n\n\n\n\n\n\nSep 30, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nHow to perform code review for scientific software\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nAug 28, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Github account for your research group with free private repositories\n\n\n\n\n\n\ngithub\n\n\ngit\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nAug 19, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on a career as a computational scientist\n\n\n\n\n\n\nhpc\n\n\n\n\n\n\n\n\n\nJun 5, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning at scale with Python\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nMar 20, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nPython on Gordon\n\n\n\n\n\n\nhpc\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 20, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nBuild Software Carpentry lessons with Pelican\n\n\n\n\n\n\npython\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nFeb 26, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nopenproceedings - Github/FigShare based publishing platform for conference proceedings\n\n\n\n\n\n\npython\n\n\nopenscience\n\n\n\n\n\n\n\n\n\nFeb 13, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nwget file from google drive\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJan 31, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nRun IPython Notebook on a HPC Cluster via PBS\n\n\n\n\n\n\npython\n\n\nhpc\n\n\n\n\n\n\n\n\n\nDec 18, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nJoining San Diego Supercomputer Center\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nPublished paper on Destriping Cosmic Microwave Background Polarimeter data\n\n\n\n\n\n\npython\n\n\ncosmology\n\n\n\n\n\n\n\n\n\nNov 20, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nJiffylab multiuser IPython notebooks\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 14, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nHow to log exceptions in Python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 1, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Plus comments plugin for Pelican\n\n\n\n\n\n\npython\n\n\ngithub\n\n\n\n\n\n\n\n\n\nSep 27, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nHow to automatically build your Pelican blog and publish it to Github Pages\n\n\n\n\n\n\npython\n\n\ngit\n\n\n\n\n\n\n\n\n\nSep 26, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nclviewer, interactive plot of CMB spectra\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nSep 17, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nPlanck CMB map at high resolution\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nSep 10, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nRun Hadoop Python jobs on Amazon with MrJob\n\n\n\n\n\n\ngit\n\n\npython\n\n\n\n\n\n\n\n\n\nSep 2, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive figures in the browser - CMB Power Spectra\n\n\n\n\n\n\nastrophysics\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nAug 30, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nPlanck CTP angular power spectrum ell binning\n\n\n\n\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nAug 20, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nHEALPix map of the Earth using healpy\n\n\n\n\n\n\nhealpy\n\n\n\n\n\n\n\n\n\nAug 8, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nExport google analytics data via API with Python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 4, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing sources in Planck maps with Hadoop and Python\n\n\n\n\n\n\nhpc\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 15, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use the IPython notebook on a small computing cluster\n\n\n\n\n\n\nhpc\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 22, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nIPython parallell setup on Carver at NERSC\n\n\n\n\n\n\nhpc\n\n\npython\n\n\n\n\n\n\n\n\n\nApr 11, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Mixin usage in python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nApr 8, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nNoise in spectra and map domain\n\n\n\n\n\n\nhealpy\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nApr 8, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nBasic fork/pull git workflow\n\n\n\n\n\n\ngit\n\n\n\n\n\n\n\n\n\nApr 6, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive 3D plot of a sky map\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMar 12, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nHow to cite HDF5 in bibtex\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCompile healpix C++ to javascript\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJan 28, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nElliptic beams, FWHM and ellipticity\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nJan 18, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nUbuntu PPA for HEALPix and healpy\n\n\n\n\n\n\nhealpy\n\n\nlinux\n\n\n\n\n\n\n\n\n\nDec 17, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nButterworth filter with Python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nOct 6, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nIPython.parallel for Planck data analysis at NERSC\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nSep 27, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nhomepage on about.me\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2012\n\n\n\n\n\n\n\n\n\n\n\n\ndoctests and unittests happiness 2\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 16, 2012\n\n\n\n\n\n\n\n\n\n\n\n\ncompile python module with mpi support\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nsome python resources\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nNov 1, 2011\n\n\n\n\n\n\n\n\n\n\n\n\ncfitsio wrapper in python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 21, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nunit testing happiness\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 21, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nPink noise (1/f noise) simulations in numpy\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMay 18, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nVim regular expressions\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 29, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nset python logging level\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nApr 13, 2011\n\n\n\n\n\n\n\n\n\n\n\n\npyfits memory leak in new_table\n\n\n\n\n\n\npython\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMar 28, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nipython and PyTrilinos\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 16, 2011\n\n\n\n\n\n\n\n\n\n\n\n\ngit make local branch tracking origin\n\n\n\n\n\n\ngit\n\n\n\n\n\n\n\n\n\nFeb 2, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nmemory map npy files\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 7, 2011\n\n\n\n\n\n\n\n\n\n\n\n\nforce local install of python module\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nDec 3, 2010\n\n\n\n\n\n\n\n\n\n\n\n\ngnome alt f2 popup launcher\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nAug 31, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nswitch to interactive backend with ipython -pylab\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 21, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nnumpy dtypes and fits keywords\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 4, 2010\n\n\n\n\n\n\n\n\n\n\n\n\ncount hits with numpy\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 23, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nchange column name in a fits with pyfits\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 30, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nhealpix coordinates\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nJun 23, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nparallel computing the python way\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 21, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nquaternions for python\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJun 21, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nchange permission recursively to folders only\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nMar 23, 2010\n\n\n\n\n\n\n\n\n\n\n\n\naptitude search ‘and’\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nMar 16, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nusing numpy dtype with loadtxt\n\n\n\n\n\n\nlinux\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 3, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nStop ipcluster from a script\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 19, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nexecute bash script remotely with ssh\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJan 7, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nlock pin hold a package using apt on ubuntu\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nJan 7, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nload arrays from a text file with numpy\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nJan 5, 2010\n\n\n\n\n\n\n\n\n\n\n\n\nLatest Maxima and WxMaxima for Ubuntu Karmic\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nDec 15, 2009\n\n\n\n\n\n\n\n\n\n\n\n\nnumber of files in a folder and subfolders\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nDec 10, 2009\n\n\n\n\n\n\n\n\n\n\n\n\nforcefully unmount a disk partition\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 17, 2008\n\n\n\n\n\n\n\n\n\n\n\n\nnetcat, quickly send binaries through network\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nApr 29, 2008\n\n\n\n\n\n\n\n\n\n\n\n\nDecibels, dB and dBm, in terms of Power and Amplitude\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMar 29, 2008\n\n\n\n\n\n\n\n\n\n\n\n\nRelation between Power density and temperature in an antenna\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nMar 28, 2008\n\n\n\n\n\n\n\n\n\n\n\n\nProducing PDF from XML files\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nMar 28, 2008\n\n\n\n\n\n\n\n\n\n\n\n\nvim costumization\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nOct 17, 2006\n\n\n\n\n\n\n\n\n\n\n\n\nusing gnu find\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nOct 3, 2006\n\n\n\n\n\n\n\n\n\n\n\n\nbeginners bash guide\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nOct 3, 2006\n\n\n\n\n\n\n\n\n\n\n\n\ntar quickref\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 25, 2006\n\n\n\n\n\n\n\n\n\n\n\n\nsoftware carpentry\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 25, 2006\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware libero per il trattamento di dati scientifici\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 22, 2006\n\n\n\n\n\n\n\n\n\n\n\n\ncommand line processing\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 22, 2006\n\n\n\n\n\n\n\n\n\n\n\n\nawk made easy\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\nSep 22, 2006\n\n\n\n\n\n\n\n\n\n\n\n\npillole di astrofisica\n\n\n\n\n\n\nastrophysics\n\n\n\n\n\n\n\n\n\nSep 20, 2006\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-12-06-simons-observatory-doi.html",
    "href": "posts/2024-12-06-simons-observatory-doi.html",
    "title": "Proposal for of Simons Observatory Data Products Attribution",
    "section": "",
    "text": "CC BY 4.0 only requiring attribution and not restricting commercial use or remixes."
  },
  {
    "objectID": "posts/2024-12-06-simons-observatory-doi.html#license",
    "href": "posts/2024-12-06-simons-observatory-doi.html#license",
    "title": "Proposal for of Simons Observatory Data Products Attribution",
    "section": "",
    "text": "CC BY 4.0 only requiring attribution and not restricting commercial use or remixes."
  },
  {
    "objectID": "posts/2024-12-06-simons-observatory-doi.html#attribution",
    "href": "posts/2024-12-06-simons-observatory-doi.html#attribution",
    "title": "Proposal for of Simons Observatory Data Products Attribution",
    "section": "Attribution",
    "text": "Attribution\nWe would like people that use a public dataset to be able to properly attribute the work to the SO Collaboration. If there is a related paper, clearly this should be cited, but this does not necessarily appropriately credit everyone who worked on the infrastructure for producing this open data.\n\nCite a paper\nTraditionally attribution is achieved by asking people to cite a specific paper, while this works well with Academia rewarding paper citations, if anyone joins the collaboration after publication of the referenced paper, they would not receive proper recognition. Moreover, some data products do not have a specific paper to refer to. Finally, given the large quantity of data that (A)SO plans to release on short timescales, there may not be a specific paper that is relevant for a particular data product.\nPublishing a paper takes a lot of time, especially in a large collaboration which does a first round of internal review, therefore an author list is often obsolete by the time the paper is out. Moreover, papers on a specific topic might be spaced by 1 or 2 years and all data products published between those releases would reference an obsolete author list.\n\n\nDOI for each dataset\nThis is the most straighforward use of DOIs, and it is how Zenodo or Figshare work, each dataset (not necessarily each data product, but each data release) is assigned a DOI.\nUCSD offers a DOI service which is free to use and has API can that be used to mint DOIs for datasets programmatically within our data release pipeline.\nThis works well for identifying the data source but (possibly) dilutes citations. It depends a lot on the data release cycle, if data releases are rare, then this could work, each dataset has as authors the people contributing to that release.\nBeing all based on automated tools, in this case we also need a service (HTTP API) that automatically keeps track of the author list, so that the data release pipeline can query this service and get back the correct list of people to be added to a DOI based on the current status of the collaboration, the type of data product, and the time of the release. This service implements in software all the policies that we decide on how to assign authorship to a data product.\nExamples of data products:\n\nOccasional data release, e.g. once every few months, this could be either simulations or real data or results of some analysis. This is straightforward, we mint a single DOI for the release with the full member list at the time of the release. This doesn’t need to be completely automated, there could be a manual step where the membership list got from the automated service is merged with external collaborators and then the DOI is minted.\nDaily products, for example maps, in this case a single DOI per day would make it difficult to aggregate citations, it would be best to create a hierarchical DOI, with a canonical DOI “Simons Observatory Daily Maps” and a DOI for each month which is minted at the beginning of the month, and then data added to it. In this way we could coalesce citations for all the maps under the single canonical DOI.\n\nAn interesting usage of the hierarchical DOIs is Matplotlib releases:\nhttps://matplotlib.org/stable/project/citing.html#dois\nthey also have different authors for each version:\n\nhttps://zenodo.org/records/1343133\nhttps://zenodo.org/records/3633844\n\nthe “generic” DOI resolves to the last version, which is the standard behaviour in Zenodo.\nSee for example LIGO, for a data release they both have a page on their website with a DOI https://gwosc.org/eventapi/html/O4_Discovery_Papers/GW230529_181500/v1/ and then they have additional (smaller I guess) data products on Zenodo with another DOI.\n\n\nTimed DOI for the SO collaboration\nThis is an uncommon use of DOIs, the idea is that instead of having DOIs pointing to a specific dataset, the DOI is basically tracking the current members of the collaboration.\nFirst we create a DOI for the “Data produced by Simons Observatory” which is the canonical DOI which ideally points to all data ever published by the collaboration. Then we use the DOI versioning system to create new DOIs regularly, for example every quarter or every 6 months which points to a single URL which collects all data produced in that period, therefore the meaning of a DOI would be for example “Data produced by Simons Observatory between January and March 2025”. We can create the DOI in advance and then add the data to it as it is produced. So both data releases and daily products would be added to this DOI. If necessary, metadata of the DOI can be modified, so if we want to add a new member to the doi, we can do that.\nAgain, it would be best to have the management of authorship automated, and build a service that runs periodically to mint a new DOI and keep it up to date with the current membership list.\nOne weakness of this system is that it is not clear how to handle an outside member that collaborates just for a single product. In that case, if they are added to the DOI, they would be authors of all data products produced in that time-frame."
  },
  {
    "objectID": "posts/2024-11-13-self-invite-calendar-event.html",
    "href": "posts/2024-11-13-self-invite-calendar-event.html",
    "title": "Allow users to self-invite to a Google Calendar event",
    "section": "",
    "text": "Google Calendar is the most popular calendar service, and it is widely used in academia, often Scientific Collaborations maintain one or multiple shared calendars to keep track of events, deadlines, and meetings.\nOne of the missing features of Google Calendar is the ability to allow users to self-invite to an event, without the need for the event organizer to manually add them.\nCurrently if you want to share an event with users and allow them to be notified of any changes, you have to manually add them to the event, which can be cumbersome if you have a large number of users. The situation is better if you have all your users already in a Google Group, as you can add the group to the event, but this is not always the case.\nThe other option is to just share the entire calendar with the users, but this is not always desirable, as the users will be able to see all the events in the calendar, and not just the one they are interested in.\nIn this post, I will show you how to create a Google Form that allows users to subscribe to a Google Calendar event, without the need for the event organizer to manually add them.\nWe will use Google Apps Script to create a trigger that runs every time the form is submitted, and adds the user to the event.\nFirst, we need the Calendar ID of the calendar we want to add the users to.\nFrom the “Integrate calendar” section of the Calendar settings, copy the “Calendar ID”, which is in the format:\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx@group.calendar.google.com\nCreate a Google Form, in settings, activate “Collect email addresses” as “Responder input”, so that they can subscribe even if they do not have a Google Account associated with that email address. It would be best, if possible, to set “Collect email addresses” to “Verified”, so that users are required to verify their email address before submitting the form by logging in to Google.\nNext go to “Responses”, click on “Link to Sheets” and create a new spreadsheet.\nCreate a new Project in the Google Cloud Platform at the URL https://console.cloud.google.com and enable the Calendar API under “APIs & Services” -&gt; “Library”.\nGet the Project number from the “Project info” section and paste it in the “Project number” field in the Apps Script. This will require to configure the OAuth consent screen, just set it to External and add yourself as a test user.\nIn the spreadsheet, go to “Extensions” -&gt; “Apps Script” and paste the following code:\nhttps://github.com/zonca/google_calendar_autoinvite/blob/main/add_email_to_event.js\nInspired by this Gist.\nNext, we need the Event ID of the event we want to add the users to,\n\nOpen event editor in Calendar Web UI\nPaste in the Google Apps script the last part of the URL after “…eventedit/”\n\nGo under Services and add the Calendar API.\nThen go to “Triggers” and add a new trigger that runs the function we create every time the form is submitted.\nYou need to grant permissions, so go to the Apps Script and click on the “Run” or “Debug” button, and select the test_addEmailToEvent function then click on “Review Permissions” and grant the permissions.\nNow you can test the form, and you should see the user added to the event in the calendar, they will also receive an email notification."
  },
  {
    "objectID": "posts/2024-08-04-export-confluence-markdown.html",
    "href": "posts/2024-08-04-export-confluence-markdown.html",
    "title": "Export a Confluence page to Markdown",
    "section": "",
    "text": "I feel the pain, you have a page trapped in Confluence and you want to set it free. Of course the “Export” features of Confluence are useless.\nThe trick is really easy:\n\nSelect all the text in the Confluence page, no need to enter “Edit” mode, just select the entire page as it is in view mode.\nPaste it into “Google Docs”, this is going to maintain titles, links, bold, italics, even escapes underscores, unfortunately won’t be able to export images.\nIn Google Docs, go to “File” -&gt; “Download” -&gt; “Markdown (.md)”"
  },
  {
    "objectID": "posts/2024-07-29-pysm-point-source-pixell-healpix.html",
    "href": "posts/2024-07-29-pysm-point-source-pixell-healpix.html",
    "title": "Generate point source maps with pixell and convert to HEALPix",
    "section": "",
    "text": "Testing the pixell sim_objects functionality to create maps of point sources pre-smoothed with a gaussian beam. The purpose is to include this functionality in PySM to be able to generate on the fly maps of source starting from a catalog.\nCompared to the previous notebook, here I also use reproject to convert the map to HEALPix, plot the result and check the flux in HEALPix also agrees with the input.\n\nfrom pixell import enmap, utils, resample, curvedsky as cs, reproject, pointsrcs\nimport numpy as np\nimport healpy as hp\n\n\nfwhm = 5 * utils.degree\n\n\nshape, wcs = enmap.fullsky_geometry(res=fwhm / 3, proj=\"car\")\n\n\nshape\n\n(109, 216)\n\n\n\ndef fwhm2sigma(fwhm):\n    return fwhm / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n\n\ndef flux2amp(flux, fwhm):\n    sigma = fwhm2sigma(fwhm)\n    return flux / (2 * np.pi * sigma**2)\n\n\nassert flux2amp((2 * np.pi * fwhm2sigma(5) ** 2), 5) == 1\n\n\nn_sources = 1\nflux_sources = np.arange(n_sources) + 10\n\n\namplitude_sources = flux2amp(flux_sources, fwhm)\n\n\nsource_pos = np.array([[np.pi/4], [np.pi/3]])\n\n\nr, p = pointsrcs.expand_beam(fwhm2sigma(fwhm))\n\n\nsource_map = pointsrcs.sim_objects(shape, wcs, source_pos, amplitude_sources, ((r, p)))\n\n\nimport matplotlib.pyplot as plt\n\n\nsource_pos\n\narray([[0.78539816],\n       [1.04719755]])\n\n\n\nplt.imshow(source_map)\n\n\n\n\n\n\n\n\n\ndef aperture_photometry(\n    thumbs, aperture_radius, annulus_width=None, modrmap=None, pixsizemap=None\n):\n    \"\"\"\n    Flux from aperture photometry.\n\n    from https://github.com/msyriac/orphics/blob/master/orphics/maps.py\n\n    Parameters\n    ----------\n    thumb : ndmap\n        An (...,Ny,Nx) ndmap (i.e. a pixell enmap) containing the thumbnails.\n    aperture_radius : float\n        Aperture inner radius in radians\n    annulus_width : float\n        Annulus width for mean subtraction in radians.\n        Defaults to sqrt(2)-1 times the aperture inner radius.\n    modrmap : ndmap, optional\n        An (Ny,Nx) ndmap containing distances of each pixel from the center in radians.\n    modrmap : ndmap, optional\n        An (Ny,Nx) ndmap containing pixel areas in steradians.\n\n    Returns\n    -------\n    flux : ndarray\n        (...,) array of aperture photometry fluxes.\n\n    \"\"\"\n    if modrmap is None:\n        modrmap = thumbs.modrmap()\n    if annulus_width is None:\n        annulus_width = (np.sqrt(2.0) - 1.0) * aperture_radius\n    # Get the mean background level from the annulus\n    mean = thumbs[\n        ...,\n        np.logical_and(\n            modrmap &gt; aperture_radius, modrmap &lt; (aperture_radius + annulus_width)\n        ),\n    ].mean()\n    if pixsizemap is None:\n        pixsizemap = thumbs.pixsizemap()\n    # Subtract the mean, multiply by pixel areas and sum\n    return (((thumbs - mean) * pixsizemap)[..., modrmap &lt;= aperture_radius]).sum(\n        axis=-1\n    )\n\n\nfrom astropy import units as u\n\n\nbox_half_size_rad = 2 * fwhm\nbox_center = [source_pos[0, -1], source_pos[1, -1]]\nbox = np.array(\n    [\n        [box_center[0] - box_half_size_rad, box_center[1] - box_half_size_rad],\n        [box_center[0] + box_half_size_rad, box_center[1] + box_half_size_rad],\n    ]\n)  # in radians\n\n\nbox_center\n\n[0.7853981633974483, 1.0471975511965976]\n\n\n\ncutout = source_map.submap(box)\n\n\nplt.imshow(cutout)\n\n\n\n\n\n\n\n\n\naperture_photometry(cutout, 2 * fwhm)\n\n9.982631206252375\n\n\n\nfwhm/3\n\n0.02908882086657216\n\n\n\nfrom pixell import reproject\n\nsource_map_healpix = reproject.map2healpix(source_map)\n\n\nhp.npix2nside(source_map_healpix.size)\n\n64\n\n\n\nhp.nside2resol(_)\n\n0.015989479811663883\n\n\n\nhp.mollview(source_map_healpix)\n\n\n\n\n\n\n\n\n\n\nhp.gnomview(source_map_healpix, rot=hp.vec2ang(hp.ang2vec(source_pos[0], source_pos[1]), lonlat=True), xsize=1900, reso=.5)\n\n\n\n\n\n\n\n\n\nfrom pysm3.utils import healpix_aperture_photometry\n\nhealpix_aperture_photometry(source_map_healpix, source_pos[0,0], source_pos[1,0], 2 * fwhm)\n\n9.995092243734167"
  },
  {
    "objectID": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html",
    "href": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html",
    "title": "Gateways 2024 - Tutorial on Creating a Serverless Research Data Repository based on Globus",
    "section": "",
    "text": "Following our work published at PEARC 2024 presenting a Data Portal based on a static website built on top of Globus HTTPS services, we presented a tutorial at Gateways 2024."
  },
  {
    "objectID": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#tutorial-description",
    "href": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#tutorial-description",
    "title": "Gateways 2024 - Tutorial on Creating a Serverless Research Data Repository based on Globus",
    "section": "Tutorial Description",
    "text": "Tutorial Description\nAbstract: In this tutorial, we will walk our attendees through the process of starting from data files in a folder, organizing them into a Globus collection, and then publishing a data catalog website to GitHub pages using the “Serverless Research Data Repository” (SRDR) tools. At each stage of the process, we will highlight the features required to make a published dataset compliant with the FAIR (Findability, Accessibility, Interoperability, and Reuse) principles. The tutorial is targeted both at gateway developers interested in deploying and administering their own data portal and at attendees interested in publishing their data in the most effective way."
  },
  {
    "objectID": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#tutorial-material",
    "href": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#tutorial-material",
    "title": "Gateways 2024 - Tutorial on Creating a Serverless Research Data Repository based on Globus",
    "section": "Tutorial Material",
    "text": "Tutorial Material\nYou can find the tutorial material on GitHub:\nCheap and FAIR Gateways 2024"
  },
  {
    "objectID": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#video-tutorial",
    "href": "posts/2025-03-26-tutorial-globus-serverless-research-data-repository.html#video-tutorial",
    "title": "Gateways 2024 - Tutorial on Creating a Serverless Research Data Repository based on Globus",
    "section": "Video Tutorial",
    "text": "Video Tutorial\nBelow is the recording of the tutorial:\nYoutube video"
  },
  {
    "objectID": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html",
    "href": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html",
    "title": "AI Code Assistant at NERSC",
    "section": "",
    "text": "If you are used to Github Copilot on VS Code or on Google Colaboratory as I am, coding without an AI assistant is so slow!\nIn this tutorial we will see how to activate Google AI Gemini on Jupyter@NERSC, this will work as well in any other Jupyter environment. I choose Google Gemini because it has a generous free tier allowance, ChatGPT instead requires to buy credits for using it through the API.\nNotice that using the Gemini API you are sending your interactions with AI to Google, see the Gemini API privacy policy for more details.\nThe good news is that we do not need to be in control of the environment running JupyterHub, we can successfully install what is necessary in a Jupyter kernel we control.\nFor example we could use our own conda environment at NERSC.\nThis web page was generated from a Notebook, from the sidebar you can directly download the source Notebook, upload it to Jupyter@NERSC and directly execute it there, no need to copy-paste."
  },
  {
    "objectID": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#install-packages",
    "href": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#install-packages",
    "title": "AI Code Assistant at NERSC",
    "section": "Install packages",
    "text": "Install packages\nFirst make sure you are running the right Jupyter Kernel, which should be running in your own conda environment, see above on instructions on how to create it. You need to install the packages just once, either from this notebook or from the command line after having activated the environment. Once the packages are installed, no need to run this cell anymore.\nJupyter AI supports many model providers, each with different required packages:\n\n%pip install jupyter-ai langchain-google-genai"
  },
  {
    "objectID": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#configure-google-api-key",
    "href": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#configure-google-api-key",
    "title": "AI Code Assistant at NERSC",
    "section": "Configure Google API key",
    "text": "Configure Google API key\nNext we need a Google API key to authenticate, go to:\nhttps://aistudio.google.com/app/apikey\ncreate a new secret key, and paste it below:\n\n%env GOOGLE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\nenv: GOOGLE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  },
  {
    "objectID": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#load-and-test-jupyter-ai",
    "href": "posts/2024-05-28-ai-code-assistant-jupyter-nersc.html#load-and-test-jupyter-ai",
    "title": "AI Code Assistant at NERSC",
    "section": "Load and test Jupyter AI",
    "text": "Load and test Jupyter AI\nFinally we can test it\n\n%load_ext jupyter_ai\n\n/global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n%ai list gemini\n\n\n\n\n\n\n\n\n\n\nProvider\nEnvironment variable\nSet?\nModels\n\n\n\n\ngemini\nGOOGLE_API_KEY\n✅\n\n\n\n\n\n\n\n%%ai gemini:gemini-pro -f code\n\nfind indices of the largest 10 values in a numpy array\n\nAI generated code inserted below ⬇️\n\n\n\nimport numpy as np\n\ndef find_indices_of_largest_10_values(array):\n  \"\"\"Finds the indices of the largest 10 values in a numpy array.\n\n  Args:\n    array: A numpy array.\n\n  Returns:\n    A list of the indices of the largest 10 values in the array.\n  \"\"\"\n\n  # Find the indices of the largest 10 values in the array.\n  indices = np.argsort(array)[-10:]\n\n  # Return the list of indices.\n  return indices"
  },
  {
    "objectID": "posts/2025-03-30-panexp-model-suite-planck-wmap.html",
    "href": "posts/2025-03-30-panexp-model-suite-planck-wmap.html",
    "title": "New Simulations of the Panexp Model Suite for Planck and WMAP",
    "section": "",
    "text": "Following up on my recent post about PySM models for Galactic emission, I have recently executed three new simulations of the Panexp model suite:"
  },
  {
    "objectID": "posts/2025-03-30-panexp-model-suite-planck-wmap.html#new-simulations",
    "href": "posts/2025-03-30-panexp-model-suite-planck-wmap.html#new-simulations",
    "title": "New Simulations of the Panexp Model Suite for Planck and WMAP",
    "section": "New Simulations",
    "text": "New Simulations\n\nPlanck: Using NPIPE beam window functions and PR3 bandpasses for HFI, Beyond Planck for LFI\nWMAP: Including bandpasses and beam window functions\nIdealized delta-frequency instrument: Features a 5 arcmin beam at all frequencies to be used for component separation studies (50 frequency channels spanning 20-800 GHz)"
  },
  {
    "objectID": "posts/2025-03-30-panexp-model-suite-planck-wmap.html#documentation-and-access",
    "href": "posts/2025-03-30-panexp-model-suite-planck-wmap.html#documentation-and-access",
    "title": "New Simulations of the Panexp Model Suite for Planck and WMAP",
    "section": "Documentation and Access",
    "text": "Documentation and Access\nDetailed documentation about these simulations can be found in the Simon Observatory’s map-based simulations repository: Available Simulations Documentation\nAs these are public datasets, they have also been published on the CMB-S4 data portal: CMB-S4 Data Portal\nThese simulations provide valuable resources for researchers working on component separation and other CMB-related studies."
  },
  {
    "objectID": "posts/2024-05-29-fastapi-sso-github-organization.html",
    "href": "posts/2024-05-29-fastapi-sso-github-organization.html",
    "title": "Authenticate FastAPI endpoints with a Github organization",
    "section": "",
    "text": "FastAPI is a framework that simplifies building APIs in Python. Authentication via Google, Github and many more is provided by FastAPI-SSO.\nWe can also combine this with PyGithub to check if the user is a member of a specific organization and handle permissions on specific objects provided by an endpoint.\nSee the implementation in this Gist\nNotice that on from your Developer Settings on Github you both need to create an OAuth app (with http://localhost:5000/auth/callback callback) and a Personal Access Token (classic) with read:orgs scope to query group membership and store all their credentials in github_env.sh.\nOnce the app is running, try accessing http://127.0.0.1:5000/protected, you should first get “Not authenticated”, now login at http://localhost:5000/auth/login, you should be redirected to Github for authentication, then you should see if you are or not a member of the ORG organization as defined in the source file."
  },
  {
    "objectID": "posts/2024-05-28-quarto-notebook-frontmatter.html",
    "href": "posts/2024-05-28-quarto-notebook-frontmatter.html",
    "title": "Jupyter Notebook frontmatter for Quarto to download source notebook",
    "section": "",
    "text": "Quarto is one of the best platforms to create statically generated blogs with Markdown and Jupyter Notebooks.\nI switched zonca.dev to Quarto back in 2022.\nIt is convenient to give the possibility to users to download the .ipynb source of a page when that page is generated from a Jupyter Notebook, so that they can execute it themselves.\nQuarto supports this out of the box using this frontmatter as the first cell of a Notebook (in Raw format):\n---\nlayout: post\ntoc: true\nformat:\n  html: default\n  ipynb: default\ndate: '2024-05-28'\ncategories:\n- category1\n- category2\n---"
  },
  {
    "objectID": "posts/2024-05-16-globus-javascript-sdk-pyscript.html",
    "href": "posts/2024-05-16-globus-javascript-sdk-pyscript.html",
    "title": "Load data from Globus in the browser and plot with pyscript",
    "section": "",
    "text": "Globus supports a Javascript SDK which allows to authenticate against Globus Auth and then access all the Globus services, for example accessing protected data or setting up transfers between endpoints.\nI think the project is really powerful because brings Globus to the browser, however it is sparsely documented, so it is really difficult to leverage its potential.\nThis is somehow related to my work on the CMB-S4 data portal, the idea is that on top of the static Catalog inside a Data Portal we could also provide additional services via Javascript, without the need of a backend server.\nFor example plotting a dataset which requires authentication.\nRick Wagner created an example that uses the Globus Javascript SDK to authenticate and retrieve a protected JSON dataset and plot it with Chart.js, see https://rpwagner.github.io/globus-sdk-javascript-ex-01/.\nI expanded the example by passing the JSON dataset to PyScript (Python running in the browser) and then using pandas capabilities to parse the JSON into a DataFrame and create a Histogram on the page.\n\nWebpage with Globus and PyScript\nGithub repository\nIf you would like to reproduce this, follow the instructions on the Globus Javascript SDK Basic example to create a Globus app\n\nA few notes on the implementation:\n\nI was not able to use pydom in PyScript to make visible the iframe with the code, so I used Javascript for that\nThe HTTP request to load the data is automatically launched just after signing in, when completed, the “Plot data” button is activated\nI didn’t find a way to call the Python function pandas_plot from Javascript, that is why I added the “Plot data” button\nI directly pass the JSON string to Python, then use pandas.read_json to parse it\nImpressive how you can quickly load a huge package like pandas inside the browser, it is overkill for this example, but could be handy"
  },
  {
    "objectID": "posts/italian-classes-chess-san-diego-2025.html",
    "href": "posts/italian-classes-chess-san-diego-2025.html",
    "title": "Italian and chess classes in San Diego",
    "section": "",
    "text": "My wife Maura D’Andrea is Principal and Teacher of Italian school of San Diego.\nThey offer an extensive Adults Italian classes program in-person in San Diego (Kearny Mesa), including Italian for Travelers specifically for tourists travelling to Italy.\nThey recently launched 2 new interesting programs starting in April 2025:\n\nChess classes in person in San Diego for kids and adults, for all levels\nCrochet classes in person in San Diego in the morning for adults"
  },
  {
    "objectID": "posts/2024-05-02-python-nersc-conda.html",
    "href": "posts/2024-05-02-python-nersc-conda.html",
    "title": "Setup a conda environment at NERSC",
    "section": "",
    "text": "NERSC recommends to use the “Global Common” to store Conda environments, because it is optimized to store lots of small files. Consider that it is mounted read-only on computing nodes. The filesystem is organized by groups, so choose one of your groups:\ngroups\nfor example for me it is the cmb group.\nGROUP=cmb\nmkdir -p /global/common/software/$GROUP/$USER/conda\ncd ~\nln -s /global/common/software/$GROUP/$USER c\nSo we can access it quickly under ~/c.\nThen we create a Conda environment with mamba, specifying the version of python and other packages:\nexport ENV=pycmb\nmodule load conda\nmamba create --prefix /global/common/software/$GROUP/$USER/conda/$ENV python==3.10 numpy astropy matplotlib ipykernel numba  pytest toml cython scipy namaster -c conda-forge\nWe can also set that path for conda to automatically search into, this will pickup also future Conda environments on the same path:\nconda config --append envs_dirs /global/common/software/$GROUP/$USER/conda\nMoreover, this will be the default location for new environments, so we will be able to create other environments just with:\nmamba create --name onlypython python==3.11\nAnd they will be automatically created in “Global Common Software”.\nWe do not want that long path in our prompt, so (this is not necessary if you created the environment with --name):\nconda config --set env_prompt '({name}) '\nSo we can activate the environment specifying only the name:\nconda activate $ENV\nIn order to use it also on Jupyter@NERSC you will need to register the kernel:\nipython kernel install --name $ENV --user\nTip for CMB people, make sure you build healpy from source to get the best performance on Spherical Harmonics Transforms:\nCC=gcc CXX=g++ CFLAGS=\"-fPIC -O3 -march=native\" CXXFLAGS=\"-fPIC -O3 -march=native\" pip3 install --user --no-binary healpy --ignore-installed healpy"
  },
  {
    "objectID": "posts/2025-03-03-paper-pysm-models.html",
    "href": "posts/2025-03-03-paper-pysm-models.html",
    "title": "New Paper on Arxiv about Models of Galactic Emission in the Microwaves for CMB Experiments",
    "section": "",
    "text": "A new paper has been published on Arxiv discussing models of Galactic emission in the microwaves for Cosmic Microwave Background (CMB) experiments:\nNew Paper on Arxiv\nThe models are available publicly through the PySM3 (Python Sky Model) software package:\nPySM3 Documentation\nMoreover, the CMB-S4 data portal has simulations of the Galactic emission using PySM3 for several past, current and future CMB experiments, see:\nCMB-S4 Data Portal\nFor more information, you can also check out my older post about the PEARC paper:\nPEARC Paper Post"
  },
  {
    "objectID": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html",
    "href": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html",
    "title": "Ubuntu 22.04 Minimal on Jetstream with GPU Support",
    "section": "",
    "text": "Just notes on how to create a Ubuntu 22.04 image with GPU support\nFirst create an image with Ubuntu Minimal from Horizon\nAssociate a floating IP and save it in an environment variable we will use later:\nThe default kernel of Ubuntu minimal does not work with the Nvidia GPU proprietary driver, it gives the same error detailed in this issue\nTherefore we need to install another kernel with the headers, this occupies more than 2 GB because it grabs many other libraries:"
  },
  {
    "objectID": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#install-the-gpu-driver",
    "href": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#install-the-gpu-driver",
    "title": "Ubuntu 22.04 Minimal on Jetstream with GPU Support",
    "section": "Install the GPU driver",
    "text": "Install the GPU driver\nWe can use the Ansible recipe created by the Jetstream team. I created a branch which only installs the GPU driver.\ngit clone --single-branch --branch ubuntu_minimal_gpu https://gitlab.com/zonca/jetstream-image-build-pipeline.git\nDue to licensing issues, the NVIDIA GPU driver for Jetstream can only be accessed by members of the Jetstream team. The files need to be copied to the nvidia_driver subfolder\nExecute run.sh"
  },
  {
    "objectID": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#check",
    "href": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#check",
    "title": "Ubuntu 22.04 Minimal on Jetstream with GPU Support",
    "section": "Check",
    "text": "Check\nNo need to reboot again, SSH into the instance, check:\nnvidia-smi\nTue May 14 01:15:22 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  GRID A100X-8C                  On  | 00000000:00:06.0 Off |                    0 |\n| N/A   N/A    P0              N/A /  N/A |      1MiB /  8192MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#disk-usage",
    "href": "posts/2024-05-13-ubuntu22-minimal-gpu-image-jetstream.html#disk-usage",
    "title": "Ubuntu 22.04 Minimal on Jetstream with GPU Support",
    "section": "Disk usage",
    "text": "Disk usage\nJust installing the kernel brings more than 2 GB of packages\ndf -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        58G  4.7G   54G   9% /\ntmpfs           7.4G     0  7.4G   0% /dev/shm\ntmpfs           3.0G  704K  3.0G   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\ntmpfs           1.5G  4.0K  1.5G   1% /run/user/1000"
  },
  {
    "objectID": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html",
    "href": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html",
    "title": "PEARC24 Paper Cheap and FAIR Data Portal",
    "section": "",
    "text": "In this blog post, we summarize the key points from our recent paper presented at PEARC 2024, titled “Cheap and FAIR: A Serverless Research Data Repository for the Next Generation Cosmic Microwave Background Experiment.”"
  },
  {
    "objectID": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#introduction",
    "href": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#introduction",
    "title": "PEARC24 Paper Cheap and FAIR Data Portal",
    "section": "Introduction",
    "text": "Introduction\nThe paper discusses the development and implementation of a cost-effective and equitable data portal designed to facilitate data sharing and collaboration among researchers involved in the CMB-S4 experiment. CMB-S4 is a large-scale scientific experiment jointly funded by NSF and DOE to measure radiation emitted just after the Big Bang."
  },
  {
    "objectID": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#key-features",
    "href": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#key-features",
    "title": "PEARC24 Paper Cheap and FAIR Data Portal",
    "section": "Key Features",
    "text": "Key Features\n\nCost-Effective: The portal leverages open-source technologies to minimize costs.\nEquitable Access: Ensures fair access to data for all researchers, regardless of their institutional affiliation or funding status.\nUser-Friendly Interface: Designed with a focus on ease of use to encourage widespread adoption."
  },
  {
    "objectID": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#implementation",
    "href": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#implementation",
    "title": "PEARC24 Paper Cheap and FAIR Data Portal",
    "section": "Implementation",
    "text": "Implementation\nThe portal is built using a combination of modern web technologies and cloud infrastructure, ensuring scalability and reliability. Key components include:\n\nBackend: Powered by Globus for authentication, authorization, and data access.\nFrontend: A static web interface for browsing available datasets and their metadata.\nSecurity: Implements stringent security measures to protect sensitive data using Globus groups."
  },
  {
    "objectID": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#conclusion",
    "href": "posts/2024-07-31-pearc-paper-cheapandfair-data-portal.html#conclusion",
    "title": "PEARC24 Paper Cheap and FAIR Data Portal",
    "section": "Conclusion",
    "text": "Conclusion\nThe Cheap and FAIR Data Portal represents a significant step forward in democratizing access to research data. By reducing costs and ensuring equitable access, it aims to foster greater collaboration and innovation in the research community.\nFor more details, you can read the full paper here.\n\n\n\nCheap and FAIR Data Portal"
  },
  {
    "objectID": "posts/2024-10-09-jetstream-manila-mount.html",
    "href": "posts/2024-10-09-jetstream-manila-mount.html",
    "title": "Generate script to mount a Manila share",
    "section": "",
    "text": "Jetstream 2 makes creating volumes that are shared across instances as easy as creating standard volumes. This is provided by the Manila service, which creates volumes based on the Ceph file system that can be mounted simultaneously on multiple Jetstream 2 Virtual Machines.\nExosphere, the Jetstream 2 user friendly UI, is able to create and manage shares, and has the convenient feature that it prints out a command that can be executed in a Jetstream virtual machines to mount a Manila share.\nThis functionality could be useful also outside of Exosphere, for example if we are creating Manila shares programmatically via the Jetstream API.\nTherefore I have created a script that generates the same command.\nThe only requirement is the Openstack Manila client:\nI tested with python-manilaclient-5.0.0\nWe assume we have already created both the Manila share and an access rule (the access rule is created automatically by Exosphere, it needs to be explicitely created if using the CLI).\nThe script generate_mount_command.sh is available in the usual Github repository under manila/.\nIt is executed from a client with Openstack-CLI installed with:\nThis will generate a command of the form:\nThis can be executed at the command line in a Jetstream 2 VM, which does not require any access to the Openstack API."
  },
  {
    "objectID": "posts/2024-10-09-jetstream-manila-mount.html#how-to-mount-on-an-external-machine",
    "href": "posts/2024-10-09-jetstream-manila-mount.html#how-to-mount-on-an-external-machine",
    "title": "Generate script to mount a Manila share",
    "section": "How to mount on an external machine",
    "text": "How to mount on an external machine\nFIXME: this is not supported, Manila shares can only be mounted on Jetstream 2 instances.\nAll Jetstream images have the ceph-common package installed, for other machines, follow the instructions on the Quincy release.\nFor Ubuntu 22:\nwget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add -\ncodename=jammy\nsudo apt-add-repository \"deb https://download.ceph.com/debian-quincy/ ${codename} main\"\nsudo apt install ceph-common"
  },
  {
    "objectID": "posts/2024-08-04-cmb-data-licensing.html",
    "href": "posts/2024-08-04-cmb-data-licensing.html",
    "title": "Cosmic Microwave Background data licensing",
    "section": "",
    "text": "Some thoughts on how to license and attribute data products from the Simons Observatory.\nPublished data products, like papers and software, should have a license, this is for example required by the FAIR Principles.\nWe should most certainly choose a license which protects Attribution in order for the SO Collaboration to get credit. Then, we need to decide what is the best strategy to implement this Attribution.\nBelow we first show how other institution are handling this, then provide some options for the license and some options for implementing the Attribution guaranteed by those licenses."
  },
  {
    "objectID": "posts/2024-08-04-cmb-data-licensing.html#summary",
    "href": "posts/2024-08-04-cmb-data-licensing.html#summary",
    "title": "Cosmic Microwave Background data licensing",
    "section": "Summary",
    "text": "Summary\nWe suggest applying a default CC BY 4.0 license to all public SO data products, and tagging them with a DOI produced quarterly that apportions credit to current and past members of the collaboration."
  },
  {
    "objectID": "posts/2024-08-04-cmb-data-licensing.html#what-other-institutionsdata-repositories-are-doing",
    "href": "posts/2024-08-04-cmb-data-licensing.html#what-other-institutionsdata-repositories-are-doing",
    "title": "Cosmic Microwave Background data licensing",
    "section": "What other institutions/data repositories are doing",
    "text": "What other institutions/data repositories are doing\n\nESA Planck data (not specific to Planck but the entire archive): https://www.cosmos.esa.int/web/esdc/terms-and-conditions , they use CC BY-NC 3.0 IGO which is a modification of CC-BY non-attribution specific for InterGovermental organizations.\n\nNSF Guidelines https://new.nsf.gov/public-access#policies, “In 2022, the White House Office of Science and Technology Policy (OSTP) mandated that agencies undertake new plans to ensure that by 2025 peer-reviewed publications and associated data arising from federally funded research be made immediately and freely available upon date of publication. “ but they don’t specify a license\n\nLAMBDA: No mention of any license of the data they host: https://lambda.gsfc.nasa.gov/contact/\n\nFAIR: For “Reusability” it requires a license, mentions MIT and CC https://www.go-fair.org/fair-principles/r1-1-metadata-released-clear-accessible-data-usage-license/"
  },
  {
    "objectID": "posts/2024-08-04-cmb-data-licensing.html#available-license-options-for-datasets",
    "href": "posts/2024-08-04-cmb-data-licensing.html#available-license-options-for-datasets",
    "title": "Cosmic Microwave Background data licensing",
    "section": "Available license options for datasets",
    "text": "Available license options for datasets\nA data license tells downstream users both how they can use the data and how they should ensure appropriate attribution. Notably, unlicensed data is considered proprietary, with the copyright held by the original producer. It cannot, strictly, be used without specific permission.\n\nCreative Commons: CC {BY}-{NC}-{SA} 4.0 https://creativecommons.org/licenses/by-nc-sa/4.0/. This multi-faceted license contains a number of (potential) provisions:\n\nYou are free to share and adapt the data, if and only if:\n\nYou provide appropriate attribution (optional)\n\nYou only use it for non-commercial purposes (optional)\n\nYou must distribute your re-mixed material under the same license (optional)\n\n\nPerhaps the most appropriate license here is CC BY 4.0; i.e only requiring attribution and not restricting commercial use or remixes.\n\nWe could also consider the non-commercial clause, with an ability to dual-license (i.e. if a commercial enterprise would like to use the data we can create a license specifically for them to use it).\n\n\n\nOpen Data Commons, similar to CC, less popular but designed for data: ODC-By https://opendatacommons.org/licenses/by/1-0/"
  },
  {
    "objectID": "posts/2024-08-04-cmb-data-licensing.html#attribution",
    "href": "posts/2024-08-04-cmb-data-licensing.html#attribution",
    "title": "Cosmic Microwave Background data licensing",
    "section": "Attribution",
    "text": "Attribution\nWe would like people that use a public dataset to be able to properly attribute the work to the SO Collaboration. If there is a related paper, clearly this should be cited, but this does not necessarily appropriately credit everyone who worked on the infrastructure for producing this open data.\n\nCite a paper\nTraditionally attribution is achieved by asking people to cite a specific paper, while this works well with Academia rewarding paper citations, if anyone joins the collaboration after publication of the referenced paper, they would not receive proper recognition. Moreover, some data products do not have a specific paper to refer to. Finally, given the large quantity of data that (A)SO plans to release on short timescales, there may not be a specific paper that is relevant for a particular data product.\nPublishing a paper takes a lot of time, especially in a large collaboration which does a first round of internal review, therefore an author list is often obsolete by the time the paper is out. Moreover, papers on a specific topic might be spaced by 1 or 2 years and all data products published between those releases would reference an obsolete author list.\n\n\nDOI for each dataset\nThis is how Zenodo or Figshare work, each dataset (not necessarily each data product, but each data release) is assigned a DOI. This works well for identifying the data source but (possibly) dilutes citations. It depends a lot on the data release cycle, if data releases are rare, then this could work, each dataset has as authors the people contributing to that release. Finally, assigning each data product a DOI may be expensive and wasteful (‘properly’ tracked DOIs are ~$1 to mint).\n\n\nVersioned DOI for the SO collaboration\nDOI can be versioned, so they allow to have a single DOI that is a general reference to all the versions, and then any number of DOIs for each specific version of the product.\nIn our case we would create a main DOI mostly to make it easier to find all the versions, but then whenever we release a data product, we tag it with the most recent version of the DOI.\nA version of the DOI is generated every quarter (or other timescale) and tracks all current members of the SO Collaboration and those with appropriate status at that time. The actual “paper” referenced by the DOI could just be author list + institutions. This is the most flexible and easy to implement strategy."
  },
  {
    "objectID": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html",
    "href": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html",
    "title": "Ubuntu 22.04 Minimal on Jetstream",
    "section": "",
    "text": "Update July 2024: Ubuntu 22.04 Minimal image with Generic Kernel\nVirtual Machine images provided by the Jetstream team are fully fledged and therefore quite large.\nJulien Chastang proposed the shift to a minimalist distribution to save disk space, resources and shorten deployment time, see the relevant issue discussion on the Jetstream Gitlab organization.\nWe are mostly interested in replacing Ubuntu as the default image for our Kubespray developments, so we are looking only at OS supported by Kubespray\nI previously tested the container-specific Flatcar image, now testing Ubuntu 22.04 Minimal."
  },
  {
    "objectID": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html#ubuntu-22.04-minimal-cloud-image",
    "href": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html#ubuntu-22.04-minimal-cloud-image",
    "title": "Ubuntu 22.04 Minimal on Jetstream",
    "section": "Ubuntu 22.04 Minimal cloud image",
    "text": "Ubuntu 22.04 Minimal cloud image\nBest features:\n\nMinimalist, image is less than 200 MB\nIt’s Ubuntu, everyone is familiar with it\nWide support (for example GPU virtualization used in Jetstream should work)\nAutomatic updates\n\nDownloaded from https://cloud-images.ubuntu.com/minimal/releases/jammy/release/\nThe name of the image is Ubuntu2204Minimal, it is set as a Community image (script to load image to Openstack:\nopenstack image list --community | grep Minimal\n| 0fa9f3b4-d29f-4f68-a8a7-16bf44ffae69 | Ubuntu2204Minimal                                   | active      |"
  },
  {
    "objectID": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html#ubuntu-minimal-version-with-generic-kernel",
    "href": "posts/2024-05-07-ubuntu22-minimal-image-jetstream.html#ubuntu-minimal-version-with-generic-kernel",
    "title": "Ubuntu 22.04 Minimal on Jetstream",
    "section": "Ubuntu minimal version with Generic kernel",
    "text": "Ubuntu minimal version with Generic kernel\nBy default Ubuntu Minimal ships with the kvm kernel which is a smaller kernel, it does not offer some modules, for example nfsd is not available, this prevents running NFS servers on the node or even in containers inside the node.\nTo circumvent this issue, I created a new version of the Minimal image with the only modification being the generic kernel instead of the kvm one.\nThe name of the image is Ubuntu2204MinimalGenKernel, ID 671f03f1-4d86-4363-b03c-f5d54818693a\nSee https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/80\nThis image a bit larger image:\nFilesystem      Size  Used Avail Use% Mounted on\ntmpfs           297M  664K  297M   1% /run\n/dev/sda1        20G  2.7G   17G  14% /\ntmpfs           1.5G     0  1.5G   0% /dev/shm\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\ntmpfs           297M  4.0K  297M   1% /run/user/1000\n\nGPU support\nWhile Ubuntu potentially supports GPUs, the default image from Ubuntu does not. We need some specific customization on the image.\nTherefore we would need to create a GPU-specific minimal version that also includes the GPU drivers.\nSee the dedicated tutorial about GPU support.\n\n\nExosphere\nThe instance boots correctly on Exosphere, I can ssh into the instance as exouser.\nIt looks like Exosphere installs another 1.4GB of packages, so disk usage is higher compared to Horizon, anyway still tiny compared to a full-featured Ubuntu.\ndf -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        20G  2.5G   17G  13% /\ntmpfs           1.5G     0  1.5G   0% /dev/shm\ntmpfs           600M  672K  599M   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\ntmpfs           300M  4.0K  300M   1% /run/user/1000\n\n\nHorizon\nAll normal, boots fine, usual login with the ubuntu user and the Openstack keypair.\nOccupies minimal space:\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        20G  1.1G   19G   6% /\ntmpfs           1.5G     0  1.5G   0% /dev/shm\ntmpfs           600M  460K  599M   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\ntmpfs           300M  4.0K  300M   1% /run/user/1000\nNo warnings in dmesg.\n\n\nKubespray\nThe real objective of this test is to deploy Kubernetes on Jetstream with this smaller image, following the latest tutorial with minor modifications.\nWe can replace the official Ubuntu with the minimal image on Terraform by modifying in cluster.tfvars:\nimage = \"Ubuntu2204Minimal\"\nWith the minimal image, Kubespray gives the error:\nmodprobe: FATAL: Module ip_vs_sh not found in directory /lib/modules/5.15.0-1058-kvm\\n\"\nthe simple workaround is to set:\nkube_proxy_mode: iptables\ninstead of\nkube_proxy_mode: ipvs\nin inventory/kubejetstream/group_vars/k8s_cluster/k8s-cluster.yml\nCPU Nodes are fine, but GPU nodes as expected do not work.\nk get nodes -o wide\nNAME                              STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP      OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME\nkubejetstream-1                   Ready      control-plane   20m   v1.25.6   10.1.90.167   149.165.174.34   Ubuntu 22.04.4 LTS   5.15.0-1058-kvm   containerd://1.6.15\nkubejetstream-k8s-node-nf-cpu-1   Ready      &lt;none&gt;          18m   v1.25.6   10.1.90.186   &lt;none&gt;           Ubuntu 22.04.4 LTS   5.15.0-1058-kvm   containerd://1.6.15\nkubejetstream-k8s-node-nf-cpu-2   Ready      &lt;none&gt;          18m   v1.25.6   10.1.90.5     &lt;none&gt;           Ubuntu 22.04.4 LTS   5.15.0-1058-kvm   containerd://1.6.15\nkubejetstream-k8s-node-nf-gpu-1   NotReady   &lt;none&gt;          18m   v1.25.6   10.1.90.230   &lt;none&gt;           Ubuntu 22.04.4 LTS   5.15.0-1058-kvm   containerd://1.6.15\nDid not feel it was necessary to test JupyterHub, I’m confident it will work.\nLet’s review the disk space occupied on a m3.medium worker with Ubuntu Minimal before running Ansible:\nubuntu@kubejetstream-k8s-node-1:~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        58G  831M   58G   2% /\ntmpfs            15G     0   15G   0% /dev/shm\ntmpfs           5.9G  484K  5.9G   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\ntmpfs           3.0G  4.0K  3.0G   1% /run/user/1000\nand after Kubernetes has being installed (from 800 MB to 6.8 GB):\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/root        58G  6.8G   52G  12% /\ntmpfs            15G     0   15G   0% /dev/shm\ntmpfs           5.9G  2.1M  5.9G   1% /run\ntmpfs           5.0M     0  5.0M   0% /run/lock\n/dev/sda15      105M  6.1M   99M   6% /boot/efi\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/1a777fcf86f2e982969b83f78302bb19427470995262032dabe0384af26d202b/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/1331d819ef46d1d4ee94cd1faa58c0965e6c63b25d5fd69bb86e5b1769c9041e/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/6c6a245269c66d8a7ba69d31e590e9e807d17d19acfe9d2a251daa6b73127b78/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/2ed3c7de52d4c63e3a4df8e00486f4fcde91634891708e88aaf506256c2637c7/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/03b1e64d75fd951f7aefa050746717baa3811636636254132d4d11758c1a239c/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/bda38bc2b547cd4bdeca1aa7653fdf6483da2fbb8b69abbda2a35a3d93b982a8/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/3288593c52f5a21fec4be1222326d22f65612c61135e289addab1fe11b423334/shm\nshm              64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/e9fd081509ca17913118ae4c69a80dc7cd50ea3a14469a90bc7e63ff5ff36c6f/shm\ntmpfs           3.0G  4.0K  3.0G   1% /run/user/1000\nHere is instead the comparison with the default Jetstream Ubuntu 22 image:\nFilesystem                                                                                                                                                                                       Size  Used Avail Use% Mounted on\nudev                                                                                                                                                                                              15G     0   15G   0% /dev\ntmpfs                                                                                                                                                                                            3.0G  1.2M  3.0G   1% /run\n/dev/sda1                                                                                                                                                                                         58G  8.8G   50G  16% /\ntmpfs                                                                                                                                                                                             15G     0   15G   0% /dev/shm\ntmpfs                                                                                                                                                                                            5.0M     0  5.0M   0% /run/lock\ntmpfs                                                                                                                                                                                             15G     0   15G   0% /sys/fs/cgroup\n/dev/loop0                                                                                                                                                                                        64M   64M     0 100% /snap/core20/2264\n/dev/loop1                                                                                                                                                                                        92M   92M     0 100% /snap/lxd/24061\n/dev/sda15                                                                                                                                                                                       105M  6.1M   99M   6% /boot/efi\n/dev/loop2                                                                                                                                                                                        39M   39M     0 100% /snap/snapd/21465\ntmpfs                                                                                                                                                                                            3.0G  4.0K  3.0G   1% /run/user/1000\nxxxxxxxxxxxx          9.8T  493G  9.3T   5% /software\nAfter having installed Kubernetes (from 8.8 GB to 13):\n                      Size  Used Avail Use% Mounted on\nudev                                                                                                                                                                                              15G     0   15G   0% /dev\ntmpfs                                                                                                                                                                                            3.0G  3.0M  3.0G   1% /run\n/dev/sda1                                                                                                                                                                                         58G   13G   46G  23% /\ntmpfs                                                                                                                                                                                             15G     0   15G   0% /dev/shm\ntmpfs                                                                                                                                                                                            5.0M     0  5.0M   0% /run/lock\ntmpfs                                                                                                                                                                                             15G     0   15G   0% /sys/fs/cgroup\n/dev/loop0                                                                                                                                                                                        64M   64M     0 100% /snap/core20/2264\n/dev/loop1                                                                                                                                                                                        92M   92M     0 100% /snap/lxd/24061\n/dev/sda15                                                                                                                                                                                       105M  6.1M   99M   6% /boot/efi\n/dev/loop2                                                                                                                                                                                        39M   39M     0 100% /snap/snapd/21465\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/c0fc748f6469ded859983f821e8f167691b52a529bdc5e9a1269646ab1d1466f/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/5c853382c912e5cddd3dc6c726bce09fdbec753a89b60b6cf334214c34216cbc/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/1d5f384f212ecf62aafa44e8cd1dbabf5bc0fa28aeb10759da9e372494d82661/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/7d8ab2baa7b00523538b0921b0fc6c9d6aee1d121db8752fdb38bdce877784b7/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/1669d88641a2eafd7dc3c841e9f71ef6014d61bd986bc527e899b2b972837279/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/7d0ded66f7e811fdb0fee58dba737494aca1e36510f8291d7ebca5f5a3ad9824/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/4ec65ad53b8ab3391a04cc2442fa3278b114fbe4791ca1b24cc7db213be8f896/shm\nshm                                                                                                                                                                                               64M     0   64M   0% /run/containerd/io.containerd.grpc.v1.cri/sandboxes/2bb33f80ec8afc1639dd57094dbbd9c59e97640cb10df557f68d3bd8582a10f1/shm\ntmpfs                                                                                                                                                                                            3.0G  4.0K  3.0G   1% /run/user/1000\nxxxxxxxxxxxx          9.8T  493G  9.3T   5% /software"
  },
  {
    "objectID": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html",
    "href": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html",
    "title": "Use Jetstream’s DeepSeek R1 as a code assistant on JupyterAI",
    "section": "",
    "text": "Thanks to Openrouter, there is now a way of using the Jetstream LLM inference system, in particular the powerful Deepseek R1 model, as a code and documentation assistant in JupyterLab via JupyterAI."
  },
  {
    "objectID": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#access-the-jetstream-llm-service-from-your-machine",
    "href": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#access-the-jetstream-llm-service-from-your-machine",
    "title": "Use Jetstream’s DeepSeek R1 as a code assistant on JupyterAI",
    "section": "Access the Jetstream LLM service from your machine",
    "text": "Access the Jetstream LLM service from your machine\nFollow the Jetstream docs to configure local access to the service, below I copied the instrutions for a Linux (Ubuntu) machine:\nsudo apt install sshuttle\nI assume below you have .ssh/config already setup for passwordless login to a Jetstream VM:\nsshuttle -r jsvm 149.165.157.253/32\nNow you can open your browser and check you can access the chat web UI at:\n&lt;https://llm.jetstream-cloud.org/&gt;\nNotice that if you are using a Chromebook with the Linux environment, this only works from a browser installed within the Ubuntu environment (e.g. firefox), not with Chrome."
  },
  {
    "objectID": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#test-access-from-langchain",
    "href": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#test-access-from-langchain",
    "title": "Use Jetstream’s DeepSeek R1 as a code assistant on JupyterAI",
    "section": "Test access from LangChain",
    "text": "Test access from LangChain\nCreate env for JupyterAI and langchain-openai (needed to support OpenRouter):\npip install jupyter-ai langchain-openai\nFind the model id pointing the browser to (sshuttle needs to be running for this to work):\nhttps://llm.jetstream-cloud.org/sglang/v1/models\nCurrently it is DeepSeek-R1.\nOr, for the vLLM based models:\nhttps://llm.jetstream-cloud.org/vllm/v1/models\nNow you can test this is working first inside a Notebook:\nfrom openai import OpenAI\n\nopenai_api_key=\"EMPTY\"\nopenai_api_base=\"https://llm.jetstream-cloud.org/sglang/v1/\"\n\nclient = OpenAI(\n    api_key=openai_api_key, base_url=openai_api_base)\n\ncompletion=client.completions.create(model=\"Deepseek-R1\", prompt=\"Jetstream at Indiana University is a\")\ncompletion"
  },
  {
    "objectID": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#configure-jupyter-ai-to-use-deepseek",
    "href": "posts/2025-02-04-jetstream-llm-service-deepseek-jupyterai.html#configure-jupyter-ai-to-use-deepseek",
    "title": "Use Jetstream’s DeepSeek R1 as a code assistant on JupyterAI",
    "section": "Configure Jupyter AI to use Deepseek",
    "text": "Configure Jupyter AI to use Deepseek\n\nPatch for openrouter_api_key keyword not accepted error\nThis is now fixed in jupyter_ai 2.29.1, skip this step\nIf you still find this error, make sure you update jupyter_ai to 2.29.1 or newer version.\nAt the moment, with jupyter_ai==2.29.0 and langchain-openai==0.3.3, it seems like Lanchain does not accept the API key passed by JupyterAI, so we need to remove it. Hopefully this won’t be necessary in the future.\nUnfortunately at the moment this requires a really harmless tiny “patch” (ok, it’s more of a hack than a patch), in the file (adapt for your system):\n~.virtualenvs/jupyterai/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\nAdd the pop line just before the create line which should be around line 890 in the file:\npayload.pop(\"openrouter_api_key\")\nresponse = await self.async_client.create(**payload)\n\n\nJupyterAI settings\nOpen the JupyterAI settings, and configure:\n\nCompletion model = OpenRouter :: *\nLocal model ID = Deepseek R1 or whatever model id you get in the previous step\nAPI Base url = https://llm.jetstream-cloud.org/sglang/v1/ for Deepseek or https://llm.jetstream-cloud.org/vllm/v1/ for Llama.\nOPENROUTER_API_KEY = “EMPTY”\n\nNext we can test in the chat box (oh how Deepseek loves to talk!):\n\n\n\nDeepseek JupyterAI"
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "",
    "text": "In a production Kubernetes deployment it is necessary to make it easier to monitor the status of the cluster effectively. Kubernetes provides Prometheus to gather data from the different components of Kubernetes and Grafana to access those data and provide real-time plotting and inspection capability. Moreover, they both provide systems to send alerts in case some conditions on the state of the cluster are met, i.e. using more than 90% of RAM or CPU.\nThe only downside is that the pods that handle monitoring consume some resource themselves, so this could be significant for small clusters below 5 nodes or so, but shouldn’t be a problem for typical larger production deployments.\nBoth Prometheus and Grafana can be installed separately with Helm recipes or using the Prometheus operator Helm recipe, however those deployments do not have any preconfigured dashboards, it is easier to get started thanks to the kube-prometheus project, which not only installs Prometheus and Grafana, but also preconfigures about 10 different Grafana dashboards to explore in depth the status of a Kubernetes cluster.\nThe main issue is that customizing it is really complicated, it requires modifying jsonnet templates and recompiling them with a jsonnet builder which requires go, however I don’t foresee the need to do that for most users.\nFirst check the Kubernetes compatibility matrix to check what version of Kube-Prometheus we should use, for the Jetstream 2 deployment via Kubespray as of October 2024 we deploy Kubernetes 1.25, therefore we need 0.12.\nUnfortunately it is not based on Helm, so you need to first checkout the repository, I have a fork with a minor modification to export the JupyterHub-related pod labels:\nThis is release-0.12 branch with minor modifications focused on exporting the JupyterHub-related pod labels\nand then follow the instructions in the documentation, copied here for convenience:\nThis creates several pods in the monitoring namespace:\nThen you can setup forwarding on your laptop to export grafana locally:\nAccess localhost:3000 with your browser and you should be able to navigate through all the statistics of your cluster, see for example this screenshot. The credentials are user admin and password admin.\nFrom the “Home” page, you can access all the preconfigured dashboards by clicking on the top “Home” button, it will show a searchable list of all available dashboards."
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#access-the-ui-from-a-different-machine",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#access-the-ui-from-a-different-machine",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Access the UI from a different machine",
    "text": "Access the UI from a different machine\nIn case you are running the configuration on a remote server and you would like to access the Grafana UI (or any other service) from your laptop, you can install kubectl also on your laptop, then copy the .kube/config to the laptop with:\n scp -r KUBECTLMACHINE:~/.kube/config ~/.kube\nand run:\n ssh ubuntu@$IP -f -L 6443:localhost:6443 sleep 3h &\nfrom the laptop and then run the port-forward command locally on the laptop."
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#monitor-jupyterhub-with-the-default-dashboards",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#monitor-jupyterhub-with-the-default-dashboards",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Monitor JupyterHub with the default dashboards",
    "text": "Monitor JupyterHub with the default dashboards\nOnce we have deployed JupyterHub with Helm, we can pull up the “namespace” monitor and select the jhub namespace to visualize resource usage but also usage requests and limits of all pods created by JupyterHub and its users. See a screenshot below.\n\n\n\nScreenshot of the Grafana namespace UI"
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#monitor-jupyterhub-with-custom-dashboards",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#monitor-jupyterhub-with-custom-dashboards",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Monitor JupyterHub with custom dashboards",
    "text": "Monitor JupyterHub with custom dashboards\nThe fork of Kube-Prometheus exports JupyterHub-related labels, so we can create custom dashboards to monitor JupyterHub more in depth.\nFor example we can track how many Notebook sessions are used in JupyterHub.\nIt is best to create a new dashboard, becase we cannot mix panels generated programmatically by Kube-Prometheus with Panels created by the Grafana UI itself.\nWe can create a new dashboard and add a new Timeseries panel with the following query:\nsum(kube_pod_labels{label_component=\"singleuser-server\"}) \nThis will show the number of singleuser servers running in the jhub namespace.\nThe problem is that by default Prometheus retains metrics only for 10 days by default, we can modify this in manifests/prometheus-prometheus.yaml.\nIn the branch release-0.12-jupyterhub-labels I have already modified the Prometheus resource to retain metrics for 365 days, you can modify the retention field to change this. Moreover, the same manifest also creates a 50 GB Persistent Volume Claim for each of the Prometheus pods, you can modify the storage field to change this."
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#setup-alerts",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#setup-alerts",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Setup alerts",
    "text": "Setup alerts\nWarning: the “Setup alerts” section needs to be updated\nGrafana supports email alerts, but it needs a SMTP server, and it is not easy to setup and to avoid being filtered as spam. The easiest way is to setup an alert to Slack, and optionally be notified via email of Slack messages.\nFollow the instructions for slack on the Grafana documentation\n\nCreate a Slack app, name it e.g. Grafana\nAdd feature “Incoming webhook”\nCreate a incoming webhook in the workspace and channel your prefer on Slack\nIn the Grafana Alerting menu, set the webhook incoming url, the channel name\n\n\n\n\nScreenshot of the Grafana slack notification"
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#configure-a-domain",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#configure-a-domain",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Configure a domain",
    "text": "Configure a domain\nWe need to serve both JupyterHub and Grafana from the same Jetstream instance, so we need 2 domains, Jetstream automatically creates a domain based on the name of the instance and the allocation, typically:\nkubejetstream-1.xxx000000.projects.jetstream-cloud.org\nwhere xxx000000 is the allocation code.\nWe can create a new DNS record with the Openstack-cli, the best is to use a CNAME, so that if we redeploy and the VM changes address, the record would still be valid, the command is (the . at the end of the records is intentional):\nopenstack recordset create xxx000000.projects.jetstream-cloud.org. grafana --type CNAME --record 'kubejetstream-1.xxx000000.projects.jetstream-cloud.org.'\nNow we should be able to access https://grafana.xxx000000.projects.jetstream-cloud.org and get a nice 404 error from our ingress.\nAny other domain would work, as long as it points to the IP of the master node of Kubernetes. The proper way of deploying this would be through a Load Balancer, but I haven’t had much success in making that work on Jetstream 2 in the past."
  },
  {
    "objectID": "posts/2024-10-25-jetstream_kubernetes_grafana.html#configure-ingress",
    "href": "posts/2024-10-25-jetstream_kubernetes_grafana.html#configure-ingress",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Configure ingress",
    "text": "Configure ingress\nIt is also possible to expose Grafana to the web via an Ingress, see an example ingress. It is important that it is in the monitoring namespace.\nThe configuration also supports HTTPS, for that to work you also need to create an Issuer in the namespace monitoring (also rename the secret key), for more details see the tutorial on deploying letsencrypt"
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "",
    "text": "Work and post contributed by Ana Espinoza, only tested and benchmarked by Andrea Zonca."
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#summary",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#summary",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "Summary",
    "text": "Summary\nTo scale down:\nkubectl cordon &lt;node-name&gt;\nkubectl drain --ignore-daemonsets &lt;node-name&gt; # --delete-emptydir-data # &lt;-- Flag may be necessary\nopenstack server shelve &lt;node-name&gt;\nTo scale back up:\nopenstack server unshelve &lt;node-name&gt;\nkubectl uncordon &lt;node-name&gt;"
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#the-problem",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#the-problem",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "The Problem",
    "text": "The Problem\nSometimes,a JupyterHub cluster deployed on top of Kubernetes will receive heavy intermittent use. For example, a cluster may be deployed with enough nodes–let’s say 10 m3.mediums–to serve a regularly held workshop, each with a large number of participants. However, between workshops, when partipants aren’t expected to be logging into the server, this many nodes are not necessary, and would be needlessly exhausting Jetstream Service Units (SUs)."
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#hard-scaling-the-cluster-the-slow-way",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#hard-scaling-the-cluster-the-slow-way",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "“Hard Scaling” the Cluster (the Slow Way)",
    "text": "“Hard Scaling” the Cluster (the Slow Way)\nTo conserve SUs, one might consider removing the extra nodes from the cluster then re-running Terraform with fewer nodes. When it’s time to run the workshop again, the reverse process is done: run Terraform to re-create the previously destroyed nodes, then running k8s_scale.sh to re-add these nodes to the still existing cluster.\nThis however, can be a lengthy process as this involves reinstalling Kubernetes dependencies on the freshly recreated nodes."
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#soft-scaling-the-cluster-the-fast-way",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#soft-scaling-the-cluster-the-fast-way",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "“Soft Scaling” the Cluster (the Fast Way)",
    "text": "“Soft Scaling” the Cluster (the Fast Way)\nInstead of destorying the excess nodes, we can soft scale the cluster, a technique first described here. The technique is simple. To downscale, you “cordon” and “drain” the excess nodes before finally shelving them via openstack, a VM state that conserves SU’s. When you’re ready to scale back up, you unshelve the nodes and uncordon them. The entire scaling process (both downscaling and upscaling) can take as little as 5 minutes.\n\nSoft Scaling Down\nFirst, cordon off the nodes you want to scale down. This will ensure that no new Pods can be scheduled on the node.\nkubectl get nodes # Run to find the node names\nkubectl cordon &lt;node-name&gt;\nNext, drain the node. This will remove any Pods currently running on that node. Kubernetes will reschedule these Pods on an available node.\nkubectl drain &lt;node-name&gt; --ignore-daemonsets\nYou may get an error that looks like the following:\nerror: unable to drain node \"&lt;node&gt;\" due to error:cannot delete Pods with local\nstorage (use --delete-emptydir-data to override):\nkube-system/csi-cinder-controllerplugin-xxxxxxxxx, continuing command...\nThis is Kubernetes warning you that the csi-cinder-controller-plugin Pod is storing some temporary data in an “emptyDir” volume mount. This temporary data is safe to delete and will be recreated once the Pod is rescheduled. Rerun the drain command with the --delete-emptydir-data flag.\nOnce the node has been successfully drained, you can run a kubectl get nodes and you should see the node status as Ready,SchedulingDisabled.\nNow run the shelve command:\nopenstack server shelve &lt;node-name&gt;\nOne final kubectl get nodes should reveal the status of the down scaled node as NotReady,SchedulingDisabled.\n\n\nSoft Scaling Back Up\nTo scale back up, we perform the tasks in reverse.\nUnshelve the node:\nopenstack server unshelve &lt;node-name&gt;\nUncordon the node:\nkubectl uncordon &lt;node-name&gt;\nYour node’s status should now be Ready,Schedulable.\n\n\nScaling Many Nodes at Once\nSince the node names for worker nodes created using Kubespray/Jetstream_Kubespray are of the form $CLUSTER-k8s-node-&lt;number&gt; or $CLUSTER-k8s-node-nf-&lt;number&gt;, we can use a bash for loop to soft scale multiple nodes at once.\nSee the k8s_softscale.sh script in the repository. It is recommended to execute one step at a time and verify that it executed to completion before rerunning the script uncommenting the next step."
  },
  {
    "objectID": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#benchmark",
    "href": "posts/2024-02-02-soft-scaling-kubernetes-jetstream.html#benchmark",
    "title": "Soft Scaling Kubernetes on Jetstream",
    "section": "Benchmark",
    "text": "Benchmark\nJust as a reference, here are time measurements I took executing the default configuration of the Kubernetes deployment (m3.medium instances both for master and for nodes), with no deployed services, just executed once:\n\nAnsible run for initial deployment of 1 master + 1 worker: 27m\nTerraform run to create other 9 workers: &lt; 1m\nAnsible run to scale the Kubernetes cluster to the 9 extra nodes: 43m\nCordon 9 nodes: 3s\nShelve 9 nodes: 3m\nUnshelve 9 nodes: &lt;5m\nUncordon 9 nodes: 3s\n\nThis technique is quite impressive, bring the time needed to scale a cluster from 1 to 10 nodes from 43m down to less than 5m."
  },
  {
    "objectID": "posts/2023-04-01-dreamhost-letsencrypt.html",
    "href": "posts/2023-04-01-dreamhost-letsencrypt.html",
    "title": "Get a Letsencrypt certificate for Dreamhost with certbot",
    "section": "",
    "text": "Dreamhost has discontinued offering free Letsecrypt certificated in their shared hosting.\nHowever, they support pasting a manually created certificate, so we can generate one on a different machine with the manual mode of certbot and then deploy it.\nThe issue that costed me 1 hour today is that certbot generated a ECDSA key by default instead of RSA, so here the script to generate the right key, just run:\nexport DOMAIN=www.yourdomain.com\nand the script:\n\nThen paste:\n\ncert.pem in certificate\nprivkey.pem in private key\nchain.pem in intermediate certificates"
  },
  {
    "objectID": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html",
    "href": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html",
    "title": "Unit conversion with broadband detectors looking at foregrounds",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pysm3 import units as u\nimport pysm3 as pysm\n%matplotlib inline\nnp.random.seed(4)\nnside = 128\ndip = hp.synfast([0,1], lmax=1, nside=nside) * u.V\n\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:438: FutureChangeWarning: The order of the input cl's will change in a future release.\nUse new=True keyword to start using the new order.\nSee documentation of healpy.synalm.\n  category=FutureChangeWarning,\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:824: UserWarning: Sigma is 0.000000 arcmin (0.000000 rad) \n  sigma * 60 * 180 / np.pi, sigma\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:829: UserWarning: -&gt; fwhm is 0.000000 arcmin\n  sigma * 60 * 180 / np.pi * (2.0 * np.sqrt(2.0 * np.log(2.0)))\nhp.mollview(dip, unit=dip.unit)\nWe measure the sky with out broadband instrument, we assume we only measure the CMB solar dipole, initially the units are arbitrary, for example Volts of our instrument.\nNext we calibrate on the solar dipole, which is known to be 3.3 mK.\ncalibration_factor = 2 * 3.3 * u.mK_CMB / (dip.max() - dip.min())\ncalibration_factor\n\n\\(3.486509 \\; \\mathrm{\\frac{mK_{{CMB}}}{V}}\\)\ntheta, phi = hp.pix2ang(nside, np.arange(hp.nside2npix(nside)))\ndust_amplitude_V = 2 * u.V\ndust_latitude = 20 * u.deg\ndust = dust_amplitude_V * np.logical_and(theta &gt; (90 * u.deg - dust_latitude).to_value(u.rad),  theta &lt; (90 * u.deg + dust_latitude).to_value(u.rad))\nhp.mollview(dip + dust, unit=dip.unit, title=\"CMB dipole + dust\")\ncalibrated_dip = calibration_factor * dip\nFor a delta frequency it is straightforward to compute the temperature of the dust in any unit:\ncalibrated_dust_amplitude = dust_amplitude_V * calibration_factor\ncalibrated_dust_amplitude\n\n\\(6.9730181 \\; \\mathrm{mK_{{CMB}}}\\)\nFirst we simplify and consider a delta-frequency instrument at 300 GHz\ncenter_frequency = 300 * u.GHz\ncalibrated_dust_amplitude.to(u.mK_RJ, equivalencies=u.cmb_equivalencies(center_frequency))\n\n\\(0.99846974 \\; \\mathrm{mK_{{RJ}}}\\)\ncalibrated_dust_amplitude.to(u.MJy/u.sr, equivalencies=u.cmb_equivalencies(center_frequency))\n\n\\(2.7608912 \\; \\mathrm{\\frac{MJy}{sr}}\\)"
  },
  {
    "objectID": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#broadband-instrument",
    "href": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#broadband-instrument",
    "title": "Unit conversion with broadband detectors looking at foregrounds",
    "section": "Broadband instrument",
    "text": "Broadband instrument\nNext we assume instead that we have a broadband instrument, of 20% bandwidth, with uniform response in power (Spectral radiance) in that range. For simplicity, we only take 4 points.\n\nfreq = [270, 290, 310, 330] * u.GHz\n\n\nweights = [1, 1, 1, 1]\n\n\nweights /= np.trapz(weights, freq)\n\n\nweights\n\n\\([0.016666667,~0.016666667,~0.016666667,~0.016666667] \\; \\mathrm{\\frac{1}{GHz}}\\)\n\n\nThe instrument bandpass is defined in power so we can transform our signal in MJy/sr at the 4 reference frequencies, then integrate."
  },
  {
    "objectID": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#dust-model",
    "href": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#dust-model",
    "title": "Unit conversion with broadband detectors looking at foregrounds",
    "section": "Dust model",
    "text": "Dust model\nLet’s assume for the dust a power-law model with a spectral index of 2 (more realistic models use a modified black body), i.e.:\n$I_{dust}()= A_{dust}(_0)( )^2 $\nin the case of a delta-bandpass, $A_{dust}(_0)$ would coincide with the measured value:\n\nA_dust_delta_bandpass = calibrated_dust_amplitude.to(u.MJy/u.sr, equivalencies=u.cmb_equivalencies(center_frequency))\n\n\nA_dust_delta_bandpass\n\n\\(2.7608912 \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\n${dust}()= A{dust}(_0)g() ( )^2 d$\n${dust}()= A{dust}(_0)g() ( )^2 d$\n\\(\\tilde{I}_{dust}(\\nu)[K_{CMB}] = \\dfrac{ A_{dust}(\\nu_0)\\left[\\frac{MJy}{sr}\\right] \\int g(\\nu) \\left( \\dfrac{\\nu}{\\nu_0} \\right)^2 d\\nu}  { \\int C_{K_{CMB}}^{Jy~sr^{-1}}(\\nu) g(\\nu) d\\nu}\\)\n\nI_dust_bandpass = A_dust_delta_bandpass * np.trapz(weights * (freq**2/center_frequency**2), freq)\n\n\nSR = u.MJy/u.sr\n\n\nSR_to_K_CMB = ((1*SR).to(u.mK_CMB, equivalencies=u.cmb_equivalencies(freq)))/(1*SR)\n\n\nSR_to_K_CMB\n\n\\([2.253665,~2.420677,~2.646055,~2.9379134] \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)\n\n\n\\(\\int C_{K_{CMB}}^{Jy~sr^{-1}}(\\nu) g(\\nu) d\\nu\\)\n\nSR_to_K_CMB_bandpassintegrated = np.trapz(1/SR_to_K_CMB * weights, freq)\n\n\nA_dust_bandpass = calibrated_dust_amplitude * SR_to_K_CMB_bandpassintegrated / np.trapz(weights * (freq**2/center_frequency**2), freq)\n\n\nA_dust_bandpass\n\n\\(2.7387177 \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\n\n(A_dust_bandpass / A_dust_delta_bandpass).to(u.pct)\n\n\\(99.196874 \\; \\mathrm{\\%}\\)"
  },
  {
    "objectID": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#crosscheck-starting-from-the-dust-model",
    "href": "posts/2020-09-04-unit-conversion-broadband-foregrounds.html#crosscheck-starting-from-the-dust-model",
    "title": "Unit conversion with broadband detectors looking at foregrounds",
    "section": "Crosscheck starting from the dust model",
    "text": "Crosscheck starting from the dust model\nintegrate the model over the bandpass in SR\n\nA_dust_bandpass * np.trapz(weights * (freq**2/center_frequency**2), freq)\n\n\\(2.7498755 \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\nConvert to \\(K_{CMB}\\), the conversion factor is tailored to the CMB, if we had a different calibration source, we would have different conversion factors:\n\n_ / SR_to_K_CMB_bandpassintegrated\n\n\\(6.9730181 \\; \\mathrm{mK_{{CMB}}}\\)"
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html",
    "href": "posts/2022-11-15-binderhub-jetstream2.html",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "This work has been supported by Indiana University and is cross-posted on the Jetstream 2 official documentation website.\nBinderHub is a kubernetes-based cloud service that allows users to share reproducible interactive computing environments from code repositories. See for example https://mybinder.org.\nWhen pointed to a Github repository, it builds a Docker container from the metadata in that repository (for example a requirements.txt file), then it gives the user a live Jupyter Notebook session with this custom computing environment and the checked-out repository. The user can then browse the repository and execute Notebooks.\nOn top of Kubernetes, it also needs a Container Registry to store the Docker containers it builds, in the following tutorial we rely on Google Cloud.\n\n\nThe first step is to have a Kubernetes deployment on Jetstream 2, for example deployed with Kubespray using this tutorial\n\n\n\nWe first need to activate the Google Container Registry in the Google Cloud account:\n\nGo to the API Library\nSearch for Google Container Registry API\nClick on Enable\nIt will ask to configure billing\nIt will ask for credit card information, but you will have $300 free credits for 90 days and the card won’t be charged unless you manually switch to a paid account\nGo to the Google Container registry settings, set the images to public, so that they can be pulled back from BinderHub to the container dedicated to execution\n\n\n\n\nFollow the instructions on “Zero to BinderHub” to setup the Google Container Registry some notes:\n\nBetter to create a dedicated project just for BinderHub\n“Create key” is under “Actions”-&gt;“Manage keys”\n\n\n\n\nOn Jetstream 2 we do not have a Load Balancer service like public clouds, therefore we need to use Ingress.\nFirst we need to setup Cert-Manager to provide HTTPS certificates via Letsencrypt.\nFollow the instructions at https://zonca.dev/2020/03/setup-https-kubernetes-letsencrypt.html to install a cluster issuer\n\n\n\nBinderHub requires 2 subdomains, one for BinderHub and one for JupyterHub. For the initial testing I recommend to use Jetstream 2 provided domains, once they are working it is easy to replace them with custom domains.\nLogin to the Jetstream 2 Horizon instance, choose the right allocation, click on DNS, Zones, choose the Zone which starts with tg, then click on “Create Record Set”.\nCreate a “A” record where name is binder.tg-xxx000000.projects.jetstream-cloud.org and record is the IP of your master instance. Create a second A record with the name hub.tg-xxx000000.projects.jetstream-cloud.org and the same record.\nThese should be ready to be used in a few minutes.\n\n\n\nCheckout the repository with the configuration files:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/\nEnter the binderhub folder.\nFollow the instructions on how to create just the secret.yaml file.\nThe repository provides a template named config_template.yaml to create config.yaml, this file contains all of the configuration items needed to setup the following items, see the links for the relevant documentation:\n\nBinderHub with Google Container Registry\nConfigure HTTPS ingress for BinderHub and JupyterHub\nUse Docker-in-Docker to build the containers, which is necessary because the Jetstream 2 Kubernetes deployment uses containerd.\n\nCreate a file named config_vars.sh in the same folder which defines your Jetstream allocation and Google Project IDs:\nexport ALLOCATION=xxx000000\nexport GOOGLE_PROJECT=binderhub-000000\nThen deploy BinderHub running:\nbash install_binderhub.sh\nThis script first creates config.yaml (and overwrites it, so keep your changes in config_template.yaml), then deploys or updates the deploy of BinderHub via Helm.\n\n\n\nConnect to https://binder.tg-xxx000000.projects.jetstream-cloud.org, you should see the binder login page:\n\n\n\nBinder login page\n\n\nYou can then test with one of the binder examples:\nhttps://binder.tg-xxx000000.projects.jetstream-cloud.org/v2/gh/binder-examples/requirements/master\nThis should build the container using the Docker-in-Docker pod, push it to the registry on Google Cloud, then spawn a Jupyter Notebook instance inside JupyterHub and redirect you to https://hub.tg-xxx000000.projects.jetstream-cloud.org."
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#setup-kubernetes",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#setup-kubernetes",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "The first step is to have a Kubernetes deployment on Jetstream 2, for example deployed with Kubespray using this tutorial"
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#enable-the-google-container-registry",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#enable-the-google-container-registry",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "We first need to activate the Google Container Registry in the Google Cloud account:\n\nGo to the API Library\nSearch for Google Container Registry API\nClick on Enable\nIt will ask to configure billing\nIt will ask for credit card information, but you will have $300 free credits for 90 days and the card won’t be charged unless you manually switch to a paid account\nGo to the Google Container registry settings, set the images to public, so that they can be pulled back from BinderHub to the container dedicated to execution"
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#setup-permissions-on-the-google-container-registry",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#setup-permissions-on-the-google-container-registry",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "Follow the instructions on “Zero to BinderHub” to setup the Google Container Registry some notes:\n\nBetter to create a dedicated project just for BinderHub\n“Create key” is under “Actions”-&gt;“Manage keys”"
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#setup-ingress-with-https-support",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#setup-ingress-with-https-support",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "On Jetstream 2 we do not have a Load Balancer service like public clouds, therefore we need to use Ingress.\nFirst we need to setup Cert-Manager to provide HTTPS certificates via Letsencrypt.\nFollow the instructions at https://zonca.dev/2020/03/setup-https-kubernetes-letsencrypt.html to install a cluster issuer"
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#configure-the-domains",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#configure-the-domains",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "BinderHub requires 2 subdomains, one for BinderHub and one for JupyterHub. For the initial testing I recommend to use Jetstream 2 provided domains, once they are working it is easy to replace them with custom domains.\nLogin to the Jetstream 2 Horizon instance, choose the right allocation, click on DNS, Zones, choose the Zone which starts with tg, then click on “Create Record Set”.\nCreate a “A” record where name is binder.tg-xxx000000.projects.jetstream-cloud.org and record is the IP of your master instance. Create a second A record with the name hub.tg-xxx000000.projects.jetstream-cloud.org and the same record.\nThese should be ready to be used in a few minutes."
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#install-binderhub-with-helm",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#install-binderhub-with-helm",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "Checkout the repository with the configuration files:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/\nEnter the binderhub folder.\nFollow the instructions on how to create just the secret.yaml file.\nThe repository provides a template named config_template.yaml to create config.yaml, this file contains all of the configuration items needed to setup the following items, see the links for the relevant documentation:\n\nBinderHub with Google Container Registry\nConfigure HTTPS ingress for BinderHub and JupyterHub\nUse Docker-in-Docker to build the containers, which is necessary because the Jetstream 2 Kubernetes deployment uses containerd.\n\nCreate a file named config_vars.sh in the same folder which defines your Jetstream allocation and Google Project IDs:\nexport ALLOCATION=xxx000000\nexport GOOGLE_PROJECT=binderhub-000000\nThen deploy BinderHub running:\nbash install_binderhub.sh\nThis script first creates config.yaml (and overwrites it, so keep your changes in config_template.yaml), then deploys or updates the deploy of BinderHub via Helm."
  },
  {
    "objectID": "posts/2022-11-15-binderhub-jetstream2.html#test-the-deployment",
    "href": "posts/2022-11-15-binderhub-jetstream2.html#test-the-deployment",
    "title": "Deploy BinderHub on top of Kubernetes on Jetstream 2",
    "section": "",
    "text": "Connect to https://binder.tg-xxx000000.projects.jetstream-cloud.org, you should see the binder login page:\n\n\n\nBinder login page\n\n\nYou can then test with one of the binder examples:\nhttps://binder.tg-xxx000000.projects.jetstream-cloud.org/v2/gh/binder-examples/requirements/master\nThis should build the container using the Docker-in-Docker pod, push it to the registry on Google Cloud, then spawn a Jupyter Notebook instance inside JupyterHub and redirect you to https://hub.tg-xxx000000.projects.jetstream-cloud.org."
  },
  {
    "objectID": "posts/2022-10-26-click-commandline-class-arguments.html",
    "href": "posts/2022-10-26-click-commandline-class-arguments.html",
    "title": "Generate Click command line options dynamically from class arguments",
    "section": "",
    "text": "In this application I need to use command line options to create class objects. In order not to repeat all class arguments in the Click configuration, I created a simple function to dynamically create Click options. See the source notebook of this page on Github\n\nimport click\nimport inspect\n\nWe have 2 classes, we want to pass the class name as argument to the command line tool and then all its arguments, for example:\ncreateclass aclass --a somestring --b 6\nWe also use Python typing so we can pass that to Click.\n\nclass AClass:\n    def __init__(\n        self,\n        a: str,\n        b: int):\n        pass\n\n\nclass BClass:\n    def __init__(\n        self,\n        c: float,\n        d: bool,\n        under_score: str):\n        pass\n\n\ndef options_from_class(cls):\n    def decorator(f):\n        for par in inspect.signature(cls.__init__).parameters.values():\n            if par.name not in [\"self\"]:\n                click.option(\"--\" + par.name, required=True, type=par.annotation)(f)\n        return f\n    return decorator\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\n@options_from_class(AClass)\ndef aclass(**kwargs):\n    click.echo('kwargs: {}'.format(kwargs))\n    ac = AClass(**kwargs)\n\n\n@cli.command()\n@options_from_class(BClass)\ndef bclass(**kwargs):\n    click.echo('kwargs: {}'.format(kwargs))\n    bc = BClass(**kwargs)\n\n\nif __name__ == '__main__':\n    cli()\n\nConvert the Notebook to a Python script with:\njupyter nbconvert click-commandline-class-arguments.ipynb --to python\nFinally test at the command line:\n$ python click-commandline-class-arguments.py aclass --help\nUsage: click-commandline-class-arguments.py aclass [OPTIONS]\n\nOptions:\n  --b INTEGER  [required]\n  --a TEXT     [required]\n  --help       Show this message and exit.\npython click-commandline-class-arguments.py bclass --help\nUsage: click-commandline-class-arguments.py bclass [OPTIONS]\n\nOptions:\n  --under_score TEXT  [required]\n  --d BOOLEAN         [required]\n  --c FLOAT           [required]\n  --help              Show this message and exit.\n$ python click-commandline-class-arguments.py bclass --d true --c 4.5 --under_score works\nkwargs: {'d': True, 'c': 4.5, 'under_score': 'works'}"
  },
  {
    "objectID": "posts/2023-04-24-hiring-computational-scientist-ucsd.html",
    "href": "posts/2023-04-24-hiring-computational-scientist-ucsd.html",
    "title": "Hiring a Computational Scientist at the San Diego Supercomputer Center",
    "section": "",
    "text": "Early Career Scientist? Likes Supercomputers? Likes the Sun?\n\nCome work in my group of High Performance Computing “consultants” at the San Diego Supercomputer Center, help scientists develop, scale and run Data Intensive computations on our Supercomputers.\nSee the job ad below:\n\nThe Scientific Computing Applications group at the San Diego Supercomputer Center, led by Andrea Zonca, is hiring a Computational Scientist with a permanent contract funded by grants. The ideal candidate is an early career scientist in any scientific domain with a strong computational background, either in High Performance Computing or in Artificial Intelligence. They will just need a subset of the skills mentioned in the job description and be willing to learn more. The primary role will be to support scientists running software on the Supercomputers Expanse and Voyager, but the position can also be funded by external grants to work on any scientific project, for example CMB-S4 Data Management. The position is hybrid, the candidate will need to be in the office at UCSD at least 1 day a week, and can work remotely the rest of the week. Members of historically under-represented communities are especially encouraged to apply. For any clarification, please email Andrea Zonca\n\n\nLink to the job post"
  },
  {
    "objectID": "posts/2024-04-12-pysm-figures.html",
    "href": "posts/2024-04-12-pysm-figures.html",
    "title": "Generate figures for PySM emissions",
    "section": "",
    "text": "Generate figures for PySM\nThis notebook generates some figures of Galactic and Extra-Galactic emissions using PySM. Mostly for displaying purposes.\nThis notebook is designed to work on Google Colab, remove the apt lines if executing locally but make sure you have a Latex environment.\n\n# Install Latex to render labels\n!apt install texlive texlive-latex-extra texlive-fonts-recommended cm-super-minimal dvipng\n\n\n%pip install pysm3\n\n\nimport pysm3\nfrom pysm3 import units as u\nimport healpy as hp\n\n\nsky = pysm3.Sky(nside=128, preset_strings=[\"c3\"], output_unit=u.uK_CMB)\n\n\ncmb = sky.get_emission(100 * u.GHz)\n\n\nimport matplotlib.pyplot as plt\nplt.rcParams['text.usetex'] = True\n\n\nfontsize={\"title\":30, \"cbar_label\":20, \"cbar_tick_label\":20}\n\n\nhp.projview(cmb[0].value, min=-250, max=250,\n            fontsize=fontsize,\n            unit=r'$\\mu K_{CMB}$', title=\"Cosmic Microwave Background\");\nplt.savefig(\"1.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=256, preset_strings=[\"s5\"], output_unit=u.mK_RJ)\nsync = sky.get_emission(23 * u.GHz)[0]\n\n\nhp.projview(sync.value, min=0.02, max=2,\n            fontsize=fontsize, norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$mK$', title=r\"Synchrotron at $23 GHz$\");\nplt.savefig(\"2.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=256, preset_strings=[\"f1\"], output_unit=u.mK_RJ)\nfreefree = sky.get_emission(30 * u.GHz)[0]\n\n\nfreefree.min(), freefree.max()\n\n(&lt;Quantity 1.9947761e-07 mK_RJ&gt;, &lt;Quantity 212.05362 mK_RJ&gt;)\n\n\n\nhp.projview(freefree.value, min=0, max=10,\n            fontsize=fontsize,\n            #norm=\"hist\",\n            cmap = \"plasma\",\n            #norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$mK$', title=r\"Free-free emission at $30 GHz$\");\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=256, preset_strings=[\"a1\"], output_unit=u.mK_RJ)\name = sky.get_emission(30 * u.GHz)[0]\n\n\nhp.projview(ame.value, min=0.01, max=10,\n            fontsize=fontsize,\n            norm=\"log\",\n            cmap = \"magma\",\n            #norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$mK_{RJ}$', title=r\"Spinning dust at $30 GHz$\");\nplt.savefig(\"3.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=256, preset_strings=[\"d10\"], output_unit=u.mK_RJ)\ndust = sky.get_emission(545 * u.GHz)[0]\n\n\ndust.value.min()\n\n0.03938505\n\n\n\nhp.projview(dust.value-dust.value.min(), min=0.001, max=10,\n            fontsize=fontsize,\n            norm=\"log\",\n            cmap = \"inferno\",\n            #norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$mK_{RJ}$', title=r\"Thermal Dust at $545 GHz$\");\nplt.savefig(\"4.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=256, preset_strings=[\"co1\"], output_unit=u.mK_RJ)\nco = sky.get_emission(sky.components[0].line_frequency['10'])[0]\n\n\nc=sky.components[0]\n\n\nc.line_frequency\n\n{'10': &lt;Quantity 115.271 GHz&gt;,\n '21': &lt;Quantity 230.538 GHz&gt;,\n '32': &lt;Quantity 345.796 GHz&gt;}\n\n\n\nhp.projview(co.value, min=0.01, max=1,\n            fontsize=fontsize,\n            #norm=\"log\",\n            cmap = \"Greys_r\",\n            #norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$mK_{RJ}$', title=r\"Carbon Monoxyde line at $115 GHz$\");\nplt.savefig(\"5.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nsky = pysm3.Sky(nside=2048, preset_strings=[\"rg1\"], output_unit=u.uK_RJ)\nrg = sky.get_emission(353*u.GHz)[0]\n\n\npysm3.apply_smoothing_and_coord_transform?\n\n\nrg_smoothed = pysm3.apply_smoothing_and_coord_transform(rg, output_nside=256,\n\n                                                        fwhm=3 * u.arcmin)\n\n\nhp.projview(rg_smoothed.value, min=0, max=10,\n            fontsize=fontsize,\n            #norm=\"log\",\n            cmap = \"Oranges_r\",\n            #norm=\"symlog2\", cmap=\"planck_log\",\n            unit=r'$\\mu K_{RJ}$', title=r\"Radio galaxies at $353 GHz$\");\nplt.savefig(\"6.jpg\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n!apt install imagemagick\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nimagemagick is already the newest version (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.3).\n0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n\n\n\n!rm -f montage.jpg\n!montage -geometry -0+0 -size 2000 *.jpg montage.jpg\n\n\nfrom IPython.display import Image\nImage(filename='montage.jpg')"
  },
  {
    "objectID": "posts/2023-02-21-show-your-work.html",
    "href": "posts/2023-02-21-show-your-work.html",
    "title": "Showyourwork! Globus demo",
    "section": "",
    "text": "Showyourwork! is a great project to make scientific articles reproducible.\nI participated to its first Hack Day, I focused on building a simple recipe in SnakeMake (the workflow engine behind Showyourwork!) to interface with data stored on Globus.\nSee the repository on Github"
  },
  {
    "objectID": "posts/2022-12-05-jetstream2-kubernetes-manila.html",
    "href": "posts/2022-12-05-jetstream2-kubernetes-manila.html",
    "title": "Share data between JupyterHub users with Manila shares",
    "section": "",
    "text": "This work has been supported by Indiana University and is cross-posted on the Jetstream 2 official documentation website.\nIn the standard deployment of JupyterHub on top of Kubernetes on Jetstream 2, each user has access to their own home folder, but there is no built-in way to share data between users.\nOne option is to install a NFS server as a service in Kubernetes. However, Jetstream offers a managed service, named Manila, that offers the sharing capability out-of-the-box.\nTherefore we can lower the maintenance burden by creating a shared volume inside Manila and then mount it on any Kubernetes or JupyterHub pod, so that users can access shared data read-only or read-write."
  },
  {
    "objectID": "posts/2022-12-05-jetstream2-kubernetes-manila.html#create-a-share-in-manila",
    "href": "posts/2022-12-05-jetstream2-kubernetes-manila.html#create-a-share-in-manila",
    "title": "Share data between JupyterHub users with Manila shares",
    "section": "Create a Share in Manila",
    "text": "Create a Share in Manila\nCreate a share in Horizon following the instructions in the Jetstream documentation:\n\nfor access to choose “$USER-manila-share”\nnote down “Path” and “Access Key”"
  },
  {
    "objectID": "posts/2022-12-05-jetstream2-kubernetes-manila.html#test-in-a-pod",
    "href": "posts/2022-12-05-jetstream2-kubernetes-manila.html#test-in-a-pod",
    "title": "Share data between JupyterHub users with Manila shares",
    "section": "Test in a Pod",
    "text": "Test in a Pod\nFirst we want to test the connection with a pod, which is also useful to perform administrative tasks on the volume.\nYou can find all the necessary files in this Github repository.\nCopy the access key into ceph-secret.yml, then create the secret with:\nkubectl create -f ceph-secret.yml\nThen split the “Path” string you copied from Horizon into the different servers and the path and copy them to ceph-pod.yml and:\nkubectl create -f ceph-pod.yml\nAccess the shared volume with:\nkubectl exec --stdin -n jhub --tty ceph -- /bin/bash\ncd /mnt/cephfs\nThis container executes as root, so you should have write access to the share, instead, the JupyterHub users use the jovyan user, so we need to create a folder owned by jovyan to give them write access, all other folders will be read-only:\nmkdir readonly\nmkdir readwrite\nchown 1000:100 readwrite\nso we should have:\nroot@cephfs-pod:/mnt/cephfs# ls -la\ntotal 6\ndrwxr-xr-x 4  119   126    0 Dec  1 00:48 .\ndrwxr-xr-x 1 root root  4096 Nov 30 23:50 ..\ndrwxr-xr-x 2 root root     0 Dec  1 00:04 readonly\ndrwxr-xr-x 2 1000 users    0 Dec  1 00:07 readwrite"
  },
  {
    "objectID": "posts/2022-12-05-jetstream2-kubernetes-manila.html#configure-jupyterhub",
    "href": "posts/2022-12-05-jetstream2-kubernetes-manila.html#configure-jupyterhub",
    "title": "Share data between JupyterHub users with Manila shares",
    "section": "Configure JupyterHub",
    "text": "Configure JupyterHub\nWe can finally configure JupyterHub to mount this volume, if you have followed the JupyterHub deployment tutorial, you should have your install_jhub.sh script, edit it to also add an additional configuration file:\nRELEASE=jhub\nNAMESPACE=jhub\n\nhelm upgrade --install $RELEASE jupyterhub/jupyterhub \\\n      --namespace $NAMESPACE  \\\n      --create-namespace \\\n      --version 1.2.0 \\\n      --debug \\\n      --values config_standard_storage.yaml --values secrets.yaml \\\n      --values manila/jupyterhub_manila.yaml\nmanila/jupyterhub_manila.yaml contains the same configuration of ceph-pod.yml, so you can copy-paste from there.\nNow you should be able to access the shared folders from a JupyterHub single user session:\njovyan@jupyter-zonca:~$ cd /share\njovyan@jupyter-zonca:/share$ ls -lah\ntotal 5.5K\ndrwxr-xr-x 4    119   126    0 Dec  1 00:48 .\ndrwxr-xr-x 1 root   root  4.0K Nov 30 23:48 ..\ndrwxr-xr-x 2 root   root     0 Dec  1 00:04 readonly\ndrwxr-xr-x 2 jovyan users    0 Dec  1 00:07 readwrite\njovyan@jupyter-zonca:/share$ touch readwrite/myfile\njovyan@jupyter-zonca:/share$ touch readonly/myfile\ntouch: cannot touch 'readonly/myfile': Permission denied"
  },
  {
    "objectID": "posts/2022-10-11-install-jupyroot-jupyterhub.html",
    "href": "posts/2022-10-11-install-jupyroot-jupyterhub.html",
    "title": "Install the JupyROOT Python kernel in JupyterHub",
    "section": "",
    "text": "After installing ROOT’s conda package, for example with micromamba:\nmicromamba create -n root -c conda-forge root python==3.8 matplotlib\nand then installing the kernel for JupyterHub:\nipython kernel install --name root --user\nUnfortunately import ROOT gives the error:\nERROR in cling::CIFactory::createCI(): cannot extract standard library include paths!\nInvoking:\n  LC_ALL=C x86_64-conda-linux-gnu-c++   -DNDEBUG -xc++ -E -v /dev/null 2&gt;&1 | sed -n -e '/^.include/,${' -e '/^ \\/.*++/p' -e '}'\nResults was:\nWith exit code 0\nThe fix, found after more than 1 hour of search, is at https://root-forum.cern.ch/t/jupyroot-in-jupyterhub-cling-init-can-not-extract-standard-include-library-paths/46890:\nEdit the kernel.json file and modify the PATH variable:\n{\n \"argv\": [\n  \"/home/jovyan/micromamba/envs/root/bin/python3.8\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"root\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n },\n \"env\": {\n   \"PATH\": \"/home/jovyan/micromamba/envs/root/bin/:${PATH}\"    \n }\n}"
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "",
    "text": "This work has been supported by Indiana University and is cross-posted on the Jetstream 2 official documentation website.\nThis tutorial will explain how to install Kubernetes on Jetstream 2 relying on Kubespray.\nKubespray is a project built on top of Terraform, for creating Openstack resources, and Ansible, for configuring the Virtual Machines for Kubernetes.\nThis work is based on Kubespray v2.21.0 which was published in January 2023, which installs Kubernetes v1.25.6, released in December 2022."
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "Create Jetstream Virtual machines with Terraform",
    "text": "Create Jetstream Virtual machines with Terraform\nTerraform allows to execute recipes that describe a set of OpenStack resources and their relationship. In the context of this tutorial, we do not need to learn much about Terraform, we will configure and execute the recipe provided by kubespray.\n\nRequirements\nWe have been testing with python-openstackclient version 6.1.0, but any recent openstack client should work. install terraform by copying the correct binary to /usr/local/bin/. The requirement is a terraform version &gt; 0.14, this tutorial has been tested with 0.14.4, which can be dowloaded from the release page. Terraform 1.1.9 and 1.2.9 do not work\n\n\nRequest API access\nFollow the instructions in the Jetstream 2 documentation to create application credentials.\nAlso make sure you are not hitting any of the issues in the Troubleshooting page, in particular, it is a good idea to set your password within single quotes to avoid special characters being interpreted by the shell:\nexport OS_APPLICATION_CREDENTIAL_SECRET='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\nTest with:\nopenstack flavor list\nThis should return the list of available “sizes” of the Virtual Machines.\nYou also need to add to the app*openrc.sh also this line:\nexport OS_APPLICATION_CREDENTIAL_NAME=$OS_APPLICATION_CREDENTIAL_ID\notherwise Ansible will fail with you must either set external_openstack_username or external_openstack_application_credential_name.\n\n\nClone kubespray\nWe needed to make a few modifications to kubespray to adapt it to Jetstream:\ngit clone https://github.com/zonca/jetstream_kubespray\ngit checkout -b branch_v2.21.0 origin/branch_v2.21.0\nSee an overview of my changes compared to the standard kubespray release 2.21.0, compared to previous releases of this tutorial, now the changes in the Terraform recipes are extensive, the good news is that the deployment itself is simpler. We are having all networking handled automatically by Jetstream 2, i.e. instances are automatically assigned to the auto_allocated_network, auto_allocated_router and auto_allocated_subnet, instead of creating dedicated resources with Terraform.\nInside jetstream_kubespray, choose a name for the cluster and copy from my template:\nexport CLUSTER=yourclustername\ncp -r inventory/kubejetstream inventory/$CLUSTER\ncd inventory/$CLUSTER\nalso export CLUSTER=yourclustername is useful to add to the app*openrc.sh.\n\n\nUse a projects.jetstream-cloud.org subdomain\nOne option is to use the Designate Openstack service deployed by Jetstream to get an automatically created domain for the instances. In this case the DNS name will be of the form:\nkubejetstream-1.$PROJ.projects.jetstream-cloud.org\nwhere PROJ is the ID of your Jestream 2 allocation:\nexport PROJ=\"xxx000000\"\nThe first part of the URL is the instance name, we shortened it removing k8s-master because domains too long do not work with Letsencrypt.\nAfter having executed Terraform, you can pip install on your local machine the package python-designateclient to check what records were created (mind the final period):\nopenstack recordset list $PROJ.projects.jetstream-cloud.org.\nAs usual with stuff related to DNS, there are delays, so your record could take up to 1 hour to work, or if you delete the instance and create it again with another IP it could take hours to update.\nFor debugging purposes it is useful to use nslookup:\nnslookup ${CLUSTER}-1.$PROJ.projects.jetstream-cloud.org\nalso directly at the source nameservers:\nnslookup ${CLUSTER}-1.$PROJ.projects.jetstream-cloud.org js2.jetstream-cloud.org\nInstead, if you have a way of getting a domain outside of Jetstream, better reserve a floating IP, see below.\n\n\nReserve a floating IP\nWe prefer not to have a floating IP handled by Terraform, otherwise it would be released every time we need to redeploy the cluster, better create it beforehand:\nopenstack floating ip create public\nThis will return a public floating IP address, it can also be accessed with:\nopenstack floating ip list\nIt is useful to save the IP into the app*openrc.sh, so that every time you load the credentials you also get the address of the master node.\nexport IP=149.xxx.xxx.xxx\n\n\nRun Terraform\nOpen and modify cluster.tfvars, choose your image (by default Ubuntu 20, **Update February 2024, currently Ubuntu 22 works on CPU but broken on GPU) and number of nodes and the flavor of the nodes, by default they are medium instances (\"4\"). See the entries marked as REPLACE and replace them according to the instructions provided.\nPaste the floating ip created previously into k8s_master_fips, unless you are using a projects.jetstream-cloud.org subdomain.\nInitialize Terraform:\nbash terraform_init.sh\nCreate the resources:\nbash terraform_apply.sh\nTerraform is very fast in building all the resources, sometimes resources are not ready yet, so the Apply command fails, just run it again, it happens regularly, nothing to worry about.\nYou can SSH into the master node with:\nssh ubuntu@$IP\nInspect with Openstack the resources created:\nopenstack server list\nopenstack network list\nYou can cleanup the virtual machines and all other Openstack resources (all data is lost) with bash terraform_destroy.sh. The floating IP won’t be released so we can create a cluster again from scratch with the same IP address."
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#install-and-test-ansible",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#install-and-test-ansible",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "Install and test Ansible",
    "text": "Install and test Ansible\nChange folder back to the root of the jetstream_kubespray repository,\nFirst make sure you have a recent version of ansible installed, tested with 2.10.15, you also need additional modules, so first run:\npip install -r requirements.txt\nThis pip script installs a predefined version of ansible, currently 2.10.15, so it is useful to create a virtualenv or a conda environment and install packages inside that.\nThen following the kubespray documentation, we setup ssh-agent so that ansible can SSH from the machine with public IP to the others:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nTest the connection through ansible:\nansible -i inventory/$CLUSTER/hosts -m ping all"
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "Install Kubernetes with kubespray",
    "text": "Install Kubernetes with kubespray\nIn inventory/$CLUSTER/group_vars/k8s_cluster/k8s-cluster.yml, set the public floating IP of the master instance in supplementary_addresses_in_ssl_keys. Update February 2024: this step is not needed anymore, make sure you have the IP environmental variable set and it will be automatically passed to Ansible by the k8s_install.sh script.\nFinally run the full playbook, it is going to take a good 20 minutes:\nbash k8s_install.sh\nIf the playbook fails with “cannot lock the administrative directory”, it is due to the fact that the Virtual Machine is automatically updating so it has locked the APT directory. Just wait a minute and launch it again. It is always safe to run ansible multiple times.\nIf the playbook gives any error, try to retry the above command, sometimes there are temporary failed tasks, Ansible is designed to be executed multiple times with consistent results.\nYou should have now a Kubernetes cluster running, test it:\n$ ssh ubuntu@$IP\n$ sudo su\n$ kubectl get pods --all-namespaces\nNAMESPACE       NAME                                           READY   STATUS    RESTARTS   AGE\ningress-nginx   ingress-nginx-controller-4xd64                 1/1     Running   0          27m\nkube-system     coredns-8474476ff8-9gd8w                       1/1     Running   0          27m\nkube-system     coredns-8474476ff8-qtshk                       1/1     Running   0          27m\nkube-system     csi-cinder-controllerplugin-9fb5946bf-hwfhp    6/6     Running   0          26m\nkube-system     csi-cinder-nodeplugin-r69nl                    3/3     Running   0          26m\nkube-system     dns-autoscaler-5ffdc7f89d-s4sj4                1/1     Running   0          27m\nkube-system     kube-apiserver-kubejs2-k8s-master-1            1/1     Running   1          66m\nkube-system     kube-controller-manager-kubejs2-k8s-master-1   1/1     Running   1          66m\nkube-system     kube-flannel-2clqv                             1/1     Running   0          29m\nkube-system     kube-flannel-2wbtq                             1/1     Running   0          29m\nkube-system     kube-proxy-hmz6t                               1/1     Running   0          30m\nkube-system     kube-proxy-xkhjx                               1/1     Running   0          30m\nkube-system     kube-scheduler-kubejs2-k8s-master-1            1/1     Running   1          66m\nkube-system     nginx-proxy-kubejs2-k8s-node-1                 1/1     Running   0          64m\nkube-system     nodelocaldns-jwxg8                             1/1     Running   0          27m\nkube-system     nodelocaldns-z4sjl                             1/1     Running   0          27m\nkube-system     openstack-cloud-controller-manager-6q28z       1/1     Running   0          29m\nkube-system     snapshot-controller-786647474f-7x8zx           1/1     Running   0          25m\n\nCompare that you have all those services running also in your cluster. We have also configured NGINX to proxy any service that we will later deploy on Kubernetes, test it with:\n$ wget localhost\n--2022-03-31 06:51:20--  http://localhost/\nResolving localhost (localhost)... 127.0.0.1\nConnecting to localhost (localhost)|127.0.0.1|:80... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-03-31 06:51:20 ERROR 404: Not Found.\nError 404 is a good sign, the service is up and serving requests, currently there is nothing to deliver. If you get any other type of error, check that the nginx controller is running:\nkubectl get pods -n ingress-nginx"
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#set-the-default-storage-class",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#set-the-default-storage-class",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "Set the default storage class",
    "text": "Set the default storage class\nThis is not needed anymore, setting Cinder CSI as default storage class is included in the modifications to Kubespray."
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "(Optional) Setup kubectl locally",
    "text": "(Optional) Setup kubectl locally\nInstall kubectl locally, the tutorial has been tested with 1.26.\nWe also set kubeconfig_localhost: true, which copies the kubectl configuration admin.conf to:\ninventory/$CLUSTER/artifacts\nWe have a script to replace the IP with the floating IP of the master node, for this script to work make sure you have exported the variable IP:\nbash k8s_configure_kubectl_locally.sh\nFinally edit again the app*openrc.sh and add:\nexport KUBECONFIG=$(pwd -P)/\"jetstream_kubespray/inventory/$CLUSTER/artifacts/admin.conf\""
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#optional-setup-helm-locally",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#optional-setup-helm-locally",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "(Optional) Setup helm locally",
    "text": "(Optional) Setup helm locally\nInstall helm 3 from the release page on Github\nThe tutorial was tested with v3.8.1."
  },
  {
    "objectID": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#troubleshooting",
    "href": "posts/2023-07-19-jetstream2_kubernetes_kubespray.html#troubleshooting",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.21.0",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nLooking at container logs\n\nkubectl logs\nkubectl logs --previous for terminated or restarted containers\nIf this doesn’t work, ssh into the worker node, crictl ps -a then crictl inspect $ID"
  },
  {
    "objectID": "posts/2023-09-26-https-kubernetes-letsencrypt.html",
    "href": "posts/2023-09-26-https-kubernetes-letsencrypt.html",
    "title": "Setup HTTPS on Kubernetes with cert-manager",
    "section": "",
    "text": "Update March 2024: the routing issue that force cert-manager pods to run on the control-plane are back, see this Github issue, so we had to add back the pinning of cert-manager services on one of the nodes in the control plane.\nIn this tutorial we will deploy cert-manager in Kubernetes to automatically provide SSL certificates to JupyterHub (and other services).\nFirst make sure your payload, for example JupyterHub, is working without HTTPS, so that you check that the ports are open, Ingress is working, and JupyterHub itself can accept connections.\nLet’s follow the cert-manager documentation, for convenience I pasted the commands below:\nOnce we have cert-manager setup we can create a Cluster Issuer that works for all namespaces (first edit the yml and add your email address):\nAfter this, we can display all the resources in the cert-manager namespace to check that the services and pods are running:\nThe result should be something like:"
  },
  {
    "objectID": "posts/2023-09-26-https-kubernetes-letsencrypt.html#bind-the-pods-to-the-master-node",
    "href": "posts/2023-09-26-https-kubernetes-letsencrypt.html#bind-the-pods-to-the-master-node",
    "title": "Setup HTTPS on Kubernetes with cert-manager",
    "section": "Bind the pods to the master node",
    "text": "Bind the pods to the master node\nIn Jetstream 2 there are routing restrictions which allow Cert Manager to run only from the master node, see the details on Github. At least when the nodes do not have floating IPs, if all your Virtual Machines have a floating IP, you can safely skip this step.\nUnidata has contributed the script they created to patch the 3 Cert Manager pods to have them run on the master node, we can apply it with:\ncd setup_https\nbash deploymentPatch.sh\nThen verify that the pods are redeployed on master:\nkubectl -n cert-manager get pods -o wide"
  },
  {
    "objectID": "posts/2023-09-26-https-kubernetes-letsencrypt.html#setup-jupyterhub",
    "href": "posts/2023-09-26-https-kubernetes-letsencrypt.html#setup-jupyterhub",
    "title": "Setup HTTPS on Kubernetes with cert-manager",
    "section": "Setup JupyterHub",
    "text": "Setup JupyterHub\nThen we modify the JupyterHub ingress configuration to use this Issuer, modify secrets.yaml to:\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt\"\n  hosts:\n      - js-XXX-YYY.jetstream-cloud.org\n  tls:\n      - hosts:\n         - js-XXX-YYY.jetstream-cloud.org\n        secretName: certmanager-tls-jupyterhub\nFinally update the JupyterHub deployment rerunning the deployment script (no need to delete it):\nbash install_jhub.sh\nAfter a few minutes we should have a certificate available:\nkubectl get certificaterequest --all-namespaces\nNAMESPACE   NAME                           APPROVED   DENIED   READY   ISSUER        REQUESTOR                                         AGE\njhub        certmanager-tls-jupyterhub-1   True                True    letsencrypt   system:serviceaccount:cert-manager:cert-manager   5d\nYou can also check the state of the certificate with:\nkubectl -n jhub describe certificate certmanager-tls-jupyterhub"
  },
  {
    "objectID": "posts/2023-03-23-update-openstack-credentials-kubernetes.html",
    "href": "posts/2023-03-23-update-openstack-credentials-kubernetes.html",
    "title": "Update Openstack credentials in Kubernetes",
    "section": "",
    "text": "If you are deploying Kubernetes on top of Openstack, the Openstack External cloud provider stores the ID and Secret necessary to authenticate with the cloud infrastructure in a Secret.\nLet’s see how to replece those credentials, for example if they expired.\nFirst dump the base64 encoded secret to the terminal:\nkubectl get secret -n kube-system external-openstack-cloud-config -o jsonpath='{.data}'\nThen copy-paste just the encoded part:\necho xxxxxx | base64 –decode &gt; cloud.conf\nNow we can edit it to replace the credentials\nSet the new OS_APPLICATION_CREDENTIAL_ID in application-credential-id and application-credential-name. Set the value of OS_APPLICATION_CREDENTIAL_SECRET in application-credential-secret.\nFinally encode the content of the file again (-w0 gives the output in 1 line without wrapping):\ncat cloud.conf | base64 -w0\nand overwrite the cloud.conf value in the secret:\nkubectl edit secret -n kube-system external-openstack-cloud-config\nNow repeat the process with the cloud-config secret, which is used by Cinder CSI, you can copy the 3 relevant lines from the previous file.\nFinally we need to restart the affected pods, however, to make it easier I just rebooted all the nodes via openstack server reboot."
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html",
    "href": "posts/2023-02-02-github-overleaf.html",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "",
    "text": "In the past I have written about coordinating a large document using multiple overleaf documents and Github repositories. In this post I will just layout the simpler case where we have a Latex document on Overleaf and we also want users to be able to work on it from Github and we want Github Actions to automatically build a PDF for every commit."
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html#create-less-privileged-user-on-github",
    "href": "posts/2023-02-02-github-overleaf.html#create-less-privileged-user-on-github",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "Create less privileged user on Github",
    "text": "Create less privileged user on Github\nA concern about using Overleaf is that to have the convenience of pushing and pulling from Github, it asks for write permissions to all repositories.\nSo just to play it safe, it is useful to create a new user on Github, for example mine is called @zoncaoverleafbot, which has write access only to this repository.\nIt is useful to be logged in with your official Github user in your browser and be logged in inside an Incognito window with your “Overleaf bot” user."
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html#create-repository-on-github",
    "href": "posts/2023-02-02-github-overleaf.html#create-repository-on-github",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "Create repository on Github",
    "text": "Create repository on Github\nLet’s start with a Latex document for testing purposes, for example I used the “Astronomy & Astrophysics” template.\nStarting with a project on Overleaf is more complicated because of permission issues, so in case you have an Overleaf project, first download it locally.\nCreate a new repository on Github, this should be under your “real” Github account or any organization you have access to, upload the Latex document and all ancillary files.\nSee for example my repository https://github.com/zonca/overleaf_github, you could also fork this repository under your account.\nFinally, give “Write” access to your Overleaf bot user."
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html#create-the-linked-project-on-overleaf",
    "href": "posts/2023-02-02-github-overleaf.html#create-the-linked-project-on-overleaf",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "Create the linked project on Overleaf",
    "text": "Create the linked project on Overleaf\nNow login to Github with your “Overleaf bot” account, then login to Overleaf.\nClick on “New project” =&gt; “Import from Github”, select your repository.\nThis will create a project on Overleaf that is automatically linked to the Github repository."
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html#how-to-edit-on-overleaf",
    "href": "posts/2023-02-02-github-overleaf.html#how-to-edit-on-overleaf",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "How to edit on Overleaf",
    "text": "How to edit on Overleaf\nNow both you and all your collaborators, can use Overleaf to edit the document and then push to Github using your “Overleaf bot” user, they do not need to link their own Github user.\n\nuse the Overleaf “Write”\nBefore making any edits, click on Menu =&gt; Github =&gt; Pull changes from Github\nMake edits on Overleaf\nOnce you are finished editing, even just for 1 or 2 hours, click on Menu =&gt; Github =&gt; Push changes to Github\nEdits will show up on Github as made by the “Overleaf bot” user , which avoids giving too much permissions to Overleaf\nGithub is supposed to have the latest version of the document always"
  },
  {
    "objectID": "posts/2023-02-02-github-overleaf.html#build-the-pdf-on-github-optional",
    "href": "posts/2023-02-02-github-overleaf.html#build-the-pdf-on-github-optional",
    "title": "Work on a Latex Document with Github and Overleaf",
    "section": "Build the PDF on Github (Optional)",
    "text": "Build the PDF on Github (Optional)\nThe easiest way to access the PDF is to get it from Overleaf. However, it might be useful to also have it available on Github. We can have Github actions build it and make it available as an artifact.\nCustomize the build_latex.yml file to have Github actions build your PDF.\nThe only downside is that the PDF is hidden inside the Github actions, to simplify access we can get a direct link to the latest PDF using the https://nightly.link service, just paste the URL to the build_latex.yml, for my repository:\n\nhttps://github.com/zonca/overleaf_github/blob/main/.github/workflows/build_latex.yml\n\nThis will provide a link of the form:\nhttps://nightly.link/zonca/overleaf_github/workflows/build_latex/main/PDF.zip\nwhich provides 1-click access to the latest PDF.\n\nCreate releases with attached PDF on Github (Optional)\nThe Github Actions workflow also handles tagging releases, so if you create a tagged version in the repository, like 0,1 in git, for example:\ngit tag -a 0.1 -m \"Tagging version 0.1\"\ngit push --tags\nThis is going to create a Release on Github, which is visible on the homepage and attach the PDF to it, see the example on my repository.\nTo make this work, you need to grant writing permissions Github Action workflows, in the repository “Settings” =&gt; “Actions” =&gt; “General”, select “Read and write permissions” under “Workflow permissions”."
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "",
    "text": "I need to check that the amplitude of the maps of Radio Galaxies produced by WebSky are reasonable if compared to experimental results. The main worry is that being point sources, when we apply a beam using Spherical Harmonics transform we get spurious results due to the input maps not being band-limited.\npip install pysm3 --pre\n\nRequirement already satisfied: pysm3 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (3.4.0b4.dev138+g864a61e.d20221114)\nRequirement already satisfied: toml in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from pysm3) (0.10.2)\nRequirement already satisfied: astropy in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from pysm3) (5.1.1)\nRequirement already satisfied: numba in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from pysm3) (0.56.3)\nRequirement already satisfied: healpy in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from pysm3) (1.16.1)\nRequirement already satisfied: numpy&gt;=1.18 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from astropy-&gt;pysm3) (1.23.4)\nRequirement already satisfied: packaging&gt;=19.0 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from astropy-&gt;pysm3) (21.3)\nRequirement already satisfied: pyerfa&gt;=2.0 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from astropy-&gt;pysm3) (2.0.0.1)\nRequirement already satisfied: PyYAML&gt;=3.13 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from astropy-&gt;pysm3) (6.0)\nRequirement already satisfied: scipy in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from healpy-&gt;pysm3) (1.9.3)\nRequirement already satisfied: matplotlib in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from healpy-&gt;pysm3) (3.6.2)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from numba-&gt;pysm3) (0.39.1)\nRequirement already satisfied: setuptools in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from numba-&gt;pysm3) (65.5.1)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from packaging&gt;=19.0-&gt;astropy-&gt;pysm3) (3.0.9)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (1.0.6)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (2.8.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (9.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from matplotlib-&gt;healpy-&gt;pysm3) (4.38.0)\nRequirement already satisfied: six&gt;=1.5 in /global/common/software/cmb/zonca/conda/pycmb/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;healpy-&gt;pysm3) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nimport healpy as hp\nimport pysm3\nfrom pysm3 import units as u\n%matplotlib inline"
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html#get-the-planck-beam-widths",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html#get-the-planck-beam-widths",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "Get the Planck beam widths",
    "text": "Get the Planck beam widths\n\n!wget -nc https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/LFI_RIMO_R3.31.fits\n\nFile ‘LFI_RIMO_R3.31.fits’ already there; not retrieving.\n\n\n\n\n!wget -nc https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/HFI_RIMO_R3.00.fits\n\nFile ‘HFI_RIMO_R3.00.fits’ already there; not retrieving.\n\n\n\n\nfrom astropy.io import fits\n\n\nfwhm = {}\n\nwith fits.open(\"LFI_RIMO_R3.31.fits\") as f:\n  for ch, ch_fwhm in zip(f[2].data[\"FREQUENCY\"], f[2].data[\"FWHM\"]):\n    fwhm[ch[0]] = ch_fwhm * u.arcmin\n\n\nwith fits.open(\"HFI_RIMO_R3.00.fits\") as f:\n  for ch, ch_fwhm in zip(f[1].data[\"FREQUENCY\"], f[1].data[\"FWHM\"]):\n    fwhm[ch[0]] = ch_fwhm * u.arcmin"
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html#select-a-channel",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html#select-a-channel",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "Select a channel",
    "text": "Select a channel\n“030” or “143”\n\nch = \"030\"\nfreq = float(ch) * u.GHz\n\n\ninstrument = \"LFI\" if freq &lt; 100 * u.GHz else \"HFI\"\nnside = 1024 if instrument == \"LFI\" else 2048\nrel = \"R3\" if instrument == \"LFI\" else \"R4\""
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html#load-planck-maps",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html#load-planck-maps",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "Load Planck maps",
    "text": "Load Planck maps\n\n!wget -nc https://irsa.ipac.caltech.edu/data/Planck/release_3/all-sky-maps/maps/LFI_SkyMap_030_1024_R3.00_full.fits\n\nFile ‘LFI_SkyMap_030_1024_R3.00_full.fits’ already there; not retrieving.\n\n\n\n\n!wget -nc https://irsa.ipac.caltech.edu/data/Planck/release_3/all-sky-maps/maps/HFI_SkyMap_143_2048_R4.00_full.fits\n\nFile ‘HFI_SkyMap_143_2048_R4.00_full.fits’ already there; not retrieving.\n\n\n\n\nm_planck = hp.read_map(f\"{instrument}_SkyMap_{ch}_{nside}_{rel}.00_full.fits\")\n\n\nm_planck = (m_planck * u.K_CMB).to(u.uK_CMB)\n\n\nhp.mollview(hp.remove_dipole(m_planck), max=1e3)"
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html#create-maps-with-pysm",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html#create-maps-with-pysm",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "Create maps with PySM",
    "text": "Create maps with PySM\n\nsky = pysm3.Sky(nside=nside, preset_strings=[\"rg1\"])\n\n\nm_pysm = sky.get_emission(freq)\n\n\nm_pysm_smoothed = pysm3.apply_smoothing_and_coord_transform(m_pysm, fwhm=fwhm[ch])\n\nhp.map2alm_lsq did not converge in 10 iterations, residual relative error is 0.69\n\n\n\nhp.mollview(m_pysm_smoothed[0], max=1e3)"
  },
  {
    "objectID": "posts/2022-12-01-radio-galaxies-websky-planck.html#visualize-a-point-source",
    "href": "posts/2022-12-01-radio-galaxies-websky-planck.html#visualize-a-point-source",
    "title": "Compare WebSky Radio Galaxies maps from PySM to Planck",
    "section": "Visualize a point source",
    "text": "Visualize a point source\nCheck for ringing around point sources\n\nmax_pix = m_pysm_smoothed[0].argmax()\n\n\nlon,lat= hp.pix2ang(nside, max_pix, lonlat=True)\n\n\nlon\n\n275.2809917355372\n\n\n\nlat\n\n-62.0850844513906\n\n\n\nhp.gnomview(m_pysm_smoothed[0], rot=(lon,lat))\n\n\n\n\n\n\n\n\n\nm_pysm_smoothed[0].max()\n\n\\(156551.45 \\; \\mathrm{\\mu K_{{RJ}}}\\)\n\n\n\nm_planck.max()\n\n\\(215075.31 \\; \\mathrm{\\mu K_{{CMB}}}\\)"
  },
  {
    "objectID": "posts/2022-10-05-singularity-expanse-tutorial.html",
    "href": "posts/2022-10-05-singularity-expanse-tutorial.html",
    "title": "Singularity on Expanse tutorial",
    "section": "",
    "text": "As one of my last tasks in XSEDE, I updated and improved the tutorial about running Singularity containers in the HPC system Espanse at my institution the San Diego Supercomputer Center.\nThe new version of the tutorial is available on the Github repository zonca/expanse_singularity\nSee the README.md file for the table of contents linking to the subsection.\nThe most important changes were focused on using pre-built containers, either from DockerHub or from the repository maintained by SDSC’s Marty Kandes, and on building custom images either via Sylabs Builder, a cloud-based service that builds containers on demand, or using a Virtual Machine on Jetstream 2, the NSF-funded Openstack cloud deployment."
  },
  {
    "objectID": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html",
    "href": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html",
    "title": "Gateways 2023 tutorial about Dask and JupyterHub on Kubernetes on Jetstream",
    "section": "",
    "text": "Tutorial slides\nJupyter notebooks on Github:zonca/dask-jetstream-tutorial\nTutorial notebooks complete with output cells as executed after the tutorial, in a gist\n\n\n\nVideo on Youtube"
  },
  {
    "objectID": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#tutorial",
    "href": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#tutorial",
    "title": "Gateways 2023 tutorial about Dask and JupyterHub on Kubernetes on Jetstream",
    "section": "",
    "text": "Tutorial slides\nJupyter notebooks on Github:zonca/dask-jetstream-tutorial\nTutorial notebooks complete with output cells as executed after the tutorial, in a gist\n\n\n\nVideo on Youtube"
  },
  {
    "objectID": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#support",
    "href": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#support",
    "title": "Gateways 2023 tutorial about Dask and JupyterHub on Kubernetes on Jetstream",
    "section": "Support",
    "text": "Support\nI have funding from Jetstream to help developers deploy Kubernetes and any related service to Jetstream, please contact me if you need any help or even just to check if any of this could fit your use-case."
  },
  {
    "objectID": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#deployment-of-the-tutorial-infrastructure",
    "href": "posts/2023-09-21-gateways-2023-dask-jupyterhub.html#deployment-of-the-tutorial-infrastructure",
    "title": "Gateways 2023 tutorial about Dask and JupyterHub on Kubernetes on Jetstream",
    "section": "Deployment of the tutorial infrastructure",
    "text": "Deployment of the tutorial infrastructure\nHere is the reference to all the step-by-step tutorial on how to deploy the infrastructure used in the tutorial:\n\nDeployment of Kubernetes via Kubespray on Jetstream 2\nDeploy JupyterHub on top of Kubernetes\nInstall cert-manager to provide HTTPS support\nGithub Authentication\nDask Gateway\nConfigure access to object store"
  },
  {
    "objectID": "posts/italian-classes-san-diego.html",
    "href": "posts/italian-classes-san-diego.html",
    "title": "Italian classes in San Diego",
    "section": "",
    "text": "My wife Maura D’Andrea is Principal and Teacher of Italian school of San Diego.\nAll instructors are mother-tongue Italian, they teach to kids and adults.\nSee the available Italian in-person classes in San Diego, they are split in 2 tracks, one for people looking to learn Italian, one for kids that have Italian heritage and want to follow the same program taught in Italy’s public school system.\nThe high school classes are also certified for High School credit both with San Diego Unified and San Dieguito districts for High School credit, see the Italian classes for teenagers\nFinally they have an extensive programs of in-person Italian classes for adults\nClasses are online and in-person in Kearny Mesa, San Diego.\nCiao e grazie!"
  },
  {
    "objectID": "posts/2023-03-06-gzip-healpix-healpy.html",
    "href": "posts/2023-03-06-gzip-healpix-healpy.html",
    "title": "Gzipping or not Gzipping HEALPix maps",
    "section": "",
    "text": "HEALPix maps are generally stored in FITS format, they can be Gzipped to save disk space. Reading gzipped maps is natively supported in healpy, however, it takes longer because maps are uncompressed on the fly.\nAs usual, the best way to assess this is to test. In this case, I am testing using the JupyterHub@NERSC running on a Cori shared CPU server, the maps have about 30% of unobserved pixels and are stored in double precision.\nFor this case, the performance loss due to compression is significant, in particular if accessing just a subset of the maps. Therefore it is hard to justify compression, unless storage is really a limiting factor.\nThis is heavily case-dependent, so a new test (possibly reusing this notebook) should be performed on a different dataset and machine.\nimport healpy as hp\nhp.version.__version__\n\n'1.16.2'\ncd /global/cfs/cdirs/cmbs4/dc/dc1/\n\n/global/cfs/cdirs/cmbs4/dc/dc1\n!rm -r test_readgzip || true\n%mkdir test_readgzip\ncd test_readgzip\n\n/global/cfs/cdirs/cmbs4/dc/dc1/test_readgzip"
  },
  {
    "objectID": "posts/2023-03-06-gzip-healpix-healpy.html#stage-data",
    "href": "posts/2023-03-06-gzip-healpix-healpy.html#stage-data",
    "title": "Gzipping or not Gzipping HEALPix maps",
    "section": "Stage data",
    "text": "Stage data\n\nfolders = [\"test1\", \"test2\",  \"test1gz\", \"test2gz\"]\nfolders_string = \" \".join(folders)\n\n\n!mkdir -p $folders_string\n\n\ninput_folder = \"../staging/noise_sim/outputs_rk/coadd/LAT0_CHLAT\"\nm = \"coadd_LAT0_CHLAT_f150_001of001_map.fits\"\ncov = \"coadd_LAT0_CHLAT_f150_001of001_cov.fits\"\n\n\nfor f in folders:\n    print(f)\n    !cp $input_folder/$m $input_folder/$cov $f\n\ntest1\ntest2\ntest1gz\ntest2gz"
  },
  {
    "objectID": "posts/2023-03-06-gzip-healpix-healpy.html#compression",
    "href": "posts/2023-03-06-gzip-healpix-healpy.html#compression",
    "title": "Gzipping or not Gzipping HEALPix maps",
    "section": "Compression",
    "text": "Compression\n\nfor f in folders:\n    if f.endswith(\"gz\"):\n        !time gzip $f/$m\n        !time gzip $f/$cov\n\n\nreal    3m24.095s\nuser    3m13.681s\nsys 0m4.717s\n\nreal    5m27.430s\nuser    5m13.233s\nsys 0m7.657s\n\nreal    3m22.517s\nuser    3m12.237s\nsys 0m4.510s\n\nreal    5m21.693s\nuser    5m7.241s\nsys 0m7.664s\n\n\n\nls test1gz\n\ncoadd_LAT0_CHLAT_f150_001of001_cov.fits.gz\ncoadd_LAT0_CHLAT_f150_001of001_map.fits.gz\n\n\n\nfor each in [m, cov]:\n    fits = !stat -c \"%s\" test1/$each\n    each += \".gz\"\n    gz = !stat -c \"%s\" test1gz/$each\n    print(f\"{each} compression factor: {int(gz[0])/int(fits[0]):.0%}\")\n\ncoadd_LAT0_CHLAT_f150_001of001_map.fits.gz compression factor: 63%\ncoadd_LAT0_CHLAT_f150_001of001_cov.fits.gz compression factor: 42%"
  },
  {
    "objectID": "posts/2023-03-06-gzip-healpix-healpy.html#benchmark-map-access",
    "href": "posts/2023-03-06-gzip-healpix-healpy.html#benchmark-map-access",
    "title": "Gzipping or not Gzipping HEALPix maps",
    "section": "Benchmark map access",
    "text": "Benchmark map access\n\n%%time\n\n_ = hp.read_map(f\"test1/{m}\", (0,1,2))\n\nCPU times: user 33.2 s, sys: 26.4 s, total: 59.6 s\nWall time: 1min 6s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test1gz/{m}.gz\", (0,1,2))\n\nCPU times: user 1min 36s, sys: 24.7 s, total: 2min 1s\nWall time: 2min 2s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test2/{m}\", 0)\n\nCPU times: user 10.6 s, sys: 9.98 s, total: 20.6 s\nWall time: 27.3 s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test2gz/{m}.gz\", 0)\n\nCPU times: user 1min 12s, sys: 11.8 s, total: 1min 24s\nWall time: 1min 25s"
  },
  {
    "objectID": "posts/2023-03-06-gzip-healpix-healpy.html#benchmark-noise-covariance-access",
    "href": "posts/2023-03-06-gzip-healpix-healpy.html#benchmark-noise-covariance-access",
    "title": "Gzipping or not Gzipping HEALPix maps",
    "section": "Benchmark noise covariance access",
    "text": "Benchmark noise covariance access\n\n%%time\n\n_ = hp.read_map(f\"test1/{cov}\", (0,1,2,3,4,5))\n\nCPU times: user 1min 13s, sys: 50 s, total: 2min 3s\nWall time: 2min 23s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test1gz/{cov}.gz\", (0,1,2,3,4,5))\n\nCPU times: user 3min 10s, sys: 54.9 s, total: 4min 5s\nWall time: 4min 6s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test2/{cov}\", 0)\n\nCPU times: user 10.9 s, sys: 13.5 s, total: 24.4 s\nWall time: 44.2 s\n\n\n\n%%time\n\n_ = hp.read_map(f\"test2gz/{cov}.gz\", 0)\n\nCPU times: user 2min 7s, sys: 19.5 s, total: 2min 27s\nWall time: 2min 28s"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom astropy import units as u\nimport ducc0\nfrom time import time\nimport multiprocessing\nnthreads = multiprocessing.cpu_count()"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#objective",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#objective",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "Objective",
    "text": "Objective\nWe compare the transform from spherical harmonics space to pixel space and back using HEALPix and Gauss-Legendre pixelization. The context is the evaluation of sky emission models for PySM, we have an input power law with simulated small-scales, we need to transform to pixel space to evaluate the model (multiply maps, take exponents), then back to spherical harmonics to smooth the map and back to the output map.\nOutput a model at \\(N_{side}=2048\\) with a \\(\\ell_{max} = 3 N_{side} -1\\). Using HEALPix, we need to do modelling at higher \\(N_{side}\\) to avoid integration issues in map2alm between \\(\\ell=2 N_{side}\\) and \\(\\ell=3 N_{side}\\).\nSo we evaluate the models at \\(N_{side}=4096\\) with \\(\\ell_{max} = 2 N_{side} -1\\).\n\ntarget_nside = 2048\ntarget_lmax = 3 * target_nside - 1\n\n\nmodelling_nside = 4096\nmodelling_lmax = 2* modelling_nside"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#input-model",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#input-model",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "Input model",
    "text": "Input model\nWe assume as input a Intensity only power-law:\n$ A ^ $\n\ndef power_law(ell, amplitude, gamma):\n    np.seterr(divide = 'ignore') \n    out = amplitude * ell ** gamma\n    out[:1] = 0\n    return out\n\nwith parameters:\n\ngamma = -1\namplitude = 1\n\n\nell = np.arange(modelling_lmax + 1, dtype=np.float32)\nell_norm = ell * (ell + 1) / (2*np.pi)\nell_norm[0] = 1\n\n\ninput_power_law = power_law(ell, amplitude, gamma)\ninput_power_spectrum = input_power_law / ell_norm\n\nand we create a realization of this spectrum:\n\nnp.random.seed(8888)\nalm = hp.synalm(input_power_spectrum)"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#healpix-alm---map---alm",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#healpix-alm---map---alm",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "HEALPix alm -> map -> alm",
    "text": "HEALPix alm -&gt; map -&gt; alm\nIn PySM we can probably skip this first step and directly provide maps at all resolutions precomputed.\n\ntimings = {}\n\n\nt0=time()\nhealpix_map = hp.alm2map(alm, nside=modelling_nside)\ntimings[\"H_alm2map\"] = round(time()-t0)\nprint(f\"Alm2map, nside {modelling_nside}, lmax {modelling_lmax}\")\n\nAlm2map, nside 4096, lmax 8192\n\n\nOnce we are in pixel space, we can evaluate the model, generally multiplying multiple maps. Then we need to transform back to spherical harmonics space to apply the instrument beam window function, we only need to go to \\(1.5 N_{side}\\) when we transform back, so we safely use pixel weights:\n\nt0 = time()\nalm_from_m = hp.map2alm(healpix_map, use_pixel_weights=True, lmax=target_lmax)\ntimings[\"H_map2alm\"] = round(time()-t0)\nprint(f\"Map2alm, lmax {target_lmax}\")\ncl_from_m = hp.alm2cl(alm_from_m)\n\nMap2alm, lmax 6143"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#gauss-legendre-alm---map---alm",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#gauss-legendre-alm---map---alm",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "Gauss-Legendre alm -> map -> alm",
    "text": "Gauss-Legendre alm -&gt; map -&gt; alm\nWe can do the equivalent with Gauss-Legendre pixelization:\n\nmodelling2target_lmax = []\nlclip = target_lmax\nfor m in range(lclip+1):\n    modelling2target_lmax.append(hp.Alm.getidx(modelling_lmax, np.arange(m, lclip+1), m))\nmodelling2target_lmax = np.concatenate(modelling2target_lmax)\n\n\nalm_target = alm[modelling2target_lmax]\n\n\n# set maximum multipole moment\nlmax = target_lmax\n# maximum m.\nmmax = lmax\n\n# Number of pixels per ring. Must be &gt;=2*lmax+1, but I'm choosing a larger\n# number for which the FFT is faster.\nnlon = 2*lmax+2\n\n# create a set of spherical harmonic coefficients to transform\n# Libsharp works exclusively on real-valued maps. The corresponding harmonic\n# coefficients are termed a_lm; they are complex numbers with 0&lt;=m&lt;=lmax and\n# m&lt;=l&lt;=lmax.\n# Symmetry: a_l,-m = (-1)**m*conj(a_l,m).\n# The symmetry implies that all coefficients with m==0 are purely real-valued.\n# The a_lm are stored in a 1D complex-valued array, in the following order:\n# a_(0,0), a(1,0), ..., a_(lmax,0), a(1,1), a(2,1), ... a(lmax,1), ..., a(lmax, mmax)\n\n# number of required a_lm coefficients\nnalm = ((mmax+1)*(mmax+2))//2 + (mmax+1)*(lmax-mmax)\n# get random a_lm\nrng = np.random.default_rng(42)\n#alm = rng.uniform(-1., 1., nalm) + 1j*rng.uniform(-1., 1., nalm)\n# make a_lm with m==0 real-valued\nalm_target[0:target_lmax+1].imag = 0.\n# add an extra leading dimension to the a_lm. This is necessary since for\n# transforms with spin!=0 two a_lm sets are required instead of one.\nalm_target = alm_target.reshape((1,-1))\n\nprint(\"testing Gauss-Legendre grid with lmax+1 rings\")\n\n# Number of iso-latitude rings required for Gauss-Legendre grid\nnlat = lmax+1\n\n# go from a_lm to map\nt0 = time()\nGL_map = ducc0.sht.experimental.synthesis_2d(\n    alm=alm_target, ntheta=nlat, nphi=nlon, lmax=lmax, mmax=mmax, spin=0,\n    geometry=\"GL\", nthreads=nthreads)\ntimings[\"GL_alm2map\"] = round(time()-t0)\nprint(\"time for map synthesis: {}s\".format(timings[\"GL_alm2map\"]))\n\n# transform back to a_lm\n\nt0 = time()\nalm2 = ducc0.sht.experimental.analysis_2d(\n    map=GL_map, lmax=lmax, mmax=mmax, spin=0, geometry=\"GL\", nthreads=nthreads)\ntimings[\"GL_map2alm\"] = round(time()-t0)\nprint(\"time for map analysis: {}s\".format(timings[\"GL_map2alm\"]))\n\ntesting Gauss-Legendre grid with lmax+1 rings\ntime for map synthesis: 6s\ntime for map analysis: 6s\n\n\nA Gauss-Legendre map is a 2D array where each row is a equilatitude ring, we can also project it to Mollweide with matplotlib:\n\nplt.imshow(GL_map[0]);\n\n\n\n\n\n\n\n\n\nax = plt.axes(projection='mollweide')\nax.grid()\nax.imshow(GL_map[0], extent=[0, 1, 0, 1], aspect=ax.get_aspect(), transform=ax.transAxes);\n\n\n\n\n\n\n\n\n\n# projection conventions are different from healpy\nhp.mollview(healpix_map, rot=[-180], flip=\"geo\")\nhp.graticule()\n\n0.0 180.0 -180.0 180.0\n\n\n\n\n\n\n\n\n\n\ncl_GL = hp.alm2cl(alm2)"
  },
  {
    "objectID": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#compare-the-2-approaches",
    "href": "posts/2021-11-02-healpix-gauss-legendre-pysm.html#compare-the-2-approaches",
    "title": "Compare HEALPix and Gauss-Legendre pixelizations for sky emission modelling",
    "section": "Compare the 2 approaches",
    "text": "Compare the 2 approaches\nWe want to compute errors just until the target \\(N_{side}\\):\n\nL2_GL = ducc0.misc.l2error(alm_target[0, 1:], alm2[0, 1:])\nL2_healpix = ducc0.misc.l2error(alm_target[0, 1:], alm_from_m[1:])\n\n\ntarget_ell = np.arange(target_lmax + 1)\ntarget_ell_norm = target_ell * (target_ell + 1) / (2*np.pi)\ntarget_ell_norm[0] = 1\n\nWe can compare the output spectra and the error, the output power spectra are the same within machine precision, but the \\(a_{\\ell m}\\) are not:\n\nplt.plot(cl_GL[0]  - cl_from_m)\nplt.legend()\nplt.grid()\nplt.title(\"Difference between the power spectra\");\n\nNo handles with labels found to put in legend.\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(9, 5))\n\ncl = cl_from_m*target_ell_norm/input_power_law[:len(target_ell)]\ncl[0] = 0\nstd = cl.std()\nplt.axvline(np.pi/hp.nside2resol(target_nside), color=\"gray\")\n\nplt.plot(target_ell, cl, label=f\"HEALPix N_side {modelling_nside} Error: {L2_healpix:.2g}\")\nplt.plot(target_ell, cl_GL[0] * target_ell_norm / input_power_law[:len(target_ell)], label=f\"Gauss-Legendre Error: {L2_GL:.2g}\")\n\nplt.legend()\nplt.grid()\nplt.title(f\"output / input spectrum, input map nside {modelling_nside} powerlaw gamma={gamma}\")\nplt.ylabel(\"$\\ell(\\ell+1)C_\\ell/2\\pi [\\mu K_{RJ}]$\")\nplt.xlabel((\"$\\ell$\"))\nplt.axhline(1, color=\"black\")\nplt.savefig(f\"spec_ratio_HEALPix_GL.png\")\nplt.xlim(1, target_lmax+100);\n\n\n\n\n\n\n\n\n\nTime necessary for the transforms\nIn seconds\n\ntimings\n\n{'H_alm2map': 34, 'H_map2alm': 23, 'GL_alm2map': 6, 'GL_map2alm': 6}\n\n\n\n\nMemory usage in pixel space\nSize of temperature-only maps with the requested \\(\\ell_{max}\\) for GL and \\(N_{side}\\) for HEALPix\n\n(GL_map.nbytes * u.byte).to(u.GB)\n\n\\(0.60397978 \\; \\mathrm{Gbyte}\\)\n\n\n\n(healpix_map.nbytes * u.byte).to(u.GB)\n\n\\(1.6106127 \\; \\mathrm{Gbyte}\\)"
  },
  {
    "objectID": "posts/2021-04-27-correct-beam-healpy.html",
    "href": "posts/2021-04-27-correct-beam-healpy.html",
    "title": "Correct a power spectrum by the instrument beam with healpy",
    "section": "",
    "text": "If you are analyzing a map from an instrument with a specific beam width, you can correct the power spectrum by the smoothing factor caused by that beam and obtain a better approximation of the power spectrum of the orignal sky.\n\nimport healpy as hp\nimport numpy as np\nimport astropy.units as u\nhp.disable_warnings()\n\n\nm, h = hp.read_map(\n    \"https://portal.nersc.gov/project/cmb/so_pysm_models_data/equatorial/dust_T_ns512.fits\", h=True\n)\n\n\nhp.mollview(m, min=0, max=1000, title=\"Dust map\", unit=\"uK_RJ\")\n\n\n\n\n\n\n\n\n\ncl = hp.anafast(m)\n\nIn this case we assume that the dust map from PySM is the true sky, then we apply a smoothing caused by the beam\n\nbeam = 30 * u.arcmin\n\n\nm_smoothed = hp.smoothing(m, fwhm=beam.to_value(u.radian))\n\n\ncl_smoothed = hp.anafast(m_smoothed)\n\nWe can get the transfer function of the beam, generally referred as \\(B_\\ell\\):\n\nbl = hp.gauss_beam(fwhm=beam.to_value(u.radian), lmax=len(cl)-1)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.loglog(bl)\nplt.title(\"Beam window function\")\nplt.xlabel(\"$\\ell$\");\n\n\n\n\n\n\n\n\n\nhp.mollview(m_smoothed, min=0, max=1000)\n\n\n\n\n\n\n\n\nWe can recover the input \\(C_\\ell\\) as $C_^{input} = $:\n\nplt.style.use(\"seaborn-talk\")\nplt.loglog(cl, label=\"cl\")\nplt.plot(cl_smoothed, label=\"cl smoothed\")\nplt.plot(cl_smoothed/bl**2, label=\"cl smoothed corrected\")\nplt.xlim(1, len(cl)+100)\nplt.axvline(1100, color=\"gray\", ls=\"--\", label=\"$\\ell=1100$\");\nplt.legend()\nplt.grid();\n\n\n\n\n\n\n\n\nHowever, once the smoothed \\(C_\\ell\\) reach machine precision, there is no more signal left, the beam deconvolution then causes extra noise. We need to limit our analysis to a range of \\(\\ell\\) before the numerical error dominates, in this case for example, \\(\\ell=1100\\)."
  },
  {
    "objectID": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html",
    "href": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html",
    "title": "Compute the Planck CMB temperature power spectrum with healpy anafast",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nimport os\nimport astropy.units as u\nimport matplotlib.pyplot as plt\n%matplotlib inline\nIn this notebook we will load the Planck CMB-only temperature map and try to reproduce the Planck CMB power spectrum in temperature just using healpy."
  },
  {
    "objectID": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#load-data",
    "href": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#load-data",
    "title": "Compute the Planck CMB temperature power spectrum with healpy anafast",
    "section": "Load data",
    "text": "Load data\nFirst we load the Planck data release 3 CMB-only temperature produced by Commander by separating it out from galactic foregrounds:\n\n!wget https://irsa.ipac.caltech.edu/data/Planck/release_3/all-sky-maps/maps/component-maps/cmb/COM_CMB_IQU-commander_2048_R3.00_full.fits\n\n--2021-02-28 09:59:21--  https://irsa.ipac.caltech.edu/data/Planck/release_3/all-sky-maps/maps/component-maps/cmb/COM_CMB_IQU-commander_2048_R3.00_full.fits\nResolving irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)... 134.4.54.87\nConnecting to irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)|134.4.54.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1610660160 (1.5G) [image/x-fits]\nSaving to: ‘COM_CMB_IQU-commander_2048_R3.00_full.fits’\n\nCOM_CMB_IQU-command 100%[===================&gt;]   1.50G  43.1MB/s    in 29s     \n\n2021-02-28 09:59:50 (53.4 MB/s) - ‘COM_CMB_IQU-commander_2048_R3.00_full.fits’ saved [1610660160/1610660160]\n\n\n\n\nfilename = 'COM_CMB_IQU-commander_2048_R3.00_full.fits'\ncmb_map = hp.read_map(filename)\n\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:369: UserWarning: If you are not specifying the input dtype and using the default np.float64 dtype of read_map(), please consider that it will change in a future version to None as to keep the same dtype of the input file: please explicitly set the dtype if it is important to you.\n  \"If you are not specifying the input dtype and using the default \"\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:391: UserWarning: NSIDE = 2048\n  warnings.warn(\"NSIDE = {0:d}\".format(nside))\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:400: UserWarning: ORDERING = NESTED in fits file\n  warnings.warn(\"ORDERING = {0:s} in fits file\".format(ordering))\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:426: UserWarning: No INDXSCHM keyword in header file : assume IMPLICIT\n  warnings.warn(\"No INDXSCHM keyword in header file : \" \"assume {}\".format(schm))\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:428: UserWarning: INDXSCHM = IMPLICIT\n  warnings.warn(\"INDXSCHM = {0:s}\".format(schm))\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/fitsfunc.py:486: UserWarning: Ordering converted to RING\n  warnings.warn(\"Ordering converted to RING\")\n\n\n\n!wget https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/masks/COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits\n\n--2021-02-28 09:59:53--  https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/masks/COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits\nResolving irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)... 134.4.54.87\nConnecting to irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)|134.4.54.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 201335040 (192M) [image/x-fits]\nSaving to: ‘COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits’\n\nCOM_Mask_CMB-common 100%[===================&gt;] 192.01M  61.8MB/s    in 3.1s    \n\n2021-02-28 09:59:57 (61.8 MB/s) - ‘COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits’ saved [201335040/201335040]\n\n\n\n\nhp.mollview(cmb_map, min=-1e-3, max=1e-3, title=\"CMB only temperature map\", unit=\"K\")\n\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/projaxes.py:920: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n  newcm.set_over(newcm(1.0))\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/projaxes.py:921: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n  newcm.set_under(bgcolor)\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/projaxes.py:922: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. In future versions, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = copy.copy(mpl.cm.get_cmap(\"viridis\"))\n  newcm.set_bad(badcolor)\n/home/zonca/miniconda3/envs/so/lib/python3.7/site-packages/healpy/projaxes.py:211: MatplotlibDeprecationWarning: Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it.\n  **kwds\n\n\n\n\n\n\n\n\n\nWe see there is residual galactic emission we should mask. The contamination just close to the galactic plane, so we could run anafast and specify a few degrees of gal_cut.\nHowever, let’s exercise also how to use one of the Planck mask and use it to mask:\n\npath = 'COM_Mask_CMB-common-Mask-Int_2048_R3.00.fits'\nmask = hp.read_map(path)\nmap_masked = hp.ma(cmb_map)\nmap_masked.mask = np.logical_not(mask)\n\n\nhp.mollview(map_masked, min=-1e-3, max=1e-3)\n\n\n\n\n\n\n\n\nFinally we load the binned TT CMB power spectrum that will be our target:\n\n!wget https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/cosmoparams/COM_PowerSpect_CMB-TT-binned_R3.01.txt\n\n--2021-02-28 10:00:04--  https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/cosmoparams/COM_PowerSpect_CMB-TT-binned_R3.01.txt\nResolving irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)... 134.4.54.87\nConnecting to irsa.ipac.caltech.edu (irsa.ipac.caltech.edu)|134.4.54.87|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7143 (7.0K) [text/plain]\nSaving to: ‘COM_PowerSpect_CMB-TT-binned_R3.01.txt.2’\n\nCOM_PowerSpect_CMB- 100%[===================&gt;]   6.98K  --.-KB/s    in 0s      \n\n2021-02-28 10:00:04 (174 MB/s) - ‘COM_PowerSpect_CMB-TT-binned_R3.01.txt.2’ saved [7143/7143]\n\n\n\n\n!head -3 COM_PowerSpect_CMB-TT-binned_R3.01.txt\n\n# l                Dl               -dDl             +dDl             BestFit          \n  4.77112240e+01   1.47933552e+03   5.07654876e+01   5.07654876e+01   1.46111304e+03\n  7.64716065e+01   2.03496833e+03   5.47101576e+01   5.47101576e+01   2.06238073e+03\n\n\n\ncmb_binned_spectrum = np.loadtxt('COM_PowerSpect_CMB-TT-binned_R3.01.txt')"
  },
  {
    "objectID": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#compute-the-power-spectrum",
    "href": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#compute-the-power-spectrum",
    "title": "Compute the Planck CMB temperature power spectrum with healpy anafast",
    "section": "Compute the power spectrum",
    "text": "Compute the power spectrum\n\nAlways use use_pixel_weights=True in anafast to have a more precise spectrum estimation\nIf you compute the spectrum on the partial sky, first order correction is to divide by the sky fraction to retrieve the spectrum over the full sky\n\n\nlmax = 3000\n\n\ntest_cls_meas_frommap = hp.anafast(map_masked, lmax=lmax, use_pixel_weights=True)\n\n\nll = np.arange(lmax+1)\n\n\nsky_fraction = len(map_masked.compressed()) / len(map_masked)\n\n\nprint(f\"The map covers {sky_fraction:.1%} of the sky\")\n\nThe map covers 77.9% of the sky\n\n\n\nplt.style.use(\"seaborn-poster\")\n\n\nk2muK = 1e6\n\nPower spectra are generally plotted as \\(D_\\ell\\) which is defined as \\(\\dfrac{\\ell(\\ell+1)}{2 \\pi}C_\\ell\\), so we need to apply that factor to the \\(C_\\ell\\) calculated from the map.\n\nplt.plot(cmb_binned_spectrum[:,0], cmb_binned_spectrum[:,1], '--', alpha=1, label='Planck 2018 PS release')\nplt.plot(ll, ll*(ll+1.)*test_cls_meas_frommap*k2muK**2/2./np.pi / sky_fraction, '--', alpha=0.6, label='Planck 2018 PS from Data Map')\nplt.xlabel(r'$\\ell$')\nplt.ylabel(r'$D_\\ell~[\\mu K^2]$')\nplt.grid()\nplt.legend(loc='best')\n\n\n\n\n\n\n\n\nGood, we can reproduce the first peak, but we see that power at small scales is suppressed due to the beam."
  },
  {
    "objectID": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#correct-for-the-beam",
    "href": "posts/2021-02-28-compute-planck-spectra-healpy-anafast.html#correct-for-the-beam",
    "title": "Compute the Planck CMB temperature power spectrum with healpy anafast",
    "section": "Correct for the beam",
    "text": "Correct for the beam\nReading the documentation of the Planck commander release we see that the output has a resolution of 5 arcminutes. Therefore as a first order correction of the beam, we can divide the power spectrum by the square of the beam window function.\n\nw_ell = hp.gauss_beam((5*u.arcmin).to_value(u.radian), lmax=lmax)\n\n\nplt.plot(cmb_binned_spectrum[:,0], cmb_binned_spectrum[:,1], '--', alpha=1, label='Planck 2018 PS release')\nplt.plot(ll, ll*(ll+1.)*test_cls_meas_frommap*k2muK**2/2./np.pi / sky_fraction / w_ell**2,\n         alpha=0.6, label='Planck 2018 PS from Data Map (beam corrected)')\nplt.xlabel(r'$\\ell$')\nplt.ylabel(r'$D_\\ell~[\\mu K^2]$')\nplt.grid()\nplt.legend(loc='best');\n\n\n\n\n\n\n\n\nVery good, we were also be able to reproduce the second and third peak, after \\(\\ell\\) of about 1000, we start to overestimate the power spectrum.\nThis is as far as we can go with only healpy, the next step would be to properly handle the effect of masking. Computing the power spectrum on a masked map tends to create coupling between different scales, so power at large scales can contaminate the power spectrum at smaller scales. Next we can compute the power spectrum with a more sophisticated tool like NaMaster https://github.com/LSSTDESC/NaMaster which can correct for that."
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html",
    "href": "posts/2020-09-30-planck-spectra-healpy.html",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "",
    "text": "In this tutorial we will load CMB power spectra generated by the cosmological parameters measured by Planck, we will create a realization of the Spherical Harmonics coefficients \\(a_{\\ell m}\\), save it to disk, and use them to create maps at different resolutions.\nimport pandas as pd\nimport healpy as hp\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# filter out all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(plt.style.available)\nplt.style.use('seaborn-talk')\nFirst we go to the Planck data release page at IPAC, I prefer plain web pages instead of the funky javascript interface of the Planck Legacy Archive. The spectra are relegated to the Ancillary data section:\nhttps://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/\nWe can choose one of the available spectra, I would like the spectrum from theory, BaseSpectra, and download it locally:\n!wget https://irsa.ipac.caltech.edu/data/Planck/release_3/ancillary-data/cosmoparams/COM_PowerSpect_CMB-base-plikHM-TTTEEE-lowl-lowE-lensing-minimum-theory_R3.01.txt\nNow, I couldn’t find any documentation about the format of those spectra, there are no units and we don’t know if those spectra are just the \\(C_\\ell\\) or \\(\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell\\), it is probably a standard output of the CAMB software, anyway I prefer to cross check with some public plot.\npandas has trouble with the # at the beginning of the title line, the easiest is to edit that out with a file editor.\n!head -3 COM_PowerSpect_CMB-base-plikHM-TTTEEE-lowl-lowE-lensing-minimum-theory_R3.01.txt\nYou should not have # at the beginning of the first line\ninput_cl = pd.read_csv(\"COM_PowerSpect_CMB-base-plikHM-TTTEEE-lowl-lowE-lensing-minimum-theory_R3.01.txt\",\n                       delim_whitespace=True, index_col=0)\ninput_cl.head()\ninput_cl.plot(logx=True, logy=True, grid=True);\nWe can compare this to one of the plots from the Planck wiki:\ninput_cl.TT.plot(logx=False, logy=False, grid=True)\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\n\nplt.xlim([50, 2500]);\nVery good, they agree, therefore the data are in $ C_$, let’s transform to \\(C_\\ell [K^2]\\)\ninput_cl.head()\nlen(input_cl)\nlmax = input_cl.index[-1]\nprint(lmax)\ncl = input_cl.divide(input_cl.index * (input_cl.index+1) / (np.pi*2), axis=\"index\")\ncl /= 1e12\ncl.head()\n\\(\\ell\\) starts at 2, but all the healpy tools expect an array starting from zero, so let’s extend that.\ncl = cl.reindex(np.arange(0, lmax+1))\ncl.head()\ncl = cl.fillna(0)\ncl.head()"
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html#generate-a-cmb-map-realization",
    "href": "posts/2020-09-30-planck-spectra-healpy.html#generate-a-cmb-map-realization",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "Generate a CMB map realization",
    "text": "Generate a CMB map realization\nThe power spectrum gives us the distribution of power with the angular scale, if we want to create a map we could use hp.synfast, however the realization will be different each time that we run synfast, we could always use the same seed to make it reproducible. However, in case we want to have the same realization of a CMB map at different resolutions (\\(N_{side}\\)), it is better to generate a realization of the spectrum in Spherical Harmonics space, creating a set of \\(a_{\\ell m}\\), and then when needed transform them to a map with hp.alm2map.\nWe also want to make sure that the \\(a_{\\ell m}\\) have the same \\(\\ell_{max}\\) of the input \\(C_\\ell\\):\n\nseed = 583\nnp.random.seed(seed)\n\n\nalm = hp.synalm((cl.TT, cl.EE, cl.BB, cl.TE), lmax=lmax, new=True)\n\n\nhigh_nside = 1024\ncmb_map = hp.alm2map(alm, nside=high_nside, lmax=lmax)\n\nWe can double check that we got the order of magnitude correct, for example we can compare with one of the published Planck maps, the scale is \\(\\pm 300 \\mu K\\):\n\nhp.mollview(cmb_map[0], min=-300*1e-6, max=300*1e-6, unit=\"K\", title=\"CMB Temperature\")\n\nPolarization is generally an order of magnitude lower:\n\nfor m in cmb_map[1:]:\n    hp.mollview(m, min=-30*1e-6, max=30*1e-6, unit=\"K\", title=\"CMB Polarization\")"
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html#double-check-the-spectra-of-the-maps",
    "href": "posts/2020-09-30-planck-spectra-healpy.html#double-check-the-spectra-of-the-maps",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "Double check the spectra of the maps",
    "text": "Double check the spectra of the maps\nAs a final check, we can compare the spectra of the output maps to the input spectra.\n\ncl_from_map = hp.anafast(cmb_map, lmax=lmax, use_pixel_weights=True) * 1e12 # in muK^2\nell = np.arange(cl_from_map.shape[1])\ncl_from_map *= ell * (ell+1) / (2*np.pi)\n\n\nnp.median(cl_from_map, axis=1)\n\n\nplt.plot(cl_from_map[0], label=\"map TT\")\ninput_cl.TT.plot(logx=False, logy=False, grid=True)\nplt.legend();\n\n\ninput_cl.plot(logx=True, logy=True, grid=True)\nplt.plot(cl_from_map[0], label=\"map TT\")\nplt.plot(cl_from_map[1], label=\"map EE\")\nplt.plot(cl_from_map[2], label=\"map BB\")\nplt.plot(cl_from_map[3], label=\"map TE\")\n#plt.plot(cl_from_map[4], label=\"map EB\")\n#plt.plot(cl_from_map[5], label=\"map TB\")\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();\n\nThe scale is fine, we notice below that going through the discretization step of mapping the Spherical Harmonics into a map in pixel space and then trying to estimate the power spectrum again from that introduces some noise, on top of that, there is the issue of “Cosmic variance”, i.e. the \\(C_\\ell\\) estimates the variance of the \\(a_{\\ell m}\\) coefficients, but large angular scales, low \\(\\ell\\), there are only a small number of such coefficients, so the estimation itself is noisy see this work by Benjamin Wandelt:\n\nplt.plot(cl_from_map[0][:-2], label=\"map TT\")\ninput_cl.TT.plot(logx=False, logy=False, grid=True)\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();\n\n\nplt.plot(cl_from_map[1][:-2], label=\"map EE\")\ninput_cl.EE.plot(logx=False, logy=False, grid=True)\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();\n\n\nplt.plot(cl_from_map[2], label=\"map BB\")\ninput_cl.BB.plot(logx=False, logy=False, grid=True)\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();"
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html#save-the-a_ell-m-to-disk",
    "href": "posts/2020-09-30-planck-spectra-healpy.html#save-the-a_ell-m-to-disk",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "Save the \\(a_{\\ell m}\\) to disk",
    "text": "Save the \\(a_{\\ell m}\\) to disk\nWe can save in a standard FITS format\n\nhp.write_alm(f\"Planck_bestfit_alm_seed_{seed}.fits\", alm, overwrite=True)\n\n\n!ls *alm*fits"
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html#create-a-lower-resolution-map-from-the-same-a_ell-m",
    "href": "posts/2020-09-30-planck-spectra-healpy.html#create-a-lower-resolution-map-from-the-same-a_ell-m",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "Create a lower resolution map from the same \\(a_{\\ell m}\\)",
    "text": "Create a lower resolution map from the same \\(a_{\\ell m}\\)\nThe \\(a_{\\ell m}\\) can now be used to create the same realization of the CMB power spectrum at different resolution, it is not as straightforward as it might seem:\n\nlow_nside = 32\nlow_nside_lmax = 3*low_nside - 1\nlow_nside_cmb_map = hp.alm2map(alm, nside=low_nside)\n\n\ncl_low_nside = hp.anafast(low_nside_cmb_map, use_pixel_weights=True, lmax=low_nside_lmax) * 1e12\nell_low_nside = np.arange(cl_low_nside.shape[1])\ncl_low_nside *= ell_low_nside * (ell_low_nside+1) / (2*np.pi)\n\n\nplt.plot(cl_from_map[0], label=\"map TT\")\nplt.plot(cl_low_nside[0], label=\"low nside map TT\")\ninput_cl.TT.plot(logx=False, logy=False, grid=True)\nplt.axhline(low_nside_cmb_map.std()**2)\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();\n\nWe notice that the spectra is dominated by noise. The issue arises from the fact that Spherical Harmonics transforms are well defined only for band-limited signals, that means that at high \\(\\ell\\) the signal should be very low otherwise we are going to create artifacts, because we used \\(a_{\\ell m}\\) with a \\(\\ell_{max}\\) of ~2500 to create a map which could only represent a signal with a \\(\\ell_{max} &lt; 3 N_{side} - 1\\) (or \\(\\ell_{max} &lt; 2 N_{side}\\) to be conservative).\nUnfortunately the lmax argument of hp.map2alm doesn’t help because it is designed to specify the \\(\\ell_{max}\\) of the input \\(a_{\\ell m}\\).\nWe can fix the issue by zeroing the \\(a_{\\ell m}\\) coefficients above a specific \\(\\ell\\) using hp.almxfl, it automatically assumes that if a input \\(\\ell\\) is not provided, it is zero, therefore we can just provide an array of ones with the length of our target \\(\\ell_{max}\\).\n\nclip = np.ones(low_nside_lmax+1)\nalm_clipped = np.array([hp.almxfl(each, clip) for each in alm])\n\n\nlow_nside_cmb_map_clipped = hp.alm2map(alm_clipped, nside=low_nside)\n\n\ncl_low_nside_clipped = hp.anafast(low_nside_cmb_map_clipped, use_pixel_weights=True, lmax=low_nside_lmax) * 1e12\ncl_low_nside_clipped *= ell_low_nside * (ell_low_nside+1) / (2*np.pi)\n\n\nplt.plot(cl_from_map[0], label=\"map TT\", alpha=.5)\nplt.plot(cl_low_nside_clipped[0], label=\"low nside map TT clipped\", alpha=.8)\ninput_cl.TT.plot(logx=False, logy=False, grid=True)\nplt.axvline(2*low_nside, color=\"black\", linestyle=\"--\", label=\"ell=2*N_side\")\nplt.xlim([0, 800])\nplt.ylabel(\"$\\dfrac{\\ell(\\ell+1)}{2\\pi} C_\\ell~[\\mu K^2]$\")\nplt.xlabel(\"$\\ell$\")\nplt.legend();\n\nThanks to this the spectra is accurate at least up to \\(2 N_{side}\\) and it looks has a small bias towards higher values at higher \\(\\ell\\).\nFinally we can check by eye that the large scale features of the map (focus on a hot or cold spot) are the same in the 2 maps\n\nhp.mollview(cmb_map[0], title=\"CMB T map at $N_{side}$= \" + str(high_nside))\nhp.mollview(low_nside_cmb_map_clipped[0], title=\"CMB T map at $N_{side}$= \" + str(low_nside))\n\n\nhp.write_map(f\"map_nside_{low_nside}_from_Planck_bestfit_alm_seed_{seed}_K_CMB.fits\", low_nside_cmb_map_clipped, overwrite=True)\n\n\n!ls *.fits"
  },
  {
    "objectID": "posts/2020-09-30-planck-spectra-healpy.html#clip-a_ell-m-to-a-lower-ell_max",
    "href": "posts/2020-09-30-planck-spectra-healpy.html#clip-a_ell-m-to-a-lower-ell_max",
    "title": "Read and process Planck CMB power spectra with healpy",
    "section": "Clip \\(a_{\\ell m}\\) to a lower \\(\\ell_{max}\\)",
    "text": "Clip \\(a_{\\ell m}\\) to a lower \\(\\ell_{max}\\)\nIt was easy to use hp.almxfl to zero the coefficient above a threshold, however, the array remains very large, especially inconvenient if you need to save it to disk.\nWe want instead to actually clip the coefficients using the hp.Alm.getidx function which gives us the indices of a specific \\(\\ell,m\\) pair (they can be array):\n\nlclip = low_nside_lmax\n\n\n%%time\n\nclipping_indices = []\nfor m in range(lclip+1):\n    clipping_indices.append(hp.Alm.getidx(lmax, np.arange(m, lclip+1), m))\nprint(clipping_indices[:2])\nclipping_indices = np.concatenate(clipping_indices)\n\n\nalm_really_clipped = [each[clipping_indices] for each in alm]\n\n\nlow_nside_cmb_map_really_clipped = hp.alm2map(alm_clipped, nside=low_nside)\n\n\nhp.mollview(low_nside_cmb_map_clipped[0], title=\"zeroed - CMB T map at $N_{side}$= \" + str(low_nside))\nhp.mollview(low_nside_cmb_map_really_clipped[0], title=\"clipped - CMB T map at $N_{side}$= \" + str(low_nside))\n\n\n(low_nside_cmb_map_clipped[0] - low_nside_cmb_map_really_clipped[0]).sum()\n\n\nhp.write_alm(f\"Planck_bestfit_alm_seed_{seed}_lmax_{lclip}_K_CMB.fits\", alm_really_clipped, overwrite=True)\n\n\n!ls *.fits"
  },
  {
    "objectID": "posts/2020-09-04-unit-conversion-broadband-cmb.html",
    "href": "posts/2020-09-04-unit-conversion-broadband-cmb.html",
    "title": "Unit conversion with broadband detectors looking at the CMB",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pysm3 import units as u\nimport pysm3 as pysm\n%matplotlib inline\n\n\ndip = hp.synfast([0,1], lmax=1, nside=128) * u.V\n\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:438: FutureChangeWarning: The order of the input cl's will change in a future release.\nUse new=True keyword to start using the new order.\nSee documentation of healpy.synalm.\n  category=FutureChangeWarning,\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:824: UserWarning: Sigma is 0.000000 arcmin (0.000000 rad) \n  sigma * 60 * 180 / np.pi, sigma\n/home/zonca/zonca/p/software/healpy/healpy/sphtfunc.py:829: UserWarning: -&gt; fwhm is 0.000000 arcmin\n  sigma * 60 * 180 / np.pi * (2.0 * np.sqrt(2.0 * np.log(2.0)))\n\n\n\nhp.mollview(dip, unit=dip.unit)\n\n\n\n\n\n\n\n\nWe measure the sky with out broadband instrument, we assume we only measure the CMB solar dipole, initially the units are arbitrary, for example Volts of our instrument.\nNext we calibrate on the solar dipole, which is known to be 3.3 mK.\n\ncalibration_factor = 2 * 3.3 * u.mK_CMB / (dip.max() - dip.min())\n\n\ncalibration_factor\n\n\\(3.9033114 \\; \\mathrm{\\frac{mK_{{CMB}}}{V}}\\)\n\n\n\ncalibrated_dip = calibration_factor * dip\n\n\ncalibrated_dip\n\n\\([0.44224864,~0.41525913,~0.40329058,~\\dots,~-0.43028009,~-0.44224864,~-0.41525913] \\; \\mathrm{mK_{{CMB}}}\\)\n\n\n\nhp.mollview(calibrated_dip, unit=calibrated_dip.unit)\n\n\n\n\n\n\n\n\nFirst we simplify and consider a delta-frequency instrument at 300 GHz\n\ncenter_frequency = 300 * u.GHz\n\n\ndip_peak = calibrated_dip.max()\n\n\ncalibrated_dip.max()\n\n\\(3.3 \\; \\mathrm{mK_{{CMB}}}\\)\n\n\n\ncalibrated_dip.max().to(u.mK_RJ, equivalencies=u.cmb_equivalencies(center_frequency))\n\n\\(0.47252855 \\; \\mathrm{mK_{{RJ}}}\\)\n\n\n\ncalibrated_dip.max().to(u.MJy/u.sr, equivalencies=u.cmb_equivalencies(center_frequency))\n\n\\(1.3065993 \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\nNext we assume instead that we have a broadband instrument, of 20% bandwidth, with uniform response in that range. For simplicity, we only take 4 points.\n\nfreq = [270, 290, 310, 330] * u.GHz\n\n\nweights = [1, 1, 1, 1]\n\n\nweights /= np.trapz(weights, freq)\n\n\nweights\n\n\\([0.016666667,~0.016666667,~0.016666667,~0.016666667] \\; \\mathrm{\\frac{1}{GHz}}\\)\n\n\nThe instrument bandpass is defined in power so we can transform our signal in MJy/sr at the 4 reference frequencies, then integrate.\n\ndip_peak_MJysr = dip_peak.to(u.MJy/u.sr, equivalencies=u.cmb_equivalencies(freq))\n\n\ndip_peak_MJysr\n\n\\([1.4642815,~1.363255,~1.2471396,~1.1232462] \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\n\nintegrated_SR = np.trapz(dip_peak_MJysr * weights, freq)\n\n\nintegrated_SR\n\n\\(1.3013861 \\; \\mathrm{\\frac{MJy}{sr}}\\)\n\n\nThis is different than assuming uniform bandpass in \\(K_{CMB}\\), where instead we would recover the same result of the delta-bandpass:\n\nnp.trapz(dip_peak * weights, freq)\n\n\\(3.3 \\; \\mathrm{mK_{{CMB}}}\\)\n\n\n\nSR = u.MJy/u.sr\n\nWe use the PySM 3 function to compute unit conversion given a bandpass\n$ [unit_{out}] = [unit_{in}] { C_{unit_{out}}{Jy~sr{-1}}() g() d} $\nwhich comes from equating in power:\n$ [unit_{out}]{ C_{unit_{out}}{Jy~sr{-1}}() g() d} = [unit_{in}]C_{unit_{in}}{Jy~sr{-1}}() g() d$\n\nSR\n\n\\(\\mathrm{\\frac{MJy}{sr}}\\)\n\n\n\npysm.utils.bandpass_unit_conversion(freq, weights=weights, output_unit=u.mK_CMB, input_unit=SR)\n\n\\(2.5357578 \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)\n\n\n\nintegrated_SR * _\n\n\\(3.3 \\; \\mathrm{mK_{{CMB}}}\\)\n\n\n\n1 * u.mK_CMB / (1 * SR)\n\n\\(1 \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)\n\n\nWe can doublecheck the implementation of the PySM function by executing it here:\n\nK_CMB_to_MJysr = ((1*SR).to(u.mK_CMB, equivalencies=u.cmb_equivalencies(freq)))/(1*SR)\n\n\nK_CMB_to_MJysr\n\n\\([2.253665,~2.420677,~2.646055,~2.9379134] \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)\n\n\nIntegrating the K_CMB_to_MJysr conversion factor is wrong, we always need to do the integral in power, therefore we integrate the inverse and then take its inverse.\n\nnp.trapz(K_CMB_to_MJysr * weights, freq)\n\n\\(2.5541738 \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)\n\n\n\n1/np.trapz(1/K_CMB_to_MJysr * weights, freq)\n\n\\(2.5357578 \\; \\mathrm{\\frac{mK_{{CMB}}\\,sr}{MJy}}\\)"
  },
  {
    "objectID": "posts/2020-08-04-dask-array-rounding.html",
    "href": "posts/2020-08-04-dask-array-rounding.html",
    "title": "Dask array rounding",
    "section": "",
    "text": "This notebook is an extract from the Dask array Tutorial notebook, see also the Youtube SciPy 2020 class at https://www.youtube.com/watch?v=mqdglv9GnM8.\nWe notice that dask is automatically rounding float32 numbers to machine precision, which I think is the most sensible choice, but surprising difference compared to numpy."
  },
  {
    "objectID": "posts/2020-08-04-dask-array-rounding.html#create-the-input-data",
    "href": "posts/2020-08-04-dask-array-rounding.html#create-the-input-data",
    "title": "Dask array rounding",
    "section": "Create the input data",
    "text": "Create the input data\nNeeds only to be done once, defaults to ~4GB of data, you can reduce it by setting blocksize to a smaller number, e.g. 1000\n\nimport os\nimport h5py\nimport numpy as np\n\n\n%%time\n\nblocksize = 1000000\nnblocks = 1000\nshape = nblocks * blocksize\n\nif not os.path.exists('random.hdf5'):\n\n    with h5py.File('random.hdf5', mode='w') as f:\n        dset = f.create_dataset('/x', shape=(shape,), dtype='f4')\n        for i in range(0, shape, blocksize):\n            dset[i: i + blocksize] = np.random.exponential(size=blocksize)"
  },
  {
    "objectID": "posts/2020-08-04-dask-array-rounding.html#setup",
    "href": "posts/2020-08-04-dask-array-rounding.html#setup",
    "title": "Dask array rounding",
    "section": "Setup",
    "text": "Setup\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=24, processes=False)\n\n\n# Load data with h5py\n# this creates a pointer to the data, but does not actually load\nimport h5py\nimport os\nf = h5py.File('random.hdf5', mode='r')\ndset = f['/x']\n\n\ndset.dtype\n\ndtype('&lt;f4')\n\n\n\n!ls -lah data/random.hdf5\n\n-rw-r--r-- 1 zonca csb148 3.8G Jul 24 22:51 data/random.hdf5\n\n\nCompute sum using blocked algorithm\nBefore using dask, lets consider the concept of blocked algorithms. We can compute the sum of a large number of elements by loading them chunk-by-chunk, and keeping a running total.\nHere we compute the sum of this large array on disk by\n\nComputing the sum of each 1,000,000 sized chunk of the array\nComputing the sum of the 1,000 intermediate sums\n\nNote that this is a sequential process in the notebook kernel, both the loading and summing.\n\nlen(dset)\n\n1000000000\n\n\n\n# Compute sum of large array, one million numbers at a time\nsums = []\nfor i in range(0, 1000000000, 1000000):\n    chunk = dset[i: i + 1000000]  # pull out numpy array\n    sums.append(chunk.sum())\n\ntotal = sum(sums)\nprint(total)\n\n999976587.6875\n\n\nCreate dask.array object\nYou can create a dask.array Array object with the da.from_array function. This function accepts\n\ndata: Any object that supports NumPy slicing, like dset\nchunks: A chunk size to tell us how to block up our array, like (1000000,)\n\n\nimport dask.array as da\nimport numpy as np\nx = da.from_array(dset, chunks=(10000000,))\nx\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.00 GB\n40.00 MB\n\n\nShape\n(1000000000,)\n(10000000,)\n\n\nCount\n101 Tasks\n100 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n                                                                                                            1000000000 1\n\n\n\n\n\n\n\nx_float64 = x.astype(np.float64)\n\n\nx.sum().compute()\n\n999976700.0\n\n\nThe machine resolution of float32 is 1e-6, therefore everything after the 7th digit is garbage, so it is reasonable to remove it, otherwise it gives you the impression that the computation is more precise than it actually it. Still I am surprised dask does it, numpy above instead doesn’t care about that and prints all the digits.\nIf we need more precision, we need to increase the precision of the calculation, see below, but we are going to use a lot more memory, also, the input data were float32, so it is not very useful anyway, we should generate again the input with higher precision.\n\nnp.finfo(np.float32)\n\nfinfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32)\n\n\n\nx_float64.sum()\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n8 B\n8 B\n\n\nShape\n()\n()\n\n\nCount\n336 Tasks\n1 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nx_float64.sum().compute()\n\n999976584.1788422\n\n\n\nclient.shutdown()"
  },
  {
    "objectID": "posts/2020-06-20-white-noise-hitmap-uniform.html",
    "href": "posts/2020-06-20-white-noise-hitmap-uniform.html",
    "title": "Handle white noise with healpy 2 partial sky coverage",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport astropy.units as u\nhp.disable_warnings()\nIn this second notebook, we will handle a case of partial sky coverage.\n# Number based on Simons Observatory SAT UHF1 array of detectors\n\nnet = 10. * u.Unit(\"uK * sqrt(s)\")\n5 years with a efficiency of 20%:\nintegration_time_total = 5 * u.year * .2"
  },
  {
    "objectID": "posts/2020-06-20-white-noise-hitmap-uniform.html#download-a-hitmap",
    "href": "posts/2020-06-20-white-noise-hitmap-uniform.html#download-a-hitmap",
    "title": "Handle white noise with healpy 2 partial sky coverage",
    "section": "Download a hitmap",
    "text": "Download a hitmap\nWe can download a simulated hitmap for a Simons Observatory band, for now however, we assume a uniform coverage.\n\nhitmap_url = \"https://portal.nersc.gov/project/sobs/so_mapsims_data/v0.2/healpix/ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\"\n\n\n!wget $hitmap_url\n\n--2020-12-11 10:32:00--  https://portal.nersc.gov/project/sobs/so_mapsims_data/v0.2/healpix/ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\nResolving portal.nersc.gov (portal.nersc.gov)... 128.55.206.24, 128.55.206.26\nConnecting to portal.nersc.gov (portal.nersc.gov)|128.55.206.24|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3212515 (3.1M) [application/x-gzip]\nSaving to: ‘ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz.8’\n\nST0_UHF1_01_of_20.n 100%[===================&gt;]   3.06M  3.12MB/s    in 1.0s    \n\n2020-12-11 10:32:06 (3.12 MB/s) - ‘ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz.8’ saved [3212515/3212515]\n\n\n\n\nhitmap = hp.read_map(\"ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\")\n\n\nhitmap = hitmap &gt; 0\n\n\nhp.mollview(hitmap)"
  },
  {
    "objectID": "posts/2020-06-20-white-noise-hitmap-uniform.html#uniform-partial-sky-survey",
    "href": "posts/2020-06-20-white-noise-hitmap-uniform.html#uniform-partial-sky-survey",
    "title": "Handle white noise with healpy 2 partial sky coverage",
    "section": "Uniform partial sky survey",
    "text": "Uniform partial sky survey\nAs a reference, let’s first start with the trivial case of uniform full sky coverage, i.e. we spend the same amount of observation time in each pixel.\n\nnside = 512\nnpix = hp.nside2npix(nside)\nnpix_hit = hitmap.sum()\n\n\nstandard_deviation_per_pixel = (net / np.sqrt(integration_time_total/npix_hit)).decompose()\n\n\nstandard_deviation_per_pixel\n\n\\(1.9596849 \\times 10^{-6} \\; \\mathrm{K}\\)\n\n\n\nm = np.nan * np.ones(npix, dtype=np.double) * standard_deviation_per_pixel.unit\n\n\nm[hitmap] = np.random.normal(scale = standard_deviation_per_pixel.value, size=npix_hit) * standard_deviation_per_pixel.unit\n\n\nm = m.to(u.uK)\n\n\nm.value[np.isnan(m)] = hp.UNSEEN\n\n\nhp.mollview(m, unit=m.unit, title=\"White noise map\")"
  },
  {
    "objectID": "posts/2020-06-20-white-noise-hitmap-uniform.html#power-spectrum",
    "href": "posts/2020-06-20-white-noise-hitmap-uniform.html#power-spectrum",
    "title": "Handle white noise with healpy 2 partial sky coverage",
    "section": "Power spectrum",
    "text": "Power spectrum\n\nsky_fraction = np.mean(hitmap)\n\n\ncl = hp.anafast(m) / sky_fraction\n\n\ncl[100:].mean()\n\n1.5205726153293572e-05\n\n\n\nm[hitmap].std()\n\n\\(1.9595938 \\; \\mathrm{\\mu K}\\)\n\n\n\npixel_area = hp.nside2pixarea(nside)\n\n\nwhite_noise_cl = (standard_deviation_per_pixel**2 * pixel_area).to(u.uK**2)\n\n\nwhite_noise_cl\n\n\\(1.5341266 \\times 10^{-5} \\; \\mathrm{\\mu K^{2}}\\)\n\n\n\nwhite_noise_cl_full_sky = 3.9820426e-5 * u.uK**2\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.hlines(white_noise_cl.value, 0, len(cl), color=\"blue\",\n           label=\"White noise level\")\nplt.hlines(white_noise_cl_full_sky.value, 0, len(cl),\n           label=\"White noise level full sky\")\nplt.title(\"Fullsky white noise spectrum\")\nplt.legend()\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\");\n\n\n\n\n\n\n\n\n\nwhite_noise_cl / white_noise_cl_full_sky\n\n\\(0.38526121 \\; \\mathrm{}\\)\n\n\n\nsky_fraction\n\n0.38526121775309247"
  },
  {
    "objectID": "posts/2020-04-24-white-noise-angular-power-spectrum.html",
    "href": "posts/2020-04-24-white-noise-angular-power-spectrum.html",
    "title": "White noise NET in Radio-astronomy and Cosmology",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport astropy.units as u\nNoise-Equivalent-Temperature, it is a measure of sensitivity of a detector, in cosmology, it is often quoted in \\(\\mu K \\sqrt(s)\\), i.e. it is the sensitivity per unit time and can be divided by the integration time to get the actual standard deviation of the white noise of the instrument.\nFor example let’s consider a white noise NET of \\(200 \\mu K \\sqrt(s)\\)\nit means that if you integrate for 100 seconds for each pixel, the standard deviation will be \\(20 \\mu K\\).\nnet = 200 * u.Unit(\"uK * sqrt(s)\")\nnet\n\n\\(200 \\; \\mathrm{\\mu K\\,s^{1/2}}\\)\nintegration_time_per_pixel = 100 * u.s\nstandard_deviation = net / np.sqrt(integration_time_per_pixel)"
  },
  {
    "objectID": "posts/2020-04-24-white-noise-angular-power-spectrum.html#create-a-white-noise-map",
    "href": "posts/2020-04-24-white-noise-angular-power-spectrum.html#create-a-white-noise-map",
    "title": "White noise NET in Radio-astronomy and Cosmology",
    "section": "Create a white noise map",
    "text": "Create a white noise map\nNow that we have an estimate of the standard deviation per pixel, we can use numpy to create a map of gaussian white noise.\n\nnside = 128\nnpix = hp.nside2npix(nside)\n\n\nm = np.random.normal(scale = standard_deviation.value, size=npix) * standard_deviation.unit\n\n\nhp.mollview(m, unit=m.unit, title=\"White noise map\")"
  },
  {
    "objectID": "posts/2020-04-24-white-noise-angular-power-spectrum.html#power-spectrum",
    "href": "posts/2020-04-24-white-noise-angular-power-spectrum.html#power-spectrum",
    "title": "White noise NET in Radio-astronomy and Cosmology",
    "section": "Power spectrum",
    "text": "Power spectrum\nFinally we can compute the angular power spectrum with anafast, i.e. the power as a function of the angular scales, from low \\(\\ell\\) values for large angular scales, to high \\(\\ell\\) values for small angular scales.\nAt low \\(\\ell\\) there is not much statistics and the power spectrum is biased, but if we exclude lower ells, we can have an estimate of the white noise \\(C_\\ell\\) coefficients. We can then compare with the theoretical power computed as:\n\\[ C_\\ell = \\Omega_{pix}\\sigma^2 \\]\nWhere: \\(\\Omega_{pix}\\) is the pixel are in square-radians and \\(\\sigma^2\\) is the white noise standard variance.\n\ncl = hp.anafast(m)\n\n\ncl[100:].mean()\n\n0.02516797566530168\n\n\n\npixel_area = hp.nside2pixarea(nside)\n\n\nwhite_noise_cl = standard_deviation.value**2 * pixel_area\n\n\nwhite_noise_cl\n\n0.025566346464760685\n\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.hlines(white_noise_cl, 0, len(cl), label=\"White noise level\")\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\");"
  },
  {
    "objectID": "posts/2020-04-24-white-noise-angular-power-spectrum.html#masking",
    "href": "posts/2020-04-24-white-noise-angular-power-spectrum.html#masking",
    "title": "White noise NET in Radio-astronomy and Cosmology",
    "section": "Masking",
    "text": "Masking\nIn case we are removing some pixels from a map, for example to mask out a strong signal (e.g. the Milky Way), our estimate of the power spectrum on the partial sky is lower. However we assume that the properties of the noise will be the same also in the masked region. At first order, for simple masks, we can just correct for the amplitude by dividing the power spectrum by the sky fraction.\n\nm.value[len(m)//2-30000:len(m)//2+30000] = hp.UNSEEN\n\n\nhp.mollview(m, unit=m.unit, title=\"White noise map\")\n\n\n\n\n\n\n\n\n\ncl_masked = hp.anafast(m)\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.loglog(cl_masked, label=\"Map power spectrum (Masked)\", alpha=.7)\nplt.hlines(white_noise_cl, 0, len(cl), label=\"White noise level\")\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nsky_fraction = hp.mask_good(m).sum() / len(m)\nprint(sky_fraction)\n\n0.69482421875\n\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.loglog(cl_masked / sky_fraction, label=\"Map power spectrum (Masked) - corrected\", alpha=.7)\nplt.hlines(white_noise_cl, 0, len(cl), label=\"White noise level\")\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\")\nplt.legend();"
  },
  {
    "objectID": "posts/2024-02-09-kubernetes-gpu-jetstream2.html",
    "href": "posts/2024-02-09-kubernetes-gpu-jetstream2.html",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "",
    "text": "This work has been supported by Indiana University and is cross-posted on the Jetstream 2 official documentation website.\nThanks to work by Ana Espinoza, the standard recipe now supports GPUs out of the box and also supports hybrid clusters where some nodes are standard CPU nodes and some nodes have GPU.\nThe Jetstream 2 cloud includes 90 GPU nodes with 4 NVIDIA A100 each. If we want to leverage the GPUs inside Kubernetes pods, for example JupyterHub users, we both need to have a GPU-enabled ContainerD runtime and a compatible Docker image based off NVIDIA images."
  },
  {
    "objectID": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#deploy-kubernetes-with-nvidia-runtime",
    "href": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#deploy-kubernetes-with-nvidia-runtime",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Deploy Kubernetes with NVIDIA runtime",
    "text": "Deploy Kubernetes with NVIDIA runtime\nKubespray has built-in support for NVIDIA runtime, in a previous version of this tutorial I had a specific branch dedicated to supporting a cluster where all worker nodes had GPUs.\nTherefore it is just a matter of following the standard Kubespray deployment tutorial, configuring properly the variables in cluster.tfvars following the comments available there. In summary, for a GPU-only cluster we only set:\n supplementary_node_groups = \"gpu-node\"\nInstead for a hybrid cluster we need to set the number of worker nodes to zero and instead list explicitly all the nodes we want Terraform to create, specifying their name and if they should have a GPU or not.\nIf we deploy a hybrid GPU-CPU cluster in the default configuration from cluster.tfvars, we will have 2 CPU and 2 GPU nodes:\n&gt; kubectl get nodes\nNAME                              STATUS   ROLES           AGE   VERSION\nkubejetstream-1                   Ready    control-plane   44m   v1.25.6\nkubejetstream-k8s-node-nf-cpu-1   Ready    &lt;none&gt;          43m   v1.25.6\nkubejetstream-k8s-node-nf-cpu-2   Ready    &lt;none&gt;          43m   v1.25.6\nkubejetstream-k8s-node-nf-gpu-1   Ready    &lt;none&gt;          43m   v1.25.6\nkubejetstream-k8s-node-nf-gpu-2   Ready    &lt;none&gt;          43m   v1.25.6\nNext we need to install the k8s-device-plugin, at the moment it is just necessary to execute:\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.4/nvidia-device-plugin.yml\nHowever, make sure you check the latest k8s-device-plugin documentation.\nFor testing, you can run a simple GPU job, this is requesting a GPU, so it will automatically run on a GPU node if we have an hybrid cluster:\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      resources:\n        limits:\n          nvidia.com/gpu: 1 # requesting 1 GPU\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\nEOF\nand check the logs:\nkubectl logs gpu-pod\nThe output should be:\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone"
  },
  {
    "objectID": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#access-gpus-from-jupyterhub",
    "href": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#access-gpus-from-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Access GPUs from JupyterHub",
    "text": "Access GPUs from JupyterHub\nA Docker image derived from the NVIDIA Tensorflow image is available on DockerHub as zonca/nvidia-tensorflow-jupyterhub, the relevant Dockerfile is available on Github.\nAlso notice that this is configured to run JupyterHub 3.0.0 which should be used in conjunction with the Zero to JupyterHub Helm chart version 2.0.0.\nThen it is just a matter of modifying the install_jhub.sh script to pickup the additional configuration file by adding:\n--values gpu/jupyterhub_gpu.yaml\nNotice that some Docker images designed for GPU do not work on CPU, for example the official tensorflow image (as tested in February 2024). The image used here works fine on either type of node."
  },
  {
    "objectID": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#test-execution-on-gpu",
    "href": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#test-execution-on-gpu",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Test execution on GPU",
    "text": "Test execution on GPU\nFor testing, I have modified the Tensorflow tutorial for beginners to run on GPU, it is available in this Gist.\nYou can download it to your local machine and upload it to the GPU-enabled single user instance on Jetstream.\nDuring execution, the 3rd cell should show the available GPU device:\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nthen the Notebook should execute to completion with no errors, printing for each operation the device which executed it, i.e. the GPU. After checking that commands are properly executed on GPU, you should comment out tf.debugging.set_log_device_placement(True) to speed-up training."
  },
  {
    "objectID": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#hybrid-cluster-avoid-cpu-pods-running-on-gpu-nodes",
    "href": "posts/2024-02-09-kubernetes-gpu-jetstream2.html#hybrid-cluster-avoid-cpu-pods-running-on-gpu-nodes",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Hybrid cluster: avoid CPU pods running on GPU nodes",
    "text": "Hybrid cluster: avoid CPU pods running on GPU nodes\nOne problem of the above configuration is that users requesting a CPU pod could be spawned on a GPU node, therefore occupying resources that might be useful for GPU users. A possible fix is to taint all the GPU nodes creating a simple script to apply this command to all GPU nodes:\nkubectl taint node kubejetstream-k8s-node-nf-gpu-1 \"gpu=true:NoSchedule\"\nThen, I have already added a toleration to the GPU profile of jupyterhub_gpu.yaml, so now only users that select the GPU profile will spwan on GPU nodes."
  },
  {
    "objectID": "posts/2022-05-17-remove-cell-id-jupyter-notebook.html",
    "href": "posts/2022-05-17-remove-cell-id-jupyter-notebook.html",
    "title": "Remove unique cell id from Jupyter Notebooks",
    "section": "",
    "text": "I know! Jupyter is littering your git diff with randomly generated cell ids and nbstripout doesn’t remove them, (I’m sure they are useful for some reason).\nOpen the Notebook with your editor of choice, vim, then look for minor and set nbformat_minor to 4:\n\"nbformat_minor\": 4\nOpen and save again from Jupyter, cell ids should be gone for good!"
  },
  {
    "objectID": "posts/2022-04-27-monitor-restic-backups-kubernetes.html",
    "href": "posts/2022-04-27-monitor-restic-backups-kubernetes.html",
    "title": "Monitor Restic backups on Kubernetes",
    "section": "",
    "text": "For one of my production JupyterHub deployments on Kubernetes, I have setup an automated system to perform nightly backup of the user data, see the full tutorial on how to set it up.\nThe system writes the backups to Object store using Restic.\nIn this tutorial I’ll provide the configuration to have a CronJob on Kubernetes checking how old are the backups and be alerted if anything is not working using Healthchecks.io.\nHealthchecks.io is a free service that gives you a URL endpoint you should regularly send a GET request to, for example from a bash script, if they don’t receive a ping after a configurable amount of hours, they send an email."
  },
  {
    "objectID": "posts/2022-04-27-monitor-restic-backups-kubernetes.html#setup-the-system",
    "href": "posts/2022-04-27-monitor-restic-backups-kubernetes.html#setup-the-system",
    "title": "Monitor Restic backups on Kubernetes",
    "section": "Setup the system",
    "text": "Setup the system\nFirst register for a free account at https://healthchecks.io, configure an endpoint and get the related URL.\nCheckout my usual zonca/jupyterhub-deploy-kubernetes-jetstream repository from Github, enter the backup_volumes folder.\nMake sure you have the Restic password and the AWS-style credentials saved in text files in the same folder, make sure there is no newline at the end of the files (use vim -b with :set noeol, yes, this is a reminder for myself).\nbash create_aws_secret.sh\nThen enter the backup_volumes/monitor subfolder:\n\nEdit cronjob.yaml and enter your Healthchecks.io URL\nRun the backup_is_current.sh script locally to make sure it works properly\nRun the create_configmap_scripts.sh bash script to load the previous script into Kubernetes as a configmap\nModify the schedule in cronjob.yaml to “* * * * *“, so every minute it has a chance to run correctly\nFinally run kubectl apply -f cronjob.yaml to create the CronJob\nDebug until it works!\nIf you login to Healthchecks.io, you should see the GET requests coming in every minute.\nRelax, you’ll be notified if backups stop working for any reason."
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html",
    "title": "Custos authentication for JupyterHub",
    "section": "",
    "text": "Custos is a security middleware used to authenticate users to Airavata-based Science Gateways. It is relevant to the Science Gateways community to unify authentication and also authenticate users to JupyterHub using the same framework.\nCustos is a hosted solution managed by Indiana University, therefore it has no maintenance burden and supports CILogon so that users can login with credentials from almost all US Higher Education Institutions."
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html#requirements",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html#requirements",
    "title": "Custos authentication for JupyterHub",
    "section": "Requirements",
    "text": "Requirements\nI have been testing on Jetstream 2, however it should work easily on any Kubernetes deployment. If you also are testing on Jetstream 2, you can follow my previous tutorials to:\n\nDeploy Kubernetes\nDeploy JupyterHub\nDeploy Cert-Manager for SSL support"
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-custos",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-custos",
    "title": "Custos authentication for JupyterHub",
    "section": "Configure Custos",
    "text": "Configure Custos\nFirst you can request a new tenant on the Custos hosted service, for testing you can use the development version at:\nhttps://dev.portal.usecustos.org/\nthen, for production, switch to:\nhttps://portal.usecustos.org/\n\nLogin with CILogon (I tested using my XSEDE account)\nClick on create new tenant\nRedirect url https://your.jupyterhub.com/hub/oauth_callback\nDomain your.jupyterhub.com\nClient URI https://your.jupyterhub.com\nLogo URI, anything, for example I used my Github avatar\n\nAfter having completed the process, you should see that your tenant is in the “Requested” state, wait for the Custos admins to approve it."
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-jupyterhub",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-jupyterhub",
    "title": "Custos authentication for JupyterHub",
    "section": "Configure JupyterHub",
    "text": "Configure JupyterHub\nCustom authenticators for JupyterHub need to be installed in the Docker image used to run the hub pod. The Custos Authenticator for JupyterHub has a package on PyPI so it is easy to add it to the JupyterHub image. See this issue on zero-to-jupyterhub for more details\nThe Custos developers maintain Docker images which have this patch already applied, see the repository on DockerHub.\nWe can therefore modify the JupyterHub configuration (config_standard_storage.yaml in my tutorials) and add:\nhub:\n  image:\n    name: apachecustos/jupyter-hub-k8\n    tag: 1.2.0\nConsider that this will need to be updated if we change the version of the Helm recipe (currently 1.2.0)."
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-the-custos-authenticator",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html#configure-the-custos-authenticator",
    "title": "Custos authentication for JupyterHub",
    "section": "Configure the Custos Authenticator",
    "text": "Configure the Custos Authenticator\nOnce the Custos tenant has been approved, we can proceed to configure JupyterHub with the right credentials, again, modify config_standard_storage.yaml to add:\nhub:\n  extraConfig:\n      00-custos: |\n        from custosauthenticator.custos import CustosOAuthenticator\n        c.JupyterHub.authenticator_class = CustosOAuthenticator\n        c.CustosOAuthenticator.oauth_callback_url = \"https://your.jupyterhub.com/hub/oauth_callback\"\n        c.CustosOAuthenticator.client_id = \"custos-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        c.CustosOAuthenticator.client_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        c.CustosOAuthenticator.login_service = \"Custos Login\"\n        c.CustosOAuthenticator.custos_host= \"custos.scigap.org\"\nFinally you can test in your browser, you probably need to test with an account different than the one you used to setup the tenant, so for example if you used XSEDE, now use CILogon with your institution or use ORCID.\nWith this default configuration, any user that can login to Custos can also login to JupyterHub, so if you already have permissions setup for your Custos gateway, those will be also applied to JupyterHub."
  },
  {
    "objectID": "posts/2022-04-13-jupyterhub-custos-authentication.html#work-in-progress",
    "href": "posts/2022-04-13-jupyterhub-custos-authentication.html#work-in-progress",
    "title": "Custos authentication for JupyterHub",
    "section": "Work in progress",
    "text": "Work in progress\n\nUnderstand if we can login with owner account of tenant.\nFix the logout button"
  },
  {
    "objectID": "posts/2022-04-04-zarr_jetstream2.html",
    "href": "posts/2022-04-04-zarr_jetstream2.html",
    "title": "Use the distributed file format Zarr on Jetstream 2 object storage",
    "section": "",
    "text": "Zarr is a file format designed for cloud computing, see documentation.\nZarr is also supported by dask, the parallel computing framework for Python, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2022-04-04-zarr_jetstream2.html#zarr",
    "href": "posts/2022-04-04-zarr_jetstream2.html#zarr",
    "title": "Use the distributed file format Zarr on Jetstream 2 object storage",
    "section": "",
    "text": "Zarr is a file format designed for cloud computing, see documentation.\nZarr is also supported by dask, the parallel computing framework for Python, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2022-04-04-zarr_jetstream2.html#use-openstack-swift-on-jetstream-for-object-storage",
    "href": "posts/2022-04-04-zarr_jetstream2.html#use-openstack-swift-on-jetstream-for-object-storage",
    "title": "Use the distributed file format Zarr on Jetstream 2 object storage",
    "section": "Use OpenStack swift on Jetstream for object storage",
    "text": "Use OpenStack swift on Jetstream for object storage\nJetstream 2, like Jetstream 1, offers access to object storage via OpenStack Swift. This is a separate service from the Jetstream Virtual Machines, so you do not need to spin any Virtual Machine dedicated to storing the data but just use the object storage already provided by Jetstream. When you ask for an allocation, you can ask for volume storage and object store storage."
  },
  {
    "objectID": "posts/2022-04-04-zarr_jetstream2.html#read-zarr-files-from-object-store",
    "href": "posts/2022-04-04-zarr_jetstream2.html#read-zarr-files-from-object-store",
    "title": "Use the distributed file format Zarr on Jetstream 2 object storage",
    "section": "Read Zarr files from object store",
    "text": "Read Zarr files from object store\nIf somebody else has already made available some files on object store and set their visibility to “public”, anybody can read them.\nSee the example Notebook to read Zarr files\nOpenStack Swift already provides an endpoint which has an interface compatible with Amazon S3, therefore we can directly use the S3FileSystem provided by s3fs.\nThen we can build a S3Map object which zarr and dask.array can access.\nIn this example I am using the distributed scheduler on a single node, you can scale up your computation having workers distributed on multiple nodes, just make sure that all the workers have access to the zarr, dask, s3fs packages."
  },
  {
    "objectID": "posts/2022-04-04-zarr_jetstream2.html#write-zarr-files-or-read-private-files",
    "href": "posts/2022-04-04-zarr_jetstream2.html#write-zarr-files-or-read-private-files",
    "title": "Use the distributed file format Zarr on Jetstream 2 object storage",
    "section": "Write Zarr files or read private files",
    "text": "Write Zarr files or read private files\nIn this case we need authentication.\nFirst you need to ask to the XSEDE helpdesk API access to Jetstream, this also gives access to the Horizon interface, which has many advanced features that are not available in Atmosphere.\n\nCreate a bucket\nObject store systems are organized on buckets, which are like root folders of our filesystem. From the Horizon interface, we can choose Object Store -&gt; Containers (quite confusing way of referring to buckets in OpenStack). Here we can check content of existing buckets or create a new one.\nMake sure you create the bucket on the right project\n\n\nGet credentials\nOnce you have Jetstream 2 application credentials on your system, you can first test you can check the content of the bucket we created above:\nopenstack object list my_bucket\nNow create ec2 credentials with:\nopenstack ec2 credentials create\nThis is going to display AWS access key and AWS secret, we can save credentials in ~/.aws/credentials in the machine we want then use to write to object store.\n[default]\nregion=RegionOne\naws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\naws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nTest access\nWe can check if we can successfully login using s3fs, notice we do not use anon=True as we did before:\nimport s3fs\nfs = s3fs.S3FileSystem(client_kwargs=dict(endpoint_url=\"https://js2.jetstream-cloud.org:8001/\"))\nfs.ls(\"my_bucket\")\n\n\nGenerate a file and write it to object store\nSee a Notebook example of creating a random array in dask and saving it in Zarr format to Object Store."
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "",
    "text": "This tutorial is a followup to: Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0, so I’ll assume Kubernetes is already deployed with a default storageclass."
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html#clone-the-configuration-files-repository",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html#clone-the-configuration-files-repository",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "Clone the configuration files repository",
    "text": "Clone the configuration files repository\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\nThis is the main repository which contains configuration files for all the tutorials I write, I usually always work with this folder as the root folder."
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html#install-jupyterhub",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html#install-jupyterhub",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nInside the repository root, first run\nbash create_secrets.sh\nto create the secret strings needed by JupyterHub then edit its output secrets.yaml to make sure it is consistent, edit the hosts lines if needed. Update February 2024: if you are using Designate to have a url of the form:\nkubejetstream-1.$PROJ.projects.jetstream-cloud.org\nset this on the hosts line. If you have another domain provider, make sure you create an A record pointing to your instance and then set the subdomain name here.\nbash configure_helm_jupyterhub.sh\nkubectl create namespace jhub\nThe newest Kubespray version doesn’t install the CSI driver on the master node, so we cannot run the Hub pod on the master node, I have therefore removed the nodeSelector and tolerances I had on the configuration for Jetstream 1.\nIn any case, the Kubernetes ingress automatically handles network routing.\nFinally run helm to install JupyterHub:\nbash install_jhub.sh\nThis is installing zero-to-jupyterhub 2.0.0, you can check on the zero-to-jupyterhub release page if a newer version is available, generally transitioning to new releases is painless, they document any breaking changes very well.\nCheck pods running with:\nkubectl get pods -n jhub\nOnce the proxy is running, even if hub is still in preparation, you can check in browser, you should get “Service Unavailable” which is a good sign that the proxy is working.\nYou can finally connect with your browser to the domain you have configured and check if the Hub is working fine, after that, the pods running using:\nkubectl get pods -n jhub\nshoud be:\nNAME                              READY   STATUS    RESTARTS   AGE\ncontinuous-image-puller-xlkf6     1/1     Running   0          18m\nhub-554bf64f9b-kc2h9              1/1     Running   0          2m26s\njupyter-zonca                     1/1     Running   0          12s\nproxy-567d5d9f8d-jr4k9            1/1     Running   0          18m\nuser-scheduler-79c85f98dd-jpl9l   1/1     Running   0          18m\nuser-scheduler-79c85f98dd-sg78t   1/1     Running   0          18m"
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html#customize-jupyterhub",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html#customize-jupyterhub",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "Customize JupyterHub",
    "text": "Customize JupyterHub\nAfter JupyterHub is deployed and integrated with Cinder for persistent volumes, for any other customizations, first authentication, you are in good hands as the Zero-to-Jupyterhub documentation is great."
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html#setup-https-with-letsencrypt",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html#setup-https-with-letsencrypt",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "Setup HTTPS with letsencrypt",
    "text": "Setup HTTPS with letsencrypt\nKubespray has the option of deploying also cert-manager, but I had trouble deploying an issuer, it was easier to just deploy it afterwards following my previous tutorial, recently updated.\nNote the new step about binding the Cert Manager pods to the master node."
  },
  {
    "objectID": "posts/2022-03-31-jetstream2_jupyterhub.html#feedback",
    "href": "posts/2022-03-31-jetstream2_jupyterhub.html#feedback",
    "title": "Deploy JupyterHub on Jetstream 2 on top of Kubernetes",
    "section": "Feedback",
    "text": "Feedback\nFeedback on this is very welcome, please open an issue on the Github repository."
  },
  {
    "objectID": "posts/2022-03-22-jupyter-notebooks-version-control.html",
    "href": "posts/2022-03-22-jupyter-notebooks-version-control.html",
    "title": "Workflow for Jupyter Notebooks under version control",
    "section": "",
    "text": "I’ll present here my strategy for keeping Jupyter Notebooks under version control.\nTLDR:"
  },
  {
    "objectID": "posts/2022-03-22-jupyter-notebooks-version-control.html#requirements",
    "href": "posts/2022-03-22-jupyter-notebooks-version-control.html#requirements",
    "title": "Workflow for Jupyter Notebooks under version control",
    "section": "Requirements",
    "text": "Requirements\n\nGithub handles ipynb pretty well, so I prefer not to use jupytext\nI want small repositories, so only commit the input cells\nI want to save the executed notebooks, not in the repo, but where I can easily reference them if needed"
  },
  {
    "objectID": "posts/2022-03-22-jupyter-notebooks-version-control.html#workflow",
    "href": "posts/2022-03-22-jupyter-notebooks-version-control.html#workflow",
    "title": "Workflow for Jupyter Notebooks under version control",
    "section": "Workflow",
    "text": "Workflow\n\nNotebooks and git\nI work on Notebooks as I work with normal Python files, so I always run black on them to fix formatting, use git add -p to add snippet by snippet. I don’t mind the little extra escaping the Notebook introduces.\nI have the nbstripout filters activated (nbstripout --install) so that even if the notebook is partially executed, when I run git add -p I only get the patches on the input cells.\nMoreover, I configure nbstripout to remove metadata like the kernelspec or the Python version, which doesn’t do by default\n\n\nSnapshot executed Notebooks\nThe Jupyter Notebook inside the repository has only the inputs, but I would like to save executed Notebooks for tracking purposes without increasing the size of the repository.\nFirst I do a clean execution of the Notebook (with Restart & Run All), then I save the changes to the input cells to the repository. I don’t need to clear the outputs from the Notebook, nbstripout does it on th e fly before submitting changes to git.\nThen I post the executed Notebook with all outputs to a Gist from the command line with the Github CLI:\ngh gist create my_notebook.ipynb\nOptionally with --public to make it show on my Gist profile.\nThe gh tool returns the link to the Gist, that I can add to the commit message or post in a Pull Request or an Issue.\n\n\nAdd executed Notebooks to the documentation\nOnce I have the final version of a Notebook, I often use nbsphinx to add it to the documentation.\nSo I disable nbstripout with:\nnbstripout --uninstall\nThen I add just the last executed state of the Notebook to the repository, so that Sphinx/Readthedocs can compile it into the documentation, including all plots.\nSee an example of a Notebook compiled into HTML by Sphinx"
  },
  {
    "objectID": "posts/2022-03-22-jupyter-notebooks-version-control.html#automation-script",
    "href": "posts/2022-03-22-jupyter-notebooks-version-control.html#automation-script",
    "title": "Workflow for Jupyter Notebooks under version control",
    "section": "Automation script",
    "text": "Automation script\nI have created a bash script that automates the process:\n\ncall with snapshot_nb your_notebook.ipynb\ncreates a Gist with the Notebook\namends the last commit to add a link to it\nit also creates a new Markdown cell with the tag snapshotlog and the title “Execution log” and appends a link to the gist with date and time of execution (Thanks @cosmic_mar for the suggestion), see how the Execution log looks like\nit actually works with any text file, even multiple files, so it could be used for log outputs for example\n\nSee https://gist.github.com/2f2eba4f0288ca4079f7f83efa6b9048\nInstall it by symlinking the 2 scripts to an executable folder, for example ~/bin, also useful if you fork my Gist."
  },
  {
    "objectID": "posts/2022-03-22-jupyter-notebooks-version-control.html#feedback",
    "href": "posts/2022-03-22-jupyter-notebooks-version-control.html#feedback",
    "title": "Workflow for Jupyter Notebooks under version control",
    "section": "Feedback",
    "text": "Feedback\nSee this thread on Twitter"
  },
  {
    "objectID": "posts/2022-01-07-make.html",
    "href": "posts/2022-01-07-make.html",
    "title": "Make for repeated commands",
    "section": "",
    "text": "I often find myself repeat the same commands in a project, for example in Openstack/Kubernetes, but also when I need to interface with Docker (build/push image).\nInstead of relying on bash history or writing a few bash scripts, it is just easier to write a Makefile, even if you do not use the Make dependencies and you just use it as a collection of useful commands.\nHere is a template to start from, which supports make list to show available targets and shows how to pass arguments to a command:\n\nhttps://gist.github.com/zonca/fd1980c1aeac394bfc854fce24b9b0df"
  },
  {
    "objectID": "posts/2021-11-04-cka-ckad-kubernetes-certification.html",
    "href": "posts/2021-11-04-cka-ckad-kubernetes-certification.html",
    "title": "Kubernetes certifications CKA and CKAD",
    "section": "",
    "text": "I recently pursued 2 Kubernetes certifications by Linux Foundation:\nI have been deploying, managing and using Kubernetes on Jetstream for more than 4 years (my first tutorial back in 2017).\nHowever I never did any formal training so my knowledge was sparse. I was extremely useful to follow the 2 classes by Linux Foundation related to the certifications, they gave me a more systematic view of all parts of Kubernetes.\nI decided to follow both, but there is a lot of overlap, so better choose one of the 2, if you are more interested in using Kubernetes to deploy applications, do only CKAD and the related class, if you need to administer Kubernetes deployments, take only CKA.\nThe most important part of the training is the “test session” on Killer.sh, this is a simulation of the real certification exam and gives you a lot of experience in being fast in navigating the docs and using kubectl. The exam itself also teaches a lot, you are logging into Kubernetes clusters, solving issues and performing real-world tasks.\nThe certification exam is done via proctoring, but the process is quite painless. For the exam you really need to be fast and know how to create resources with kubectl create instead of writing YAML every time, go for YAML just for the more complicated resources. I got to the end of both exams with 15 minutes to spare on the total of 2 hours, that I used to debug the questions I couldn’t do in the first pass."
  },
  {
    "objectID": "posts/2021-11-04-cka-ckad-kubernetes-certification.html#suggestions-for-the-tests",
    "href": "posts/2021-11-04-cka-ckad-kubernetes-certification.html#suggestions-for-the-tests",
    "title": "Kubernetes certifications CKA and CKAD",
    "section": "Suggestions for the tests",
    "text": "Suggestions for the tests\nHave bookmarks ready, I found a set on the web and added a few of my own, see https://gist.github.com/zonca/b1f7ee0f884cae8011e86a41e6c525d5, you can also copy the links to the YAML files and do wget from the terminal.\n\nBash\nYou need to memorize these variables and aliases to type into .bashrc:\nexport do=\"--dry-run=client -o yaml\"\nexport now=\"--force --grace-period 0\"\nalias  kr=\"kubectl --replace $now -f\"\nexport VISUAL=vim\nexport EDITOR=vim\nThe variables about vim are needed to have vim keybindings in TMUX. kr is very useful because you can use it instead of k apply -f and you have it ready if you need to modify the YAML and replace the resource.\n$do is necessary to create the YAML for a resource with k create and then go from there. $now just to quickly delete resources.\n\n\nVim\nMinimal .vimrc for editing YAML files:\nset expandtab\nset shiftwidth=2\nset tabstop=2\n\n\nTMux\nYou already use TMux, so it is useful to have 2 windows, the first one for kubectl and the second for vim, it is important not to confuse them, you only go configure the environment in the first window, so you can run kubectl only there.\nIn case you use the screen mode with CTRL-A as I do, no need to memorize the spell, just open man tmux, search for default prefix and copy paste into .tmux.conf:\nset-option -g prefix C-a\nunbind-key C-b\nbind-key C-a send-prefix"
  },
  {
    "objectID": "posts/2021-10-06-healpix-plot-pixel-boundaries.html",
    "href": "posts/2021-10-06-healpix-plot-pixel-boundaries.html",
    "title": "Plot HEALPix pixel boundaries",
    "section": "",
    "text": "Some time ago I created a Jupyter Notebook based on plotly to create a 3D visualization of the boundaries of HEALPix pixels on the sphere.\nThe notebook is available on Gist at https://gist.github.com/zonca/b3045651cbc90fe699d2e56df490b005.\nSee below the outputs ranging from Nside 1 to 8:\n\n\n\nNside 1\n\n\n\n\n\nNside 2\n\n\n\n\n\nNside 4\n\n\n\n\n\nNside 8"
  },
  {
    "objectID": "posts/2021-09-24-globus-groups-python-sdk.html",
    "href": "posts/2021-09-24-globus-groups-python-sdk.html",
    "title": "Manage Globus groups with the Python SDK",
    "section": "",
    "text": "Tested in March 2023, still working fine\nGlobus is the best tool to transfer quickly Terabytes data between Supercomputers because it automatically parallelizes the transfer to saturate the network (yeah I know I always simplify too much).\nIf you want to share a folder with collaborators, you can create a Globus endpoint and give them access, see how to do that at NERSC.\nNow, it’s handy to create a group and share the endpoint directly with the group instead of individual users.\nThe web interface of Globus allows you to create groups, but you have to add people one at a time, and this is where the Globus Python SDK comes handy.\nInstall globus_sdk with pip and follow the tutorial to have it configured.\nClone my repository of scripts, assuming you use gh:\nCreate a file globus_config.toml with your client ID and the name of the group you already created with the web interface:"
  },
  {
    "objectID": "posts/2021-09-24-globus-groups-python-sdk.html#create-the-tokens",
    "href": "posts/2021-09-24-globus-groups-python-sdk.html#create-the-tokens",
    "title": "Manage Globus groups with the Python SDK",
    "section": "Create the tokens",
    "text": "Create the tokens\nRun the authentication script:\npython get_access_tokens.py\nThis will ask to open a link and paste back a string.\nThis will save the authentication tokens in 3 TOML files, those are sensitive, DO NOT COMMIT to public repositories."
  },
  {
    "objectID": "posts/2021-09-24-globus-groups-python-sdk.html#batch-add-members-to-group",
    "href": "posts/2021-09-24-globus-groups-python-sdk.html#batch-add-members-to-group",
    "title": "Manage Globus groups with the Python SDK",
    "section": "Batch add members to group",
    "text": "Batch add members to group\nI assume you have a users.csv file with a column named “Email Address”.\npython add_users_to_group.py users.csv\nis going to read that CSV, then grab 50 emails at a time, contact the Globus API to get the members ID if they have one, otherwise they are just skipped. Then it batch-adds them to the group."
  },
  {
    "objectID": "posts/2021-09-24-globus-groups-python-sdk.html#contribute",
    "href": "posts/2021-09-24-globus-groups-python-sdk.html#contribute",
    "title": "Manage Globus groups with the Python SDK",
    "section": "Contribute",
    "text": "Contribute\nPlease leave feedback to the https://github.com/zonca/globus-sdk-scripts repository via issues or contribute improvements via Pull Requests."
  },
  {
    "objectID": "posts/2021-08-01-joss-to-arxiv.html",
    "href": "posts/2021-08-01-joss-to-arxiv.html",
    "title": "Upload a JOSS paper to Arxiv",
    "section": "",
    "text": "If you are in Astrophysics, you probably want to have your JOSS paper published to the Arxiv.\nUnfortunately some authors don’t get it accepted due to the paper being too short for the Arxiv standard.\nAnyway, here I am explaining how to make the upload using Github Actions and Overleaf, so we do not even need a machine with Latex. I started from the suggestions discussed in this issue, where you also find other methods."
  },
  {
    "objectID": "posts/2021-08-01-joss-to-arxiv.html#instructions",
    "href": "posts/2021-08-01-joss-to-arxiv.html#instructions",
    "title": "Upload a JOSS paper to Arxiv",
    "section": "Instructions",
    "text": "Instructions\n\nmodified Github Action and saved all artifacts, see the diff of my modification\ndownloaded JOSS logo from https://github.com/openjournals/whedon/blob/master/resources/joss/logo.png\nedited the tex file to point to local logo.png\nuploaded paper.tex paper.bib logo.png to Overleaf\ndownloaded output.bbl from Overleaf, renamed to paper.bbl\nuploaded paper.tex, logo.png and paper.bbl to arxiv"
  },
  {
    "objectID": "posts/2021-06-22-healpy-1.15.0-release.html",
    "href": "posts/2021-06-22-healpy-1.15.0-release.html",
    "title": "healpy 1.15.0 released",
    "section": "",
    "text": "Just released healpy 1.15.0, this is a large release with many important changes, please test and open issues at https://github.com/healpy/healpy/issues\n\nNew viz function projview based on plain matplotlib, with lon/lat labels and more https://healpy.readthedocs.io/en/latest/newvisufunc_example.html\nFinally new default for I/O: write_map keeps dtype of input map array instead of float32, read_map keeps dtype of FITS file instead of upcasting to float64, write_cl uses dtype of input cl instead of float64, see discussion in https://github.com/healpy/healpy/pull/688\nChanged all warnings to using the logging module, deprecated all verbose keywords , see the docs https://healpy.readthedocs.io/en/latest/#verbosity\nDropped support for Python 2 and 3.5, use 1.14.0 if you need it\nBinary packages on PyPI and conda-forge for the Linux and MacOS\nAll other changes at https://github.com/healpy/healpy/blob/main/CHANGELOG.rst\nAs usual, remember to cite our JOSS paper https://joss.theoj.org/papers/10.21105/joss.01298"
  },
  {
    "objectID": "posts/2021-05-20-google-docs-to-overleaf-github.html",
    "href": "posts/2021-05-20-google-docs-to-overleaf-github.html",
    "title": "Migrate from Google Docs to Overleaf and Github",
    "section": "",
    "text": "Once a document on Google Docs becomes too big and complicated to make sure it is consistent, it is a good idea to migrate it to Latex. In particular we can support both Google Docs style editing via Overleaf and Pull Request reviewing via Github."
  },
  {
    "objectID": "posts/2021-05-20-google-docs-to-overleaf-github.html#convert-google-docs-document-to-latex",
    "href": "posts/2021-05-20-google-docs-to-overleaf-github.html#convert-google-docs-document-to-latex",
    "title": "Migrate from Google Docs to Overleaf and Github",
    "section": "Convert Google Docs document to Latex",
    "text": "Convert Google Docs document to Latex\nI was impressed about how well docx2latex.com worked, it correctly imported:\n\nsectioning based on title styles in Google Docs\nall images\ntables!\npage header with image!\n\nJust download the docx from Google Docs and upload to docx2latex, if needs to be &lt; 10 MB, so remove large images before exporting from Google Docs if it is bigger.\nThen you will be able to download a zip with the Latex source and images."
  },
  {
    "objectID": "posts/2021-05-20-google-docs-to-overleaf-github.html#import-to-github",
    "href": "posts/2021-05-20-google-docs-to-overleaf-github.html#import-to-github",
    "title": "Migrate from Google Docs to Overleaf and Github",
    "section": "Import to Github",
    "text": "Import to Github\nIf you don’t need Github, you can directly upload the zip archive to Overleaf and be done.\nHowever, it is nice to have a backup on Github, and support users that prefer editing latex outside of their browser…\nSo create a repository, even private and upload the content of the archive after having removed the PDF artifacts."
  },
  {
    "objectID": "posts/2021-05-20-google-docs-to-overleaf-github.html#integration-with-overleaf",
    "href": "posts/2021-05-20-google-docs-to-overleaf-github.html#integration-with-overleaf",
    "title": "Migrate from Google Docs to Overleaf and Github",
    "section": "Integration with Overleaf",
    "text": "Integration with Overleaf\nMain issue the Github integration with Overleaf is they require write access to all your repositories (!! crazy, I know).\nSo let’s create a new Github account, I called mine zoncaoverleafbot and invite the new user to be a Collaborator to the repository.\nThen login to Overleaf with your real account, click on “Import from Github”, and when you are redirected to Github to link your account, link the overleafbot account instead, which only has access to the Latex repositories.\nNow on Overleaf click on “New project” and “Import from Github”.\nIf your repository doesn’t show in the list, it is probably because it belongs to an organization, in that case see the Github help to fix it."
  },
  {
    "objectID": "posts/2021-05-20-google-docs-to-overleaf-github.html#synchronization",
    "href": "posts/2021-05-20-google-docs-to-overleaf-github.html#synchronization",
    "title": "Migrate from Google Docs to Overleaf and Github",
    "section": "Synchronization",
    "text": "Synchronization\nSynchronization of Overleaf and Github is always done on Overleaf.\nOnce in a while, you can manually synchronize from/to Github using the Github button. If you share the project by sending someone the “Edit” link, they can also use the Github integration that the project owner has configured, so no need for them to link their Github account. Best would be for each Overleaf user, each time they login, to pull changes from Github before starting to work and push their changes with a commit message when they are done."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html",
    "href": "posts/2021-03-28-sgci-tech-blog.html",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "",
    "text": "This is cross-posted from the Science Gateways Community institute tech blog, thanks to Marlon Pierce and Katherine Lawrence for their feeback."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#introduction",
    "href": "posts/2021-03-28-sgci-tech-blog.html#introduction",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "Introduction",
    "text": "Introduction\nJupyterHub handles authentication and routing for any number of users that have access to an interactive computational environment, the Jupyter Notebook, via their browser.\nThe most high-impact application of JupyterHub in the context of Science Gateways is to deploy it as a companion application for the Science Gateway to enable users to access a pre-configured computing environment. This configuration allows them to use the Gateway programmatically and apply custom processing of the gateway input or outputs. For example, the Gateway users could use the same login credentials to also access a Jupyter Notebook environment with tutorial notebooks displaying example analysis pipelines that show how they can:\n\nupload raw data\nuse packages installed in the Jupyter Notebook environment to pre-process raw data to transform them in the format suitable for the Science Gateway\nsubmit the data for processing using remote execution APIs\ncheck the job processing status\nretrieve the results\nload the output data from disk\npost-process and plot the results\n\nLet’s proceed now to show an overview on how to deploy first Kubernetes and then JupyterHub on Jetstream."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#jetstream",
    "href": "posts/2021-03-28-sgci-tech-blog.html#jetstream",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "Jetstream",
    "text": "Jetstream\nJetstream (and the upcoming Jetstream 2) is a cloud deployment part of XSEDE, the science community’s equivalent of Amazon Elastic Compute Cloud (EC2) or Google Cloud Platform. Many Science Gateways already run on Jetstream.\nJetstream allows each user to programmatically launch Virtual Machines with the desired amount of CPU/RAM resources, a pre-configured OS (Ubuntu, CentOS…), connect them together on an internal network and expose them to the Internet with public IPs. Users have then full administrative access to the machines to install any software package.\nJetstream has extensive documentation and training available both focused on using the simplified Atmosphere interface and the programmatic access via web API. Once developers decide to try Jetstream, they can request a trial or a startup allocation via XSEDE.\nDeployment on Jetstream should begin with gateway developers identifying the scale of their deployment: few-user or test deployment vs. large-memory, many-user deployment.\n\nArchitecture of the deployment of JupyterHub on Jetstream: the users connect via their browser to the Jetstream master node virtual machine, they first are redirected to external services for authentication. Once authenticated, JupyterHub spawns a container running the Jupyter Notebook for them in one of the worker nodes managed by Kubernetes. Their own persistent data volume is also mounted there. Kubernetes handles the networking so that the interactive computing session is proxied back to the users’ browser."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#single-server-deployment",
    "href": "posts/2021-03-28-sgci-tech-blog.html#single-server-deployment",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "Single server deployment",
    "text": "Single server deployment\nIn the case of a test deployment with a small number of users, each with limited computational resources, we can avoid the complications of a distributed deployment. The most important requirement is memory, a reasonable estimate is to have available 128 MB of RAM for JupyterHub services and then multiply the number of concurrent users by the memory we want to allocate for each user. For example, if we give each user 10 GB of RAM, we could host 11 concurrent users on a Jetstream XXL instance with 128 GB of RAM and have significant margin for system services. \nIn this case, we can just spawn a single, large-memory Virtual Machine on Jetstream and then follow the “Littlest JupyterHub” documentation to install JupyterHub."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#kubernetes",
    "href": "posts/2021-03-28-sgci-tech-blog.html#kubernetes",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "Kubernetes",
    "text": "Kubernetes\nHowever, if the sum of the memory needed by all the users is higher than 120GB, we need to deploy JupyterHub across multiple Jetstream Virtual Machines. The recommended way of deploying a distributed instance of JupyterHub is on top of Kubernetes, which is able to provide networking, logging, reliability, and scalability.\nI have configured and adapted the Kubespray tool to deploy Kubernetes to Jetstream and have written a step-by-step tutorial about it, which includes all the necessary configuration files. A Jetstream user can launch this script to programmatically launch any number of Jetstream instances and deploy Kubernetes across them. Consider also that Kubernetes would be a good platform to deploy a Science Gateway itself, especially if it already has a distributed architecture (for example, the gateway website, a database, and worker processes that execute computations). However, Kubernetes itself is a complex system, which requires someone in the Gateway development team to invest significant effort in learning how to configure and administer it.\nAfter Kubernetes is deployed, we now have a programmatic interface where we can deploy applications packaged as Docker containers, monitor their execution, log their warnings or errors, and scale them based on load."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#jupyterhub",
    "href": "posts/2021-03-28-sgci-tech-blog.html#jupyterhub",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "JupyterHub",
    "text": "JupyterHub\nFinally, we can leverage the Zero to JupyterHub project and customize their recipe to automatically deploy all the components needed for a distributed JupyterHub deployment on Kubernetes.\nWe can configure authentication with Github, Google, XSEDE, or CILogon, choose a Docker container with all the packages needed by the users, decide how much RAM, CPU, and disk each user has access to, configure a public URL, and setup HTTPS.\nOnce the deployment is complete, the users can point their browser to the master node of the deployment, authenticate and have a Jupyter Notebook be spawned for them across the cluster of Jetstream instances with a dedicated amount of computing resources.\nWe can also pre-populate the computational environment with tutorial notebooks that display example data analysis pipelines that jointly leverage JupyterHub and the Science Gateway. A few hours after a user disconnects, their container is decommissioned to free up resources for other users, but their data is saved as a permanent disk on Jetstream and is mounted again the next time they reconnect."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#conclusion",
    "href": "posts/2021-03-28-sgci-tech-blog.html#conclusion",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "Conclusion",
    "text": "Conclusion\nThis is a basic overview of the different components involved in deploying a distributed instance of JupyterHub on Jetstream. To know more, check out the list of references below. If you have any feedback or would like to collaborate, please open an issue in my jupyterhub-deploy-kubernetes-jetstream repository."
  },
  {
    "objectID": "posts/2021-03-28-sgci-tech-blog.html#references",
    "href": "posts/2021-03-28-sgci-tech-blog.html#references",
    "title": "Science Gateways Tech blog - Kubernetes and JupyterHub on Jetstream",
    "section": "References",
    "text": "References\n\n“Gateways 2020” paper and video recording\nVideo tutorial on Openstack and Kubernetes for the ECSS Symposium\nTutorial on how to deploy Kubernetes and JupyterHub on Jetstream via Kubespray\nTutorial on how to setup SSL for HTTPS connection to JupyterHub\nTutorial on how to deploy Dask for distributed computing\nPrototype autoscaler for JupyterHub on Jetstream"
  },
  {
    "objectID": "posts/2021-01-28-github-overleaf-large-document.html",
    "href": "posts/2021-01-28-github-overleaf-large-document.html",
    "title": "Coordinate a large Latex document with multiple Overleaf projects and Github",
    "section": "",
    "text": "In case we need to build a large document (~hundreds of pages) and we have a large number of collaborators (more than 50), it is convenient to have each section of the document be a different Overleaf project. Also, we don’t want to rely on Overleaf only, we prefer backup of everything to Github.\nThen have a Github repository that includes all of those as git submodules and can build the whole document together automatically using Github Actions and provides the PDF. We cannot use Overleaf here because it doesn’t support submodules.\nThe setup is based on the Overleaf tutorial on multi-page Latex using the “Standalone package” setup, and then splitting it into separate repositories.\nSee this Overleaf project for the project configuration before being split into separate projects."
  },
  {
    "objectID": "posts/2021-01-28-github-overleaf-large-document.html#configuration-of-the-github-repository",
    "href": "posts/2021-01-28-github-overleaf-large-document.html#configuration-of-the-github-repository",
    "title": "Coordinate a large Latex document with multiple Overleaf projects and Github",
    "section": "Configuration of the Github repository",
    "text": "Configuration of the Github repository\nHere you can use your official Github account.\nCreate a repository on Github for the main document and for each of the subsections\nSee for example:\n\nhttps://github.com/zonca/overleaf_largedoc_main\nhttps://github.com/zonca/overleaf_largedoc_intro\nhttps://github.com/zonca/overleaf_largedoc_section_1\n\nEach repository has a main.tex file in the root folder which contains the content of that section.\nIn the main repository, we configure git submodules for each section, e.g.:\ngit submodule add git@github.com:zonca/overleaf_largedoc_intro.git 0_introduction\nNow you should double-check you can compile locally the document with pdflatex."
  },
  {
    "objectID": "posts/2021-01-28-github-overleaf-large-document.html#automation-with-github-actions",
    "href": "posts/2021-01-28-github-overleaf-large-document.html#automation-with-github-actions",
    "title": "Coordinate a large Latex document with multiple Overleaf projects and Github",
    "section": "Automation with Github Actions",
    "text": "Automation with Github Actions\nI have configured Github Actions to run at every commit just in the main repository, see the details:\n\nAt each commit, Github runs latexmk to build the full PDF, then attaches it to that run, they can be downloaded from the Github Actions tab, see the artifacts tab at the end of this run.\nWhenever you create a tag in the repository, for example 1.0 or 2021.01.28, Github creates a release with the PDF attached (named after the release), see an example\nIn the main repository, I prepared a script to update all the sections to their latest commit, see update_sections.sh. This can be also triggered via web by manually running the “Update all sections” workflow (click on the “Run workflow” button and choose the master branch). This creates a new commit therefore triggers creation of the PDF. It is also configured to run every morning at 7am PT.\n\nThis should work also for private repositories, both Free and Team (academic) organization on Github have 2000+ Github action minutes a month. You will need to switch plan if you have a Legacy account which doesn’t offer Github Actions on private repositories."
  },
  {
    "objectID": "posts/2021-01-28-github-overleaf-large-document.html#integration-with-overleaf",
    "href": "posts/2021-01-28-github-overleaf-large-document.html#integration-with-overleaf",
    "title": "Coordinate a large Latex document with multiple Overleaf projects and Github",
    "section": "Integration with Overleaf",
    "text": "Integration with Overleaf\nMain issue the Github integration with Overleaf is they require write access to all your repositories (!! crazy, I know).\nSo let’s create a new Github account, I called mine zoncaoverleafbot and invite the new user to be a Collaborator to each of the sections repositories.\nThen login to Overleaf with your real account, click on “Import from Github”, and when you are redirected to Github to link your account, link the overleafbot account instead, which only has access to the sections repositories.\nNow create a Overleaf project for each document section, Overleaf can build independent PDF for each of the subsection.\nOnce in a while, you can manually synchronize from/to Github using the Github button. If you share the project by sending someone the “Edit” link, they can also use the Github integration that the project owner has configured, so no need for them to link their Github account. Best would be for each Overleaf user, each time they login, to pull changes from Github before starting to work and push their changes with a commit message when they are done.\nMoreover, Overleaf also has the standard Git interface, so if there is any complex merging issue, an expert user can rely on that to resolve and then switch back to the automatic interface."
  },
  {
    "objectID": "posts/2021-01-28-github-overleaf-large-document.html#conclusion",
    "href": "posts/2021-01-28-github-overleaf-large-document.html#conclusion",
    "title": "Coordinate a large Latex document with multiple Overleaf projects and Github",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a prototype implementation, I’d be interested in ideas for improvements, please open an issue on Github"
  },
  {
    "objectID": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html",
    "href": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html",
    "title": "Autoscaling script for JupyterHub on top of Kubespray",
    "section": "",
    "text": "I think the most reliable way of deploying Kubernetes on Openstack is Kubespray, see my latest tutorial about it focused on Jetstream.\nThe main issue with this strategy, compared to using Magnum, is that it is not supported by the Cluster Autoscaler, so we don’t have an automatic way of scaling up and down the deployment based on load.\nFor testing purposes I developed a small script that implements this, just to set the expectations, I called it hacktoscaler.\nIt is a bash script that runs on a server (ideally will deploy inside Kubernetes but for now is external), every minute:"
  },
  {
    "objectID": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html#requirements",
    "href": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html#requirements",
    "title": "Autoscaling script for JupyterHub on top of Kubespray",
    "section": "Requirements",
    "text": "Requirements\n\nAll that is needed to run Kubespray, see my latest tutorial linked above\nSymlink the kubespray folder inside the hacktoscaler folder"
  },
  {
    "objectID": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html#how-to-run",
    "href": "posts/2021-01-20-autoscaling_script_kubespray_jupyterhub.html#how-to-run",
    "title": "Autoscaling script for JupyterHub on top of Kubespray",
    "section": "How to run",
    "text": "How to run\nYou should have already checked out my jupyterhub-deploy-kubernetes-jetstream repository, cd into the hacktoscaler folder.\nCheck its configuration by editing hacktoscaler_daemon.sh and then launch it:\nbash hacktoscaler_daemon.sh\nThen you can simulate some users to trigger the scaling (this requires hubtraf):\nbash 0_simulate_users.sh\nAfter this you should be able to see the logs of what the hacktoscaler is doing.\nIf you weren’t already warned by the name of this script, USE AT YOUR OWN RISK, and good luck."
  },
  {
    "objectID": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html",
    "href": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html",
    "title": "Paper review workflow with Overleaf, git and Google Docs",
    "section": "",
    "text": "In this blog post I propose a workflow to use the collaborative features of Google Docs to review a paper being written in Overleaf (unless you have Overleaf Pro which has change tracking embedded). The key point is that we paste the Latex source to a Google Document (see an example here) so we can suggest changes and then push it back to Overleaf programmatically using wget and git."
  },
  {
    "objectID": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html#workflow",
    "href": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html#workflow",
    "title": "Paper review workflow with Overleaf, git and Google Docs",
    "section": "Workflow",
    "text": "Workflow\nCreate first the paper in Overleaf\nNext we can configure git access, see more details on Overleaf:\nLinking Github requires too much permissions, so better just activate plain Git access, this will provide a link to clone it:\ngit clone https://git.overleaf.com/xxxxxxxxxxxxxxxxxxxxxxxx overleaf-paper\nHere we can make local changes and push them back to Overleaf (you can configure the git credential helper to store the username and password).\nNow the main authors can develop the paper using a mix of local editing and git push or online editing in Overleaf, optionally they could also push to Github."
  },
  {
    "objectID": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html#review-round-on-google-docs",
    "href": "posts/2020-11-18-paper-review-overleaf-git-googledocs.html#review-round-on-google-docs",
    "title": "Paper review workflow with Overleaf, git and Google Docs",
    "section": "Review round on Google Docs",
    "text": "Review round on Google Docs\nWhen the paper is ready for a round of review with co-authors or other reviewers, the main authors can circulate the PDF and copy-paste the Latex source into a Google Doc. It can also be useful to paste the images, but not strictly necessary, the reviewers can look into the PDF.\nNow the reviewers can read the PDF source and make suggested edits and comments on Google Doc, “Review changes” is also available on Overleaf, but just for paid accounts.\n\nSynchronization from Google Docs to Overleaf\nAs the review progresses, some changes are merged using the Google Docs review functionality, they can be programmatically be merged into Overleaf going through git.\nCreate a download_google_doc.sh script (Linux, but could work on Mac as well):\nKEY=1rOksxkZUOdNW1alA6lqXg1xre4Q7bV6oyCVoIGGh0Z8\nwget -O - https://docs.google.com/document/d/$KEY/export?format=txt | dos2unix | cat -s &gt; main.tex\nit downloads a text version of the Google Doc document, fixes line endings and spurious repeated blank lines and overwrites main.tex. KEY is the key to my document for testing purposes, replace it with yours, it is the long hash in the Google Document URL i.e. https://docs.google.com/document/d/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/edit, the document needs to be public.\nThis is executed with bash download_google_doc.sh and then reviewed and merged with the standard git workflow. Finally pushed back to Overleaf with:\ngit push\nThis is useful for example while the review progresses, we can create updated PDF versions.\n\n\nSynchronization from Overleaf/git to Google Docs\nThis is not doable, because writing into Google Docs erases the comments. Therefore it would be better to organize review into rounds, 1 Google doc per round. So we never need to update Google Doc with changes made on Overleaf.\nHowever, while the review is progressing in Google Doc, the main authors could keep working in Overleaf/git and then merge the changes from Google Doc as they come in relying on git. The issue is that it is not easy to propagate their changes to the current round of review on Google Doc."
  },
  {
    "objectID": "posts/2020-09-25-ffmpeg-edit-recordings.html",
    "href": "posts/2020-09-25-ffmpeg-edit-recordings.html",
    "title": "Edit video recordings with ffmpeg",
    "section": "",
    "text": "A few scripts to speedup, concatenate and possibly other operations using ffmpeg on linux (also works on Chromebooks) from the command line:\n\nI used those to pre-record videos for a conference (after recording with Loom)"
  },
  {
    "objectID": "posts/2020-09-04-gateways-2020-paper.html",
    "href": "posts/2020-09-04-gateways-2020-paper.html",
    "title": "Gateways 2020 paper about Kubernetes and JupyterHub on Jetstream",
    "section": "",
    "text": "My paper in collaboration with Richard Signell (USGS), Julien Chastang (UCAR), John Michael Lowe (IU/Jetstream), Jeremy Fischer (IU/Jetstream) and Robert Sinkovits (SDSC) was accepted at Gateways 2020 (October 12–23, 2020).\nIt gives an overview of the architecture of the deployments of the container orchestration engine Kubernetes on the NSF-funded Jetstream Openstack cloud deployment at Indiana University. It refers to my previously published tutorials for step-by-step instructions and configuration files, the 2 most important tutorials explain the 2 strategies for deploying Kubernetes on Jetstream:\n\nUsing Magnum, the Openstack functionality to provide a ready-made Kubernetes cluster\nUsing Terraform and Ansible via kubespray\n\nOnce Kubernetes is available, it is quite easy to deploy JupyterHub configuring properly zero-to-jupyterhub\nSee the Gateways 2020 paper (open-access) on OSF, (direct link to PDF)\nHere is the recording of the presentation and the questions:\n\n\nor here just the video recording (better quality, no questions/answers)\nOn the same topic I also gave a 1-hour webinar focused on introducing how to use Openstack and Kubernetes for people with no previous experience, the video (April 2020 ECSS Symposium) is available on Youtube\nIf you have any question/feedback, reply to this tweet:\n\n\nTomorrow (Thursday 22nd), I'll be presenting a paper about:\"Deployment of #Kubernetes and @ProjectJupyter Hub on @XSEDE @jetstream_cloud\"at #gateways2020 by @sciencegateways, 11:45am PDT, see https://t.co/myaNoUe3s0 pic.twitter.com/t06kzt1CEe\n\n— Andrea Zonca (@andreazonca) October 22, 2020"
  },
  {
    "objectID": "posts/2020-09-01-migrate-travisci-readthedocs-github-actions.html",
    "href": "posts/2020-09-01-migrate-travisci-readthedocs-github-actions.html",
    "title": "Migrate from Travis-CI and Readthedocs to Github actions",
    "section": "",
    "text": "For a number of years I have been concerned about the duplication of work having to maintain continuous integration environments both on Travis-CI to run the unit tests and on Readthedocs to build and host the documentation.\nThe solution I am proposing is to use Github actions to replace both systems. I chose Github actions instead of Travis-CI because they are easier to configure, have better log-viewing interface and surprisingly enough they are better integrated with Github!\nThe biggest hurdle was to find a way to reproduce the capability of readthedocs to host multiple versions of the documentation together, fortunately the sphinx-multiversion gives a very similar functionality.\nNext I’ll be sharing an example configuration for Github actions that integrates:"
  },
  {
    "objectID": "posts/2020-09-01-migrate-travisci-readthedocs-github-actions.html#requirements",
    "href": "posts/2020-09-01-migrate-travisci-readthedocs-github-actions.html#requirements",
    "title": "Migrate from Travis-CI and Readthedocs to Github actions",
    "section": "Requirements",
    "text": "Requirements\n\nCreate a docs/requirements.txt file which includes sphinx-multiversion, I directly use https://github.com/Holzhaus/sphinx-multiversion/archive/master.zip\nFollow the sphinx-multiversion documentation to configure docs/conf.py, for example my configuration is:\n  ```\nSphinx multiversion configuration\nextensions += [“sphinx_multiversion”]\n\ntemplates_path = [ “_templates”, ] #html_sidebars = {’**’: [ # “versioning.html”, # ]}"
  },
  {
    "objectID": "posts/2020-08-11-dask-gateway-jupyterhub.html",
    "href": "posts/2020-08-11-dask-gateway-jupyterhub.html",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "",
    "text": "This tutorial is obsolete, please follow https://zonca.dev/2022/01/dask-gateway-jupyterhub.html\nThis tutorial follows the work by the Pangeo collaboration, the main difference is that I prefer to keep JupyterHub and the Dask infrastructure in 2 separate Helm recipes.\nI assume to start from a Kubernetes cluster already running and JupyterHub deployed on top of it via Helm. And SSL encryption also activated (it isn’t probably necessary, but I haven’t tested that). I tested on Jetstream, but this is agnostic of that."
  },
  {
    "objectID": "posts/2020-08-11-dask-gateway-jupyterhub.html#preparation",
    "href": "posts/2020-08-11-dask-gateway-jupyterhub.html#preparation",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Preparation",
    "text": "Preparation\nClone on the machine you use to run helm and kubectl the repository with the configuration files and scripts:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/\nThen you need to setup one API token, create it with:\nopenssl rand -hex 32\nThen paste it both in dask_gateway/config_jupyterhub.yaml and dask_gateway/config_dask-gateway.yaml, look for the string TOKEN and replace it."
  },
  {
    "objectID": "posts/2020-08-11-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "href": "posts/2020-08-11-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Launch dask gateway",
    "text": "Launch dask gateway\nSee the dask gateway documentation for reference:\n$ helm repo add daskgateway https://dask.org/dask-gateway-helm-repo/\n$ helm repo update\nenter the dask_gateway folder and run:\n$ bash install_dask-gateway.sh\nYou might want to check config_dask-gateway.yaml for extra configuration options, but for initial setup and testing it shouldn’t be necessary.\nAfter this you should see the 3 dask gateway pods running, e.g.:\n$ kubectl -n jhub get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\napi-dask-gateway-64bf5db96c-4xfd6          1/1     Running   2          23m\ncontroller-dask-gateway-7674bd545d-cwfnx   1/1     Running   0          23m\ntraefik-dask-gateway-5bbd68c5fd-5drm8      1/1     Running   0          23m"
  },
  {
    "objectID": "posts/2020-08-11-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "href": "posts/2020-08-11-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Modify the JupyterHub configuration",
    "text": "Modify the JupyterHub configuration\nOnly 2 options need to be changed in JupyterHub:\n\nWe need to run a image which has the same version of dask-gateway we installed on Kubernetes (currently 0.8.0)\nWe need to proxy dask-gateway through JupyterHub so the users can access the Dask dashboard\n\nIf you are using my install_jhub.sh script to deploy JupyterHub, you can modify it and add another values option at the end, --values dask_gateway/config_jupyterhub.yaml.\nYou can modify the image you are using for Jupyterhub in dask_gateway/config_jupyterhub.yaml.\nTo assure that there are not compatibility issues, the “Client” (JupyterHub session), the dask gateway server, the scheduler and the workers should all have the same version of Python and the same version of dask, distributed and dask_gateway. If this is not possible, you can test different combinations and they might work. For example I tested a “Client” on Python 3.6 and everything else with Python 3.7 and seems to be working fine.\nThen redeploy JupyterHub:\nbash install_jhub.sh\nCheck that the service is working correctly, if open a browser tab and access https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/api/health, you should see:\n{\"status\": \"pass\"}\nIf this is not working, you can open login to JupyterHub, get a terminal and first check if the service is working:\n&gt;  curl http://traefik-dask-gateway/services/dask-gateway/api/health\nShould give:\n{\"status\": \"pass\"}"
  },
  {
    "objectID": "posts/2020-08-11-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "href": "posts/2020-08-11-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Create a dask cluster",
    "text": "Create a dask cluster\nYou can now login to JupyterHub and check you can connect properly to dask-gateway:\nfrom dask_gateway import Gateway\ngateway = Gateway(\n    address=\"http://traefik-dask-gateway/services/dask-gateway/\",\n    public_address=\"https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/\",\n    auth=\"jupyterhub\")\ngateway.list_clusters()\nThen create a cluster and use it:\ncluster = gateway.new_cluster(public_address = gateway._public_address)\ncluster.scale(2)\nclient = cluster.get_client()\nClient is a standard distributed client and all subsequent calls to dask will go through the cluster.\nFor a full example and screenshots of the widgets and of the dashboard see:\nhttps://gist.github.com/zonca/355a7ec6b5bd3f84b1413a8c29fbc877\n(Click on the Raw button to download notebook and upload it to your session)."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "",
    "text": "This tutorial is obsolete, check the updated version of the tutorial\nThis is an update to previous tutorials, focused on deploying Kubernetes 1.17.6 (released in May 2020, based on 1.17.0 released in December 2019).\nWe will use Kubespray 2.13.1, which first runs terraform to create the Openstack resources, then ansible to configure the servers to run all the Kubernetes services."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Create Jetstream Virtual machines with Terraform",
    "text": "Create Jetstream Virtual machines with Terraform\nTerraform allows to execute recipes that describe a set of OpenStack resources and their relationship. In the context of this tutorial, we do not need to learn much about Terraform, we will configure and execute the recipe provided by kubespray.\n\nRequirements\nOn a Ubuntu 18.04 install python3-openstackclient with APT. Any other platform works as well, also install terraform by copying the correct binary to /usr/local/bin/, see https://www.terraform.io/intro/getting-started/install.html. Install the newest 0.12.x release, I tested with 0.12.26.\n\n\nRequest API access\nIn order to make sure your XSEDE account can access the Jetstream API, you need to contact the Helpdesk, see the instructions on the Jetstream Wiki. You will also receive your TACC password, which could be different than your XSEDE one (username is generally the same).\nLogin to the TACC Horizon panel at https://tacc.jetstream-cloud.org/dashboard, this is basically the low level web interface to OpenStack, a lot more complex and powerful than Atmosphere available at https://use.jetstream-cloud.org/application. Use tacc as domain, your TACC username (generally the same as your XSEDE username) and your TACC password.\nFirst choose the right project you would like to charge to in the top dropdown menu (see the XSEDE website if you don’t recognize the grant code).\nClick on Compute / API Access and download the OpenRC V3 authentication file to your machine. Source it typing:\nsource XX-XXXXXXXX-openrc.sh\nit should ask for your TACC password. This configures all the environment variables needed by the openstack command line tool to interface with the Openstack API.\nTest with:\nopenstack flavor list\nThis should return the list of available “sizes” of the Virtual Machines.\n\n\nClone kubespray\nI needed to make a few modifications to kubespray to adapt it to Jetstream:\ngit clone https://github.com/zonca/jetstream_kubespray\ngit checkout -b branch_v2.13.1 origin/branch_v2.13.1\nSee an overview of my changes compared to the standard kubespray release 2.13.1.\n\n\nRun Terraform\nInside jetstream_kubespray, copy from my template:\nexport CLUSTER=$USER\ncp -LRp inventory/zonca inventory/$CLUSTER\ncd inventory/$CLUSTER\nOpen and modify cluster.tfvars, choose your image and number of nodes. Make sure to change the network name to something unique, like the expanded form of $CLUSTER_network.\nYou can find suitable images (they need to be JS-API-Featured, you cannot use the same instances used in Atmosphere):\nopenstack image list | grep \"JS-API\"\nThe default is JS-API-Featured-Ubuntu18-Latest.\nI already preconfigured the network UUID both for IU and TACC, but you can crosscheck looking for the public network in:\nopenstack network list\nInitialize Terraform:\nbash terraform_init.sh\nCreate the resources:\nbash terraform_apply.sh\nThe last output log of Terraform should contain the IP of the master node k8s_master_fips, wait for it to boot then SSH in with:\nssh ubuntu@$IP\nor centos@$IP for CentOS images.\nInspect with Openstack the resources created:\nopenstack server list\nopenstack network list\nYou can cleanup the virtual machines and all other Openstack resources (all data is lost) with bash terraform_destroy.sh."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Install Kubernetes with kubespray",
    "text": "Install Kubernetes with kubespray\nChange folder back to the root of the jetstream_kubespray repository,\nFirst make sure you have a recent version of ansible installed, you also need additional modules, so first run:\npip install -r requirements.txt\nIt is useful to create a virtualenv and install packages inside that. This will also install ansible, it is important to install ansible with pip so that the path to access the modules is correct. So remove any pre-installed ansible.\nOr install ansible with conda, the minimum required version of ansible is 2.9.\nThen following the kubespray documentation, we setup ssh-agent so that ansible can SSH from the machine with public IP to the others:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nTest the connection through ansible:\nansible -i inventory/$CLUSTER/hosts -m ping all\nIf a server is not answering to ping, first try to reboot it:\nopenstack server reboot $CLUSTER-k8s-node-nf-1\nOr delete it and run terraform_apply.sh to create it again.\ncheck inventory/$CLUSTER/group_vars/all.yml, in particular bootstrap_os, I setup ubuntu, change it to centos if you used the Centos 7 base image.\nFinally run the full playbook, it is going to take a good 10 minutes:\nbash k8s_install.sh\nIf the playbook fails with “cannot lock the administrative directory”, it is due to the fact that the Virtual Machine is automatically updating so it has locked the APT directory. Just wait a minute and launch it again. It is always safe to run ansible multiple times.\nIf the playbook gives any error, try to retry the above command, sometimes there are temporary failed tasks, Ansible is designed to be executed multiple times with consistent results.\nYou should have now a Kubernetes cluster running, test it:\n$ ssh ubuntu@$IP\n$ sudo su\n$ kubectl get pods --all-namespaces\nNAMESPACE       NAME                                                   READY     STATUS    RESTARTS   AGE\ncert-manager    cert-manager-597bc67bf9-cf7xv                 1/1     Running   0          2m9s\ningress-nginx   ingress-nginx-controller-tfqbn                1/1     Running   0          2m31s\nkube-system     coredns-76798d84dd-hhjs9                      1/1     Running   0          98s\nkube-system     coredns-76798d84dd-l6fh2                      1/1     Running   0          83s\nkube-system     dns-autoscaler-85f898cd5c-kld9w               1/1     Running   0          90s\nkube-system     kube-apiserver-dummy2-k8s-master-1            1/1     Running   1          5m11s\nkube-system     kube-controller-manager-dummy2-k8s-master-1   1/1     Running   0          5m11s\nkube-system     kube-flannel-tzj4h                            1/1     Running   0          3m13s\nkube-system     kube-flannel-xqq4j                            1/1     Running   0          3m13s\nkube-system     kube-proxy-tfpzd                              1/1     Running   0          3m39s\nkube-system     kube-proxy-z2djx                              1/1     Running   0          3m39s\nkube-system     kube-scheduler-dummy2-k8s-master-1            1/1     Running   1          5m11s\nkube-system     kubernetes-dashboard-77475cf576-ht5th         1/1     Running   0          82s\nkube-system     kubernetes-metrics-scraper-747b4fd5cd-sdxsc   1/1     Running   0          82s\nkube-system     nginx-proxy-dummy2-k8s-node-nf-1              1/1     Running   0          3m21s\nkube-system     nodelocaldns-sg8x4                            1/1     Running   0          86s\nkube-system     nodelocaldns-vljgx                            1/1     Running   0          86s\nCompare that you have all those services running also in your cluster. We have also configured NGINX to proxy any service that we will later deploy on Kubernetes, test it with:\n$ wget localhost\n--2018-09-24 03:01:14--  http://localhost/\nResolving localhost (localhost)... 127.0.0.1\nConnecting to localhost (localhost)|127.0.0.1|:80... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2018-09-24 03:01:14 ERROR 404: Not Found.\nError 404 is a good sign, the service is up and serving requests, currently there is nothing to deliver. Finally test that the routing through the Jetstream instance is working correctly by opening your browser and test that if you access js-XX-XXX.jetstream-cloud.org you also get a default backend - 404 message. If any of the tests hangs or cannot connect, there is probably a networking issue."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "(Optional) Setup kubectl locally",
    "text": "(Optional) Setup kubectl locally\nWe also set kubectl_localhost: true and kubeconfig_localhost: true. so that kubectl is installed on your local machine\nOr install it yourself, I tested with 1.18.3.\nit also copies admin.conf to:\ninventory/$CLUSTER/artifacts\nI have a script to copy that to .config/kube and to replace the IP with localhost, because we cannot replace it with the public floating ip because the certificate is not valid for that.\nbash k8s_configure_kubectl_locally.sh\nThen make a SSH tunnel (lasts 3 hours):\nbash k8s_tunnel.sh"
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#optional-setup-helm-locally",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#optional-setup-helm-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "(Optional) Setup helm locally",
    "text": "(Optional) Setup helm locally\nInstall helm 3 from the release page on Github\nI tested with v3.2.4."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#install-jupyterhub",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#install-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nIt is preferable to run the Hub and the Proxy on the master node, just in case we want to downsize the cluster to only one node to save resources.\nWe need to remove the taint from the master node because currently the JupyterHub recipe doesn’t allow to add tolerations to the master node.\nkubectl edit node $CLUSTER-k8s-master-1\nRemove the 3 lines with the taint:\ntaints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\nNow checkout the JupyterHub configuration files repository on the local machine (if you have setup kubectl and helm locally, otherwise on the master node).\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\nInside that, first run\nbash create_secrets.sh\nto create the secret strings needed by JupyterHub then edit its output secrets.yaml to make sure it is consistent, edit the hosts lines if needed. For example, supply the Jetstream DNS name of the master node js-XXX-YYY.jetstream-cloud.org (XXX and YYY are the last 2 groups of the floating IP of the instance AAA.BBB.XXX.YYY).\nbash configure_helm_jupyterhub.sh\nkubectl create namespace jhub\nbash install_jhub.sh\nCheck some preliminary pods running with:\nkubectl get pods -n jhub\nOnce the proxy is running, even if hub is still in preparation, you can check in browser, you should get “Service Unavailable” which is a good sign that the proxy is working."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#customize-jupyterhub",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#customize-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Customize JupyterHub",
    "text": "Customize JupyterHub\nAfter JupyterHub is deployed and integrated with Cinder for persistent volumes, for any other customizations, first authentication, you are in good hands as the Zero-to-Jupyterhub documentation is great."
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#setup-https-with-letsencrypt",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#setup-https-with-letsencrypt",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Setup HTTPS with letsencrypt",
    "text": "Setup HTTPS with letsencrypt\nKubespray has the option of deploying also cert-manager, but I had trouble deploying an issuer, it was easier to just deploy it afterwards following my previous tutorial"
  },
  {
    "objectID": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#feedback",
    "href": "posts/2020-06-15-jetstream_kubernetes_kubespray.html#feedback",
    "title": "Deploy Kubernetes on Jetstream with Kubespray",
    "section": "Feedback",
    "text": "Feedback\nFeedback on this is very welcome, please open an issue on the Github repository or email me at zonca on the domain of the San Diego Supercomputer Center (sdsc.edu)."
  },
  {
    "objectID": "posts/2020-06-01-git.html",
    "href": "posts/2020-06-01-git.html",
    "title": "My own git cheatsheet",
    "section": "",
    "text": "I noticed I always google the same procedures…"
  },
  {
    "objectID": "posts/2020-06-01-git.html#remove-a-file-from-a-commit",
    "href": "posts/2020-06-01-git.html#remove-a-file-from-a-commit",
    "title": "My own git cheatsheet",
    "section": "Remove a file from a commit",
    "text": "Remove a file from a commit\nDirect remove without editing anything:\ngit reset HEAD^ -- path/to/file\ngit commit --amend --no-edit\nStep-by-step:\ngit reset --soft HEAD~1\ngit reset HEAD path/to/unwanted_file\ngit commit -c ORIG_HEAD\nSource"
  },
  {
    "objectID": "posts/2020-06-01-git.html#only-checkout-a-subset-of-files-stored-in-git-lfs",
    "href": "posts/2020-06-01-git.html#only-checkout-a-subset-of-files-stored-in-git-lfs",
    "title": "My own git cheatsheet",
    "section": "Only checkout a subset of files stored in git LFS",
    "text": "Only checkout a subset of files stored in git LFS\nGIT_LFS_SKIP_SMUDGE=1 gh repo clone orgname/reponame\nThen checkout only specific files:\ngit lfs fetch --include=\"pattern*\" --exclude=\"\"\ngit lfs checkout"
  },
  {
    "objectID": "posts/2020-06-01-git.html#configure-git-to-automatically-run-git-submodule-update",
    "href": "posts/2020-06-01-git.html#configure-git-to-automatically-run-git-submodule-update",
    "title": "My own git cheatsheet",
    "section": "Configure git to automatically run git submodule update",
    "text": "Configure git to automatically run git submodule update\nThis works when switching branches:\ngit config --global submodule.recurse true"
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "",
    "text": "This tutorial deploys Kubernetes on Jetstream with Magnum and then JupyterHub on top of that using zero-to-jupyterhub.\nThis is an updated version of the Kubernetes on Jetstream with Magnum tutorial based now on Kubernetes 1.15.7 instead of Kubernetes 1.11, the node images are based on Fedora Atomic 29 and the Jetstream Magnum deployment is now updated to the Openstack Train release. If you need a newer version of Kubernetes, you can install Kubernetes with Kubespray instead, see this tutorial."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#setup-access-to-the-jetstream-api",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#setup-access-to-the-jetstream-api",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Setup access to the Jetstream API",
    "text": "Setup access to the Jetstream API\nFirst install the OpenStack client, please use these exact versions, also please run at Indiana, the TACC deployment has an older release of Openstack.\npip install python-openstackclient==3.18 python-magnumclient==2.10\nLoad your API credentials from openrc.sh, check documentation of the Jetstream wiki for details.\nYou need to have a keypair uploaded to Openstack, this just needs to be done once per account. See the Jetstream documentation under the section “Upload SSH key - do this once”."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Create the cluster with Magnum",
    "text": "Create the cluster with Magnum\nAs usual, checkout the repository with all the configuration files on the machine you will use the Jetstream API from, typically your laptop.\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\ncd jupyterhub-deploy-kubernetes-jetstream\ncd kubernetes_magnum\nNow we are ready to use Magnum to first create a cluster template and then the actual cluster, edit first create_cluster.sh and set the parameters of the cluster on the top. Also make sure to set the keypair name. Create the network resources and the cluster template, these need to be created just once, then they can be reused by clusters created later on:\nbash create_network.sh\nbash create_template.sh\nThen a cluster can be created with:\nbash create_cluster.sh\nThe cluster consumes resources when active, it can be switched off with:\nbash delete_cluster.sh\nConsider this is deleting all Jetstream virtual machines and data that could be stored in JupyterHub.\nI have setup a test cluster with only 1 master node and 1 normal node but you can modify that later.\nCheck the status of your cluster, after about 10 minutes, it should be in state CREATE_COMPLETE:\nopenstack coe cluster show k8s\n\nConfigure kubectl locally\nInstall the kubectl client locally, first check the version of the master node:\nopenstack server list # find the floating public IP of the master node (starts with 149_\nIP=149.xxx.xxx.xxx\nssh fedora@$IP\nkubectl version\nNow install the same version following the Kubernetes documentation\nNow configure kubectl on your laptop to connect to the Kubernetes cluster created with Magnum:\nmkdir kubectl_secret\ncd kubectl_secret\nopenstack coe cluster config k8s\nThis downloads a configuration file and the required certificates.\nand returns export KUBECONFIG=/absolute/path/to/config\nSee also the update_kubectl_secret.sh script to automate this step, but it requires to already have setup the environment variable.\nexecute that and then:\nkubectl get nodes\nwe can also verify the Kubernetes version available, it should now be 1.15.7:\nkubectl version\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.7\", GitCommit:\"6c143d35bb11d74970e7bc0b6c45b6bfdffc0bd4\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T12:34:17Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}"
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#configure-storage",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#configure-storage",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Configure storage",
    "text": "Configure storage\nMagnum configures a provider that knows how to create Kubernetes volumes using Openstack Cinder, but does not configure a storageclass, we can do that with:\nkubectl create -f storageclass.yaml\nWe can test this by creating a Persistent Volume Claim:\nkubectl create -f persistent_volume_claim.yaml\n\nkubectl describe pv\n\nkubectl describe pvc\nName:            pvc-e8b93455-898b-11e9-a37c-fa163efb4609\nLabels:          failure-domain.beta.kubernetes.io/zone=nova\nAnnotations:     kubernetes.io/createdby: cinder-dynamic-provisioner\n                 pv.kubernetes.io/bound-by-controller: yes\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/cinder\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    standard\nStatus:          Bound\nClaim:           default/pvc-test\nReclaim Policy:  Delete\nAccess Modes:    RWO\nCapacity:        5Gi\nNode Affinity:   &lt;none&gt;\nMessage:\nSource:\n    Type:       Cinder (a Persistent Disk resource in OpenStack)\n    VolumeID:   2795724b-ef11-4053-9922-d854107c731f\n    FSType:\n    ReadOnly:   false\n    SecretRef:  nil\nEvents:         &lt;none&gt;\nWe can also test creating an actual pod with a persistent volume and check that the volume is successfully mounted and the pod started:\nkubectl create -f ../alpine-persistent-volume.yaml\nkubectl describe pod alpine\n\nNote about availability zones\nBy default Openstack servers and Openstack volumes are created in different availability zones. This created an issue with the default Magnum templates because we need to modify the Kubernetes scheduler policy to allow this. Kubespray does this by default, so I created a fix to be applied to the Jetstream Magnum templates, this needs to be re-applied after every Openstack upgrade. The Jetstream team has applied these fixes, they are linked here just for reference."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#install-helm",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#install-helm",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Install Helm",
    "text": "Install Helm\nThe Kubernetes deployment from Magnum is not as complete as the one out of Kubespray, we need to setup the NGINX ingress ourselves.\nInstall the Helm client on your laptop, make sure you install Helm 3 or later.\nRun helm ls and make sure it doesn’t give any error message but just an empty result."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#setup-nginx-ingress",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#setup-nginx-ingress",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Setup NGINX ingress",
    "text": "Setup NGINX ingress\nWe need to have the NGINX web server to act as front-end to the services running inside the Kubernetes cluster.\n\nOpen HTTP and HTTPS ports\nFirst we need to open the HTTP and HTTPS ports on the master node, you can either connect to the Horizon interface, create new rule named http_https, then add 2 rules, in the Rule drop down choose HTTP and HTTPS; or from the command line:\nopenstack security group create http_https\nopenstack security group rule create --ingress --protocol tcp --dst-port 80 http_https\nopenstack security group rule create --ingress --protocol tcp --dst-port 443 http_https\nThen you can find the name of the master node in openstack server list then add this security group to that instance:\nopenstack server add security group  k8s-xxxxxxxxxxxx-master-0 http_https\n\n\nInstall NGINX ingress with Helm\nWe prefer to run the NGINX ingress on the master node, in fact in the configuration in nginx.yaml specifies:\nnodeSelector:\n    node-role.kubernetes.io/master: \"\"\nThis is useful to reduce traffic across the cluster, install NGINX using Helm:\nbash install_nginx_ingress.sh\nNote, the documentation says we should add this annotation to ingress with kubectl edit ingress -n jhub, but I found out it is not necessary:\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\nIf this is correctly working, you should be able to run curl localhost from the master node and get a Default backend: 404 message."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Install JupyterHub",
    "text": "Install JupyterHub\nFinally, we can go back to the root of the repository and install JupyterHub, first create the secrets file:\nbash create_secrets.sh\nThen edit secrets.yaml and modify the hostname under hosts to display the hostname of your master Jetstream instance, i.e. if your instance public floating IP is aaa.bbb.xxx.yyy, the hostname should be js-xxx-yyy.jetstream-cloud.org (without http://).\nYou should also check that connecting with your browser to js-xxx-yyy.jetstream-cloud.org shows default backend - 404, this means NGINX is also reachable from the internet, i.e. the web port is open on the master node.\nFinally:\nbash configure_helm_jupyterhub.sh\nkubectl create namespace jhub\nbash install_jhub.sh\nConnect with your browser to js-xxx-yyy.jetstream-cloud.org to check if it works.\nWe are installing the zero-to-jupyterhub helm recipe version 0.9.0 instead of 0.8.2.\n\nAllow services on master\nBy default the new Kubernetes version has 1 taint on the master node:\n  taints:\n  - effect: NoSchedule\n    key: dedicated\n    value: master\nThe JupyterHub recipe does not allow to automatically set tolerations on the hub and the proxy pods, therefore if we want to run them on master, the easiest way is to delete that taint from the master node:\nkubectl edit node NODENAME"
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Issues and feedback",
    "text": "Issues and feedback\nPlease open an issue on the repository to report any issue or give feedback. Also you find out there there what I am working on next."
  },
  {
    "objectID": "posts/2020-05-21-jetstream_kubernetes_magnum.html#acknowledgments",
    "href": "posts/2020-05-21-jetstream_kubernetes_magnum.html#acknowledgments",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to Jeremy Fischer and Mike Lowe for upgrading the infrastructure to the new Magnum and Kubernetes versions and applying the necessary fixes."
  },
  {
    "objectID": "posts/2020-04-01-jetstream-object-store.html",
    "href": "posts/2020-04-01-jetstream-object-store.html",
    "title": "Use the Jetstream object store",
    "section": "",
    "text": "I plan to collect here notes about using the Openstack object store on Jetstream."
  },
  {
    "objectID": "posts/2020-04-01-jetstream-object-store.html#get-amazon-style-credentials",
    "href": "posts/2020-04-01-jetstream-object-store.html#get-amazon-style-credentials",
    "title": "Use the Jetstream object store",
    "section": "Get Amazon-style credentials",
    "text": "Get Amazon-style credentials\nMost of the ecosystem is used to Amazon S3, so Openstack Swift provides Amazon compatible APIs, to access those, make sure your openstack client can authenticate with the correct allocation, then run:\nopenstack ec2 credentials create\nThis prints on the screen the Access and the Secret keys, those can be used in all tools which expects Amazon APIs."
  },
  {
    "objectID": "posts/2020-04-01-jetstream-object-store.html#command-line-access-to-object-store-with-s3cmd",
    "href": "posts/2020-04-01-jetstream-object-store.html#command-line-access-to-object-store-with-s3cmd",
    "title": "Use the Jetstream object store",
    "section": "Command line access to object store with s3cmd",
    "text": "Command line access to object store with s3cmd\nOne of the most convenient tools is s3cmd, which allows to list, upload and download files from object store. It is included in most linux distributions.\nFirst run the interactive configuration tool:\ns3cmd --configure\nAnd set:\n\nRegion: RegionOne\nAny password for encryption\nUse HTTPS: Yes\nDo not test\nSave the configuration\n\nNow edit ~/.s3cfg:\n\nset check_ssl_certificate and check_ssl_hostname to False\nset host_base=JETSTREAM_SWIFT_ENDPOINT where JETSTREAM_SWIFT_ENDPOINT is just the hostname, without https:// and without /swift/v1, I prefer not to post publicly check on your Openstack dashboard or email me.\n\nNow you can list the content of buckets/containers:\n&gt; s3cmd ls\n2020-03-11 23:25  s3://data_store\n\n&gt; s3cmd ls s3://data_store\n                       DIR   s3://data_store/bbb/\n                       DIR   s3://data_store/data/\n                       DIR   s3://data_store/fff/\n2020-03-27 01:39       500   s3://data_store/nginx-cinder.yaml\n\n&gt; s3cmd put local_file.txt s3://data_store/fff/ggg"
  },
  {
    "objectID": "posts/2020-03-12-kill-jupyter-notebook.html",
    "href": "posts/2020-03-12-kill-jupyter-notebook.html",
    "title": "Kill Jupyter Notebook servers",
    "section": "",
    "text": "Just today I learned how to properly stop previously running Jupyter Notebook servers, here for future reference:\njupyter notebook stop\nThis is going to print all the ports of the currently running servers. Choose which ones to stop then:\njupyter notebook stop PORTNUMBER"
  },
  {
    "objectID": "posts/2020-02-26-kubernetes_cvmfs.html",
    "href": "posts/2020-02-26-kubernetes_cvmfs.html",
    "title": "Deploy CVMFS on Kubernetes",
    "section": "",
    "text": "CVMFS is a software distribution service, it is used by High Energy Physics experiments at CERN to synchronize software environments across the whole collaborations.\nIn the context of a Kubernetes + JupyterHub deployment on Jetstream, for example deployed using Magnum following my tutorial, it is useful to use CVMFS to make the software tools of a collaboration to all the users connected to JupyterHub, so that we can keep the base Docker image simpler and smaller."
  },
  {
    "objectID": "posts/2020-02-26-kubernetes_cvmfs.html#alternatives",
    "href": "posts/2020-02-26-kubernetes_cvmfs.html#alternatives",
    "title": "Deploy CVMFS on Kubernetes",
    "section": "Alternatives",
    "text": "Alternatives\nA already existing solution is the CVMFS CSI driver, however it doesn’t have much documentation, so I haven’t tested it. It would be useful for larger deployments, but we are designing for a 5 (possibly up to 10) nodes Kubernetes cluster."
  },
  {
    "objectID": "posts/2020-02-26-kubernetes_cvmfs.html#architecture",
    "href": "posts/2020-02-26-kubernetes_cvmfs.html#architecture",
    "title": "Deploy CVMFS on Kubernetes",
    "section": "Architecture",
    "text": "Architecture\nWe have a pod running in Kubernetes (running as a privileged Docker container) which runs the CVMFS client and caches locally (on a dedicated Openstack volume) some pre-defined CVMFS repositories (at the moment we do not support automounting).\nCurrently we are using the DIRECT connection for the CVMFS client, due to having just a single client which accesses a small amount of data. Using a proxy is required instead for heavier usage, and it could also be deployed inside Kubernetes.\nThe same pod also runs a NFS server and exposes it internally into the Kubernetes cluster, over the local Jetstream network, to any other pod which can use a NFS volume and mount it to the /cvmfs folder inside the container. We also activate the CVMFS configuration options for NFS support, following the documentation."
  },
  {
    "objectID": "posts/2020-02-26-kubernetes_cvmfs.html#deployment",
    "href": "posts/2020-02-26-kubernetes_cvmfs.html#deployment",
    "title": "Deploy CVMFS on Kubernetes",
    "section": "Deployment",
    "text": "Deployment\nThe repositories used in this deployment are:\n\nGithub repository for the Docker image of the CVMFS client\nDocker Hub repositories where the 2 containers are built: cvmfs-client and cvmfs-client-nfs\nThe jupyterhub-deploy-kubernetes-jetstream Github repositories with the Kubernetes configuration files\n\nFirst we need to checkout the jupyterhub-deploy-kubernetes-jetstream repository:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream.git\ncd jupyterhub-deploy-kubernetes-jetstream/cvmfs\nThen configure the CVMFS pod with the required repositories, see the CVMFS_REPOSITORIES variable in pod_cvmfs_nfs.yaml.\nThen deploy the pod with:\nkubectl create -f pod_cvmfs_nfs.yaml\nThis creates 2 Openstack volumes, a 20 GB volume for the CVMFS cache, and a 1 GB volume which is just necessary as the /cvmfs root folder of the NFS server. It also creates the nfs-service Service, with a fixed IP, so that we can use it in the pod using this.\nFinally we can create a pod using mounting the folder via NFS:\nkubectl create -f test_nfs_mount.yaml\nThen get a terminal in the pod with:\nbash ../terminal_pod.sh test-nfs-mount\nThis creates a volume which mounts the /cvmfs folder shared with NFS, this automatically also shares also all the subfolders.\nFinally we can check the content of the /cvmfs folder."
  },
  {
    "objectID": "posts/2019-10-30-jetstream_kubernetes_loadtest.html",
    "href": "posts/2019-10-30-jetstream_kubernetes_loadtest.html",
    "title": "Simulate users on JupyterHub",
    "section": "",
    "text": "Updated January 2021\nI currently have 2 different strategies to deploy JupyterHub on top of Kubernetes on Jetstream:\nIn this tutorial I’ll show how to use Yuvi Pandas’ hubtraf to simulate load on JupyterHub, i.e. programmatically generate a predefined number of users connecting and executing notebooks on the system.\nThis is especially useful to test the Cluster Autoscaler.\nhubtraf assumes you are using the Dummy authenticator, which is the default installed by the zero-to-jupyterhub helm chart. If you have configured another authenticator, temporarily disable it for testing purposes.\nFirst go through the hubtraf documentation to understand its functionalities.\nhubtraf also has a Helm recipe to run it within Kubernetes, but the simpler way is to test from your laptop, follow the [documentation of hubtraf] to install the package and then run:\nTo simulate 2 users connecting to the system, you can then check with:\nThat the pods are being created successfully and check the logs on the command line from hubtraf which explains what it is doing and tracks the time every operation takes, so it is useful to debug any delay in providing resources to users.\nConsider that volumes created by JupyterHub for the test users will remain in Kubernetes and in Openstack, therefore if you would like to use the same deployment for production, remember to cleanup the Kubernetes PersistentVolume and PersistentVolumeClaim resources.\nNow we can test scalability of the deployment with:\nMake sure you have asked the XSEDE support to increase the maximum number of volumes in Openstack in your allocation that by default is only 10. Otherwise edit config_standard_storage.yaml and set:"
  },
  {
    "objectID": "posts/2019-10-30-jetstream_kubernetes_loadtest.html#test-the-cluster-autoscaler",
    "href": "posts/2019-10-30-jetstream_kubernetes_loadtest.html#test-the-cluster-autoscaler",
    "title": "Simulate users on JupyterHub",
    "section": "Test the Cluster Autoscaler",
    "text": "Test the Cluster Autoscaler\nIf you followed the tutorial to deploy the Cluster Autoscaler on Magnum, you can launch hubtraf to create a large number of pods, then check that some pods are “Running” and the ones that do not fit in the current nodes are “Pending”:\nkubectl get pods -n jhub\nand then check in the logs of the autoscaler that it detects that those pods are pending and requests additional nodes. For example:\n&gt; kubectl logs -n kube-system cluster-autoscaler-hhhhhhh-uuuuuuu\nI1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2\nI1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&gt;2\nI1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_IN_PROGRESS status\nAfter 4 or 5 minutes the new node should be available and should show up in:\nkubectl get nodes\nAnd we can check that some user pods are now running on the new node:\nkubectl get pods -n jhub -o wide\nIn my case the Autoscaler actually requested a 3rd node to accomodate all the users pods:\nI1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2\nI1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&gt;2\nI1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_IN_PROGRESS status\nI1031 00:52:51.308054       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_COMPLETE status\nI1031 00:53:01.315179       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 3\nI1031 00:53:02.996583       1 magnum_nodegroup.go:101] Increasing size by 1, 2-&gt;3\nI1031 00:53:35.607158       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_IN_PROGRESS status\nI1031 00:56:41.834151       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_COMPLETE status\nMoreover Cluster Autoscaler also provides useful information in the status of each “Pending” node. For example if it detects that it is useless to create a new node because the node is “Pending” for some other reason (e.g. volume quota reached), this infomation will be accessible using:\nkubectl describe node -n jhub jupyter-xxxxxxx\nWhen the simulated users disconnect, hubtraf has a default of about 5 minutes, the autoscaler waits for the configured amount of minutes, by default it is 10 minutes, in my deployment it is 1 minute to simplify testing, see the cluster-autoscaler-deployment-master.yaml file. After this delay, the autoscaler scales down the size of the cluster, it is a 2 step process, it first terminates the Openstack Virtual machine and then adjusts the size of the Magnum cluster (node_count), you can monitor the process using openstack server list and openstack coe cluster list, and the log of the autoscaler:\nI1101 06:31:10.223660       1 scale_down.go:882] Scale-down: removing empty node k8s-e2iw7axmhym7-minion-1 \nI1101 06:31:16.081223       1 magnum_manager_heat.go:276] Waited for stack UPDATE_IN_PROGRESS status\nI1101 06:32:17.061860       1 magnum_manager_heat.go:276] Waited for stack UPDATE_COMPLETE status\nI1101 06:32:49.826439       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_IN_PROGRESS status\nI1101 06:33:21.588022       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_COMPLETE status"
  },
  {
    "objectID": "posts/2019-10-30-jetstream_kubernetes_loadtest.html#acknowledgments",
    "href": "posts/2019-10-30-jetstream_kubernetes_loadtest.html#acknowledgments",
    "title": "Simulate users on JupyterHub",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks Yuvi Panda for providing hubtraf, thanks Julien Chastang for testing my deployments."
  },
  {
    "objectID": "posts/2019-08-24-github-for-research-groups.html",
    "href": "posts/2019-08-24-github-for-research-groups.html",
    "title": "Create a Github account for your research group with free private repositories",
    "section": "",
    "text": "Github allows a research group to create their own webpage where they can host, share and develop their software using the git version control system and the powerful Github online issue-tracking interface.\nGithub offers unlimited private and public repositories to research groups and classrooms. Private repositories are useful for early stages of development or if it is necessary to keep software secret before publication, at publication they can easily switched to public repositories and free up their slot.\nThey also provide free data packs for git-lfs(Large File Support) which is useful to store large amount of binary data together with your software in the same repository, without actually committing the files into git but using a support server. Just go into “Settings” for your organization and under “Billing” add data packs, you will notice that the cost is $0.\nHere the steps to set this up:\n\nCreate a user account on Github and choose the free plan, use your .edu email address\nCreate an organization account for your research group\nGo to https://education.github.com/ and click on “Get benefits”\nChoose what is your position, e.g. Researcher and select you want a discount for an organization\nChoose the organization you created earlier and confirm that it is a “Research group”\nAdd details about your Research group\nFinally you need to upload a picture of your University ID card and write how you plan on using the repositories\nWithin a week at most, but generally in less than 24 hours, you will be approved for unlimited private repositories.\n\nOnce the organization is created, you can add key team members to the “Owners” group, and then create another group for students and collaborators.\nConsider also that is not necessary for every collaborator to have write access to your repositories. My recommendation is to ask a more experienced team member to administer the central repository, ask the students to fork the repository under their user accounts (forks of private repositories are always private, free and don’t use any slot), and then send a pull request to the central repository for the administrator to review, discuss and merge.\nSee for example the organization account of the “The Lab for Data Intensive Biology” led by Dr. C. Titus Brown where they share code, documentation and papers. Open Science!!\nOther suggestions on the setup very welcome!"
  },
  {
    "objectID": "posts/2019-05-30-webinar_python_hpc.html",
    "href": "posts/2019-05-30-webinar_python_hpc.html",
    "title": "Webinar about distributed computing with Python",
    "section": "",
    "text": "Recording available of the webinar I gave about “Distributed computing with Python”:\n\nThreads vs Processes, GIL\nJust-In-Time compilation with Numba\nProcessing data larger than memory with Dask\nDistributed computing with Dask\n\nLive demo on my favorite Supercomputer Comet at the San Diego Supercomputer Center.\n\nWebinar recording\nNotebooks: https://github.com/zonca/python_hpc_tutorial"
  },
  {
    "objectID": "posts/2020-09-10-wcs-astropy.html",
    "href": "posts/2020-09-10-wcs-astropy.html",
    "title": "Simple WCS with astropy modeling and gwcs",
    "section": "",
    "text": "Some notes on how to convert a FITS WCS to a WCS from gwcs (Generalized WCS) to be used within the JWST pipeline\nimport astropy\nfrom astropy.io import fits\nimport numpy as np\nfrom astropy import units as u\nfrom astropy import wcs\nhdulist = fits.open(\"example_field/iris_sim_gc_filterKN3.fits\")\nThe conventional FITS WCS is defined by keywords in the FITS file and is automatically parsed by astropy.wcs.WCS\nhdulist[0].header[:22]\n\nSIMPLE  =                    T / Written by IDL:  Mon Dec 16 16:40:46 2019      \nBITPIX  =                  -64 /  IEEE double precision floating point          \nNAXIS   =                    2 /                                                \nNAXIS1  =                 4096 /                                                \nNAXIS2  =                 4096 /                                                \nINSTR   = 'IRIS    '           /                                                \nSCALE   =           0.00400000 / pixel scale (arcsec)                           \nUNITS   = 'electrons'          /                                                \nCOORD_SY= 'C       '           /                                                \nRADECSYS= 'FK5     '           /                                                \nCTYPE1  = 'RA--LINEAR'         /                                                \nCTYPE2  = 'DEC-LINEAR'         /                                                \nCUNIT1  = 'deg     '           /                                                \nCUNIT2  = 'deg     '           /                                                \nCRPIX1  =              2048.12 /                                                \nCRPIX2  =              2048.12 /                                                \nCRVAL1  =        265.197723389 /                                                \nCRVAL2  =       -28.9921894073 /                                                \nCDELT1  =    1.11111013731E-06 /0.00400000 pixel scale                          \nCDELT2  =    1.11111013731E-06 /0.00400000 pixel scale                          \nCROTA1  =                    0 /                                                \nCROTA2  =                    0 /\nCannot make LINEAR to work, so let’s instead replace it with Gnomonic\nhdulist[0].header[\"CTYPE1\"] = \"RA---TAN\"\nhdulist[0].header[\"CTYPE2\"] = \"DEC--TAN\"\nw = wcs.WCS(hdulist[0])\n\nWARNING: FITSFixedWarning: RADECSYS= 'FK5 ' / \nthe RADECSYS keyword is deprecated, use RADESYSa. [astropy.wcs.wcs]\nWARNING: FITSFixedWarning: EPOCH = '2019-12-17T00:40:46.48107737302794Z' / \na floating-point value was expected. [astropy.wcs.wcs]\nw.to_header()\n\nWCSAXES =                    2 / Number of coordinate axes                      \nCRPIX1  =              2048.12 / Pixel coordinate of reference point            \nCRPIX2  =              2048.12 / Pixel coordinate of reference point            \nCDELT1  =    1.11111013731E-06 / [deg] Coordinate increment at reference point  \nCDELT2  =    1.11111013731E-06 / [deg] Coordinate increment at reference point  \nCUNIT1  = 'deg'                / Units of coordinate increment and value        \nCUNIT2  = 'deg'                / Units of coordinate increment and value        \nCTYPE1  = 'RA---TAN'           / Right ascension, gnomonic projection           \nCTYPE2  = 'DEC--TAN'           / Declination, gnomonic projection               \nCRVAL1  =        265.197723389 / [deg] Coordinate value at reference point      \nCRVAL2  =       -28.9921894073 / [deg] Coordinate value at reference point      \nLONPOLE =                180.0 / [deg] Native longitude of celestial pole       \nLATPOLE =       -28.9921894073 / [deg] Native latitude of celestial pole        \nMJDREFI =                  0.0 / [d] MJD of fiducial time, integer part         \nMJDREFF =                  0.0 / [d] MJD of fiducial time, fractional part      \nRADESYS = 'FK5'                / Equatorial coordinate system                   \nEQUINOX =               2000.0 / [yr] Equinox of equatorial coordinates\nWe can then convert between the pixel indices and the coordinates in the sky\npixcrd = np.array([[0, 0], [0, 4096],[4096, 4096], [4096,0]], dtype=np.float64)\nworld = w.wcs_pix2world(pixcrd, 0)\nprint(world)\n\n[[265.19512288 -28.99446396]\n [265.195123   -28.98991285]\n [265.20032602 -28.98991285]\n [265.20032613 -28.99446396]]\nWe can calculate the size of the instrument field of view\n((-world[0][0]+ world[-1][0]) * u.deg).to(u.arcsec)\n\n\\(18.731693 \\; \\mathrm{{}^{\\prime\\prime}}\\)\n((-world[0][1]+ world[1][1]) * u.deg).to(u.arcsec)\n\n\\(16.383986 \\; \\mathrm{{}^{\\prime\\prime}}\\)\nw\n\nWCS Keywords\n\nNumber of WCS axes: 2\nCTYPE : 'RA---TAN'  'DEC--TAN'  \nCRVAL : 265.197723389  -28.9921894073  \nCRPIX : 2048.12  2048.12  \nNAXIS : 4096  4096"
  },
  {
    "objectID": "posts/2020-09-10-wcs-astropy.html#create-a-gwcs-wcs-object",
    "href": "posts/2020-09-10-wcs-astropy.html#create-a-gwcs-wcs-object",
    "title": "Simple WCS with astropy modeling and gwcs",
    "section": "Create a gwcs WCS object",
    "text": "Create a gwcs WCS object\nWe want now to use astropy.modeling to build a transformation that is equivalent to the FITS WCS transformation defined above\n\nfrom gwcs import WCS\n\n\nfrom astropy.modeling import models\nfrom astropy import coordinates as coord\nfrom astropy import units as u\nfrom gwcs import coordinate_frames as cf\n\n\nshift_by_crpix = models.Shift(-(hdulist[0].header[\"CRPIX1\"] - 1)*u.pix) & models.Shift(-(hdulist[0].header[\"CRPIX2\"] - 1)*u.pix)\n\n\ntan = models.Pix2Sky_TAN()\ncelestial_rotation =  models.RotateNative2Celestial(\n    hdulist[0].header[\"CRVAL1\"]*u.deg, hdulist[0].header[\"CRVAL2\"]*u.deg, 180*u.deg)\n\n\ntan.input_units_equivalencies = {\"x\": u.pixel_scale(hdulist[0].header[\"CDELT1\"] *u.deg/u.pix),\n                                      \"y\": u.pixel_scale(hdulist[0].header[\"CDELT2\"] *u.deg/u.pix)}\n\n\ndet2sky = shift_by_crpix | tan | celestial_rotation\ndet2sky.name = \"linear_transform\"\n\n\ndetector_frame = cf.Frame2D(name=\"detector\", axes_names=(\"x\", \"y\"),\n                            unit=(u.pix, u.pix))\nsky_frame = cf.CelestialFrame(reference_frame=coord.FK5(), name='fk5',\n                              unit=(u.deg, u.deg))\n\n\npipeline = [(detector_frame, det2sky),\n            (sky_frame, None)\n           ]\nwcsobj = WCS(pipeline)\nprint(wcsobj)\n\n  From      Transform    \n-------- ----------------\ndetector linear_transform\n     fk5             None\n\n\n\n wcsobj(pixcrd[:,0]*u.pix, pixcrd[:,1]*u.pix, with_units=False)\n\n(&lt;Quantity [265.19512288, 265.195123  , 265.20032602, 265.20032613] deg&gt;,\n &lt;Quantity [-28.99446396, -28.98991285, -28.98991285, -28.99446396] deg&gt;)\n\n\n\n((-_[0][0]+ _[0][-1])).to(u.arcsec)\n\n\\(18.731693 \\; \\mathrm{{}^{\\prime\\prime}}\\)\n\n\n\n((-__[1][0]+ __[1][1])).to(u.arcsec)\n\n\\(16.383986 \\; \\mathrm{{}^{\\prime\\prime}}\\)\n\n\n\nwcsobj\n\n&lt;WCS(output_frame=fk5, input_frame=detector, forward_transform=Model: CompoundModel\nName: linear_transform\nInputs: ('x0', 'x1')\nOutputs: ('alpha_C', 'delta_C')\nModel set size: 1\nExpression: [0] & [1] | [2] | [3]\nComponents: \n    [0]: &lt;Shift(offset=-2047.12 pix)&gt;\n\n    [1]: &lt;Shift(offset=-2047.12 pix)&gt;\n\n    [2]: &lt;Pix2Sky_Gnomonic()&gt;\n\n    [3]: &lt;RotateNative2Celestial(lon=265.19772339 deg, lat=-28.99218941 deg, lon_pole=180. deg)&gt;\nParameters:\n    offset_0 offset_1     lon_3            lat_3        lon_pole_3\n      pix      pix         deg              deg            deg    \n    -------- -------- ------------- ------------------- ----------\n    -2047.12 -2047.12 265.197723389 -28.992189407300003      180.0)&gt;"
  },
  {
    "objectID": "posts/2019-08-21-large_files_python_packages.html",
    "href": "posts/2019-08-21-large_files_python_packages.html",
    "title": "Ship large files with Python packages",
    "section": "",
    "text": "It is often useful to ship large data files together with a Python package, a couple of scenarios are:\n\ndata necessary to the functionality provided by the package, for example images, any binary or large text dataset, they could be either required just for a subset of the functionality of the package or for all of it\ndata necessary for unit or integration testing, both example inputs and expected outputs\n\nIf data are collectively less than 2 GB compressed and do not change very often, a simple and a bit hacky solution is to use GitHub release assets. For each packaged release on GitHub it is possible to attach one or more assets smaller than 2 GB. You can then attach data to each release, the downside is that users need to make sure to use the correct dataset for the release they are using and the first time they use the software the need to install the Python package and also download the dataset and install it in the right folder. See an example script to upload from the command line.\nIf data files are individually less than 10 MB and collectively less than 100 MB you can directly add them into the Python package. This is the easiest and most convenient option, for example the astropy package template automatically adds to the package any file inside the packagename/data folder.\nFor larger datasets I recommend to host the files externally and use the astropy.utils.data module. This module automates the process of retrieving a file from a remote server and caching it locally (in the users home folder), next time the user needs it, it is automatically retrieved from the cache:\n    dataurl = \"https://my-web-server.ucsd.edu/test-data/\"\n    with data.conf.set_temp(\"dataurl\", dataurl), data.conf.set_temp(\n        \"remote_timeout\", 30\n    ):\n        local_file_path = data.get_pkg_data_filename(\"myfile.jpg)\nNow we need to host there files publicly, I have a few options.\n\nHost on a dedicated GitHub repository\nIf files are individually less than 100MB and collectively a few GB, you can create a dedicated repository on GitHub and push there your files. Then activate GitHub Pages so that those files are published at https://your-organization.github.io/your-repository/. Then use this URL as dataurl in the above script.\n\n\nHost on a Supercomputer or own server\nSome Supercomputers offer the feature of providing public web access from specific folders, for example NERSC allows user to publish web-pages publicly, see their documentation.\nThis is very useful for huge datasets because you can automatically detect if the package is being run at NERSC and then automatically access the files with their path instead of downloading them.\nFor example:\n\ndef get_data_from_url(filename):\n    \"\"\"Retrieves input templates from remote server,\n    in case data is available in one of the PREDEFINED_DATA_FOLDERS defined above,\n    e.g. at NERSC, those are directly returned.\"\"\"\n\n    for folder in PREDEFINED_DATA_FOLDERS:\n        full_path = os.path.join(folder, filename)\n        if os.path.exists(full_path):\n            warnings.warn(f\"Access data from {full_path}\")\n            return full_path\n    with data.conf.set_temp(\"dataurl\", DATAURL), data.conf.set_temp(\n        \"remote_timeout\", 30\n    ):\n        warnings.warn(f\"Retrieve data for {filename} (if not cached already)\")\n        map_out = data.get_pkg_data_filename(filename, show_progress=True)\n    return map_out\nSimilar setup can be achieved on a GNU/Linux server, for example a powerful machine used by all members of a scientific team, where a folder is dedicated to host these data and is also published online with Apache or NGINX.\nThe main downside of this approach is that there is no built-in version control. One possibility is to enforce a policy where no files are ever overwritten and version control is automatically achieved with filenames. Otherwise, use git lfs in that folder to track any change in a dedicated local git repository, e.g.:\n\ngit init\ngit lfs track \"*.fits\"\ngit add \"*.fits\"\ngit commit -m \"initial version of all FITS files\"\nThis method tracks the checksum of all the binary files and helps managing the history, even if only locally (make sure the folder is also regularly backed up). You could push it to GitHub, that would cost $5/month for each 50GB of storage.\n\n\nHost on Figshare\nYou can upload files to Figshare using the browser and create a dataset which also comes with a DOI and a page where you can save metadata about this object.\nOnce you have set the dataset public, you can find out the URL of the actual file, which is of the form https://ndownloader.figshare.com/files/2432432432, therefore we can set https://ndownloader.figshare.com/files/ as the repository and use the integer defined in Figshare as filename. Using integers as filenames makes it a bit cryptic, but it has the great advantage that other people can do the uploading to Figshare and you can point to their files as easily as if the are yours. This is more convenient than alternatives where instead you need to give other people access to your file repository.\n\n\nHost on Amazon S3 or other object store\nA public bucket on Amazon S3 or other object store provides cheap storage and built-in version control. The cost currently is about $0.026/GB/month.\nFirst login to the AWS console and create a new bucket, set it public by turning of “Block all public access” and under “Access Control List” set “List objects” to Yes for “Public access”.\nYou could upload files with the browser, but for larger files command line is better.\nThe files will be available at https://bucket-name.s3-us-west-1.amazonaws.com/, this changes based on the chosen region.\n\n(Advanced) Upload files from the command line\nThis is optional and requires some more familiarity with AWS. Go back to the AWS console to the Identity and Access Management (IAM) section, then users, create, create a policy to give access only to 1 bucket (replace bucket-name):\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListObjectsInBucket\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": [\"arn:aws:s3:::bucket-name\"]\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*Object\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\"arn:aws:s3:::bucket-name/*\"]\n        }\n    ]\n}\nSee the AWS documentation\nInstall s3cmd, then run s3cmd --configure to set it up and paste the Access and Secret keys, it will fail to test the configuration because it cannot list all the buckets, anyway choose to save the configuration.\nTest it:\n    s3cmd ls s3://bucket-name\nThen upload your files (reduced redundancy is cheaper):\n    s3cmd put --reduced-redundancy --acl-public *.fits s3://bucket-name"
  },
  {
    "objectID": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html",
    "href": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html",
    "title": "Scale Kubernetes manually on Jetstream",
    "section": "",
    "text": "We would like to modify the number of Openstack virtual machines available to Kubernetes. Ideally we would like to do this automatically based on the load on JupyterHub, that is the target. For now we will increase and decrease the size manually. This can be useful for example if you make a test deployment with only 1 worker node a week before a workshop and then scale it up to 10 or more instances the day before the workshop begins.\nThis assumes you have deployed Kubernetes and JupyterHub already"
  },
  {
    "objectID": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html#create-a-new-openstack-virtual-machine-with-terraform",
    "href": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html#create-a-new-openstack-virtual-machine-with-terraform",
    "title": "Scale Kubernetes manually on Jetstream",
    "section": "Create a new Openstack Virtual Machine with Terraform",
    "text": "Create a new Openstack Virtual Machine with Terraform\nTo add nodes, enter the inventory/$CLUSTER folder, we can edit cluster.tf and increase number_of_k8s_nodes_no_floating_ip, in my testing I have increased it from 1 to 3.\nThen we can run again terraform_apply.sh, this should run Terraform and create a new resource:\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\nCheck first that your machine has booted correctly running:\nopenstack server list\n+--------------------------------------+---------------------+--------+--------------------------------------------+-------------------------------------+----------+\n| ID                                   | Name                | Status | Networks                                   | Image                               | Flavor   |\n+--------------------------------------+---------------------+--------+--------------------------------------------+-------------------------------------+----------+\n| 4ea73e65-2bff-42c9-8c4b-6c6928ad1b77 | zonca-k8s-node-nf-3 | ACTIVE | zonca_k8s_network=10.0.0.7                 | JS-API-Featured-Ubuntu18-Dec-7-2018 | m1.small |                                                       | 0cf1552e-ef0c-48b0-ac24-571301809273 | zonca-k8s-node-nf-2 | ACTIVE | zonca_k8s_network=10.0.0.11                | JS-API-Featured-Ubuntu18-Dec-7-2018 | m1.small |                                                       | e3731cde-cf6e-4556-8bda-0eebc0c7f08e | zonca-k8s-master-1  | ACTIVE | zonca_k8s_network=10.0.0.9, xxx.xxx.xxx.xx | JS-API-Featured-Ubuntu18-Dec-7-2018 | m1.small |\n| 443c6861-1a13-4080-b5a3-e005bb34a77c | zonca-k8s-node-nf-1 | ACTIVE | zonca_k8s_network=10.0.0.3                 | JS-API-Featured-Ubuntu18-Dec-7-2018 | m1.small |\n+--------------------------------------+---------------------+--------+--------------------------------------------+-------------------------------------+----------+\nAs expected we have now 1 master and 3 nodes.\nThen change the folder to the root of the repository and check you can connect to it with:\nansible -i inventory/$CLUSTER/hosts -m ping all\nIf any of the new nodes is Unreachable, you can try rebooting them with:\nopenstack server reboot zonca-k8s-node-nf-3\n\nConfigure the new instances for Kubernetes\nkubespray has a special playbook scale.yml that impacts as little as possible the nodes already running. I have created a script k8s_scale.sh in the root folder of my jetstream_kubespray repository, launch:\nbash k8s_scale.sh\nSee for reference the kubespray documentation\nOnce this completes (re-run it if it stops at some point), you should see what Ansible modified:\nzonca-k8s-master-1         : ok=25   changed=3    unreachable=0    failed=0                                   zonca-k8s-node-nf-1        : ok=247  changed=16   unreachable=0    failed=0\nzonca-k8s-node-nf-2        : ok=257  changed=77   unreachable=0    failed=0                                   zonca-k8s-node-nf-3        : ok=257  changed=77   unreachable=0    failed=0\nAt this point you should check the nodes are seen by Kubernetes with kubectl get nodes:\nNAME                  STATUS   ROLES    AGE     VERSION                                                       zonca-k8s-master-1    Ready    master   4h29m   v1.12.5                                                       zonca-k8s-node-nf-1   Ready    node     4h28m   v1.12.5                                                       zonca-k8s-node-nf-2   Ready    node     5m11s   v1.12.5                                                       zonca-k8s-node-nf-3   Ready    node     5m11s   v1.12.5"
  },
  {
    "objectID": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html#reduce-the-number-of-nodes",
    "href": "posts/2019-02-22-scale-kubernetes-jupyterhub-manually.html#reduce-the-number-of-nodes",
    "title": "Scale Kubernetes manually on Jetstream",
    "section": "Reduce the number of nodes",
    "text": "Reduce the number of nodes\nKubernetes is built to be resilient to node losses, so you could just brutally delete a node with openstack server delete. However, there is a dedicated playbook, remove-node.yml, to remove a node cleanly migrating any running services to other nodes and lowering the risk of anything malfunctioning. I created a script k8s_remove_node.sh, pass the name of the node you would like to eliminate (or a comma separated list of many names):\nbash k8s_remove_node.sh zonca-k8s-node-nf-3\nNow the node has disappeared by kubectl get nodes but the underlying Openstack instance is still running, delete it with:\nopenstack server delete zonca-k8s-node-nf-3\nFor consistency you could now modify inventory/$CLUSTER/cluster.tf and reduce the number of nodes accordingly."
  },
  {
    "objectID": "posts/2019-02-22-jetstream_kubernetes_kubespray_282.html",
    "href": "posts/2019-02-22-jetstream_kubernetes_kubespray_282.html",
    "title": "Deploy Kubernetes with Kubespray 2.8.2 and JupyterHub with helm recipe 0.8 on Jetstream",
    "section": "",
    "text": "This tutorial is obsolete, check the updated version of the tutorial\nBack in September 2018 I published a tutorial to deploy Kubernetes on Jetstream using Kubernetes.\nSoftware in the Kubernetes space moves very fast, so I decided to update the recipe to use the newer Kubespray 2.8.2 that deploys Kubernetes v1.12.5.\nPlease follow the old tutorial and note the updates below.\n\nSwitch to kubespray 2.8.2\nOnce you get my fork of kubespray with a few fixes for Jetstream:\ngit clone https://github.com/zonca/jetstream_kubespray\nswitch to the newer 2.8.2 version\ngit checkout -b branch_v2.8.2 origin/branch_v2.8.2\nSee an overview of my changes compared to the standard kubespray release 2.8.2.\n\n\nUse Ansible 2.7.10\nNewer versions of ansible do not work with Kubespray 2.8.2\nIt is available on conda-forge:\nconda install -c conda-forge ansible==2.7.10\n\n\nUse the new template\nThe name of my template is now just zonca instead of zonca_kubespray:\nBefore running Terraform, inside jetstream_kubespray, copy from my template:\nexport CLUSTER=$USER\ncp -LRp inventory/zonca inventory/$CLUSTER\ncd inventory/$CLUSTER\n\n\nExplore kubernetes\nIn case you are interested in exploring some of the capabilities of Kubernetes, you can check the second part of my tutorial, nothing in this section is required to run JupyterHub.\n\n\nInstall JupyterHub\nFinally you can use helm to install JupyterHub, see the last part of my tutorial.\nConsider that I have updated the repository https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream to install the 0.8.0 version of the helm package just released yesterday, see their blog post with more details.\n\n\nThanks\nThanks to the Kubernetes, Kubespray and JupyterHub community for delivering great open-source software and to XSEDE for giving me the opportunity to work on this. Special thanks to my collaborators Julien Chastang and Rich Signell."
  },
  {
    "objectID": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html",
    "href": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html",
    "title": "Deploy Pangeo on Kubernetes deployment on Jetstream created with Kubespray",
    "section": "",
    "text": "The Pangeo collaboration for Big Data Geoscience maintains a helm chart with a prefigured JupyterHub deployment on Kubernetes which also supports launching private dask workers. This is very useful because the Jupyter Notebook users can launch a cluster of worker containers inside Kubernetes and process larger amounts of data than they could using only their notebook container."
  },
  {
    "objectID": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#setup-kubernetes-on-jetstream-with-kubespray",
    "href": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#setup-kubernetes-on-jetstream-with-kubespray",
    "title": "Deploy Pangeo on Kubernetes deployment on Jetstream created with Kubespray",
    "section": "Setup Kubernetes on Jetstream with Kubespray",
    "text": "Setup Kubernetes on Jetstream with Kubespray\nFirst check out my tutorial on deploying Kubernetes on Jetstream with Kubespray. You just need to complete the first part, do not install JupyterHub, it is installed as part of the Pangeo deployment.\nI also recommend to setup kubectl and helm to run locally so that the following steps can be executed on the local machine, see the instructions at the bottom of the tutorial mentioned above. otherwise you need to ssh into the master node and type helm commands there."
  },
  {
    "objectID": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#install-pangeo-with-helm",
    "href": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#install-pangeo-with-helm",
    "title": "Deploy Pangeo on Kubernetes deployment on Jetstream created with Kubespray",
    "section": "Install Pangeo with Helm",
    "text": "Install Pangeo with Helm\nPangeo publishes a Helm chart (a software package for Kubernetes) and we can leverage that to setup the deployment.\nFirst add the repository:\nhelm repo add pangeo https://pangeo-data.github.io/helm-chart/\nhelm repo update\nThen download my repository with all the configuration files and helper scripts:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\nCreate a secrets.yaml file running:\nbash create_secrets.sh\nThen head to the pangeo_helm folder and customize config_jupyterhub_pangeo_helm.yaml,\n\nI have prepopulated very small limits for testing, increase those for production\nI am using the docker image zonca/pangeo_notebook_rsignell, you can remove image: and the 2 lines below to use the standard Pangeo notebook image (defined in their values.yaml)\nCopy cookieSecret and secretToken from secrets.yaml you created above\nCustomize ingress - hosts with the hostname of your master instance\n\nFinally you can deploy it running:\nbash install_pangeo.sh\nLogin by pointing your browser at http://js-XXX-YYY.jetstream-cloud.org, the default dummy authenticator only needs a username and empty password."
  },
  {
    "objectID": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#customize-and-launch-dask-workers",
    "href": "posts/2018-12-20-jetstream_kubernetes_jupyterhub_pangeo.html#customize-and-launch-dask-workers",
    "title": "Deploy Pangeo on Kubernetes deployment on Jetstream created with Kubespray",
    "section": "Customize and launch dask workers",
    "text": "Customize and launch dask workers\nOnce you login to the Jupyter Notebook, you can customize the worker-template.yaml file available in your home folder, I have an example of it with very small limits in the pangeo_helm folder.\nThis file is used by dask_kubernetes to launch workers on your behalf, see for example the dask-array.ipynb notebook available in your home folder:\nfrom dask_kubernetes import KubeCluster\ncluster = KubeCluster(n_workers=3)\ncluster\nThis will launch 3 workers on the cluster which are then available to launch jobs on with dask.\nYou can check with kubectl that the workers are executing:\n$ kubectl get pods -n pangeo\nNAME                         READY   STATUS    RESTARTS   AGE\ndask-zonca-d191b7a4-d8jhft   1/1     Running   0          28m\ndask-zonca-d191b7a4-dx9dhs   1/1     Running   0          28m\ndask-zonca-d191b7a4-dzmgvv   1/1     Running   0          28m\nhub-55f5bf597-f5bnt          1/1     Running   0          55m\njupyter-zonca                1/1     Running   0          38m\nproxy-66576956d7-r926j       1/1     Running   0          55m\nAnd also access the Dask GUI, using the menu on the left or the link provided by dask_kubernetes inside the Notebook.\n\n\n\nScreenshot of the Dask UI"
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "",
    "text": "I described how to deploy JupyterHub with each user session running on a different node of a Supercomputer in my paper for PEARC18, however things are moving fast in the space and I am employing a different strategy this year, in particular relying on the littlest JupyterHub project for the initial deployment."
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#initial-deployment-of-jupyterhub",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#initial-deployment-of-jupyterhub",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Initial deployment of JupyterHub",
    "text": "Initial deployment of JupyterHub\nThe littlest JupyterHub project has great documentation on how to deploy JupyterHub working on a single server on a wide array of providers.\nIn my case I logged in to the dashboard of SDSC Cloud, a OpenStack deployment at the San Diego Supercomputer Center, and requested an instance with 16 GB of RAM and 6 vCPUs with Ubuntu 18.04. Make sure you attach a floating public IP to the instance and open up ports 22 for SSH and 80,443 for HTTP/HTTPS.\nThen I followed the installation tutorial for custom servers, just make sure that you first create in the virtual machine the admin user you specify in the installation script, also make sure to use the same username of your Github account, as we will later setup Github Authentication.\nYou can connect to the instance and check JupyterHub is working and you can login with your user and access the admin panel, for SDSC Cloud the address is http://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu, filled in with the instance floating IP address.\n\nSetup HTTPS\nFollow the Littlest JupyterHub documentation on how to get a SSL certificate through Letsencrypt automatically, after this you should be able to access JupyterHub from https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu or a custom domain you pointed there."
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#authentication-with-github",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#authentication-with-github",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Authentication with Github",
    "text": "Authentication with Github\nFollow the Littlest JupyterHub documentation, just make sure to set the http address and not the https address."
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#interface-with-comet-via-batchspawner",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#interface-with-comet-via-batchspawner",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Interface with Comet via batchspawner",
    "text": "Interface with Comet via batchspawner\nWe want all users to run on Comet as a single “Gateway” user, as JupyterHub executes as the root user on the server, we want to create a SSH key for the root user and copy the public key to the home folder of the gateway user on Comet so that we can SSH without a password.\nInstead, if you would like each user to utilize their own XSEDE account, you need them to authenticate via XSEDE and get a certificate from the XSEDE API that can be used to login to Comet on behalf of the user, see an example deployment of this.\nFirst install batchspawner with pip in the Python environment of the hub, this is different than the Python environment of the user, you can have access to it logging in with the root user and running:\nexport PATH=/opt/tljh/hub/bin:${PATH}\nSet the configuration file, see spawner.py on this Gist and copy it into the /opt/tljh/config/jupyterhub_config.d folder, then add the private SSH key of the tunnelbot user, which is a user on the Virtual Machine with no shell (set /bin/false in /etc/passwd) but that can setup a SSH tunnel from Comet back to the Hub.\nAlso customize all paths and usernames in the file.\nReload the Jupyterhub configuration with:\ntljh-config reload\nYou can then check the Hub logs with sudo journalctl -r -u jupyterhub\nThe most complicated part is making sure that the environment variables defined by JupyterHub, the most important is the token which allows the singleuser server to authenticate itself with the Hub, are correctly propagated through SSH. See in spawner.py how I explicitely pass the variables over SSH.\nAlso, as all workshop participants access Comet with the same user account, I automatically create a folder with their Github username and checkout the Notebooks for the workshop in that folder. Then start JupyterLab in that folder, so that the users do not interfere, we are not worrying about security here, with the current setup a user can open a terminal inside JupyterLab and access the folder of another person."
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#how-to-setup-the-tunnelbot-user",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#how-to-setup-the-tunnelbot-user",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "How to setup the tunnelbot user",
    "text": "How to setup the tunnelbot user\n\nOn the JupyterHub virtual machine, create a user named tunnelbot\nsudo su tunnelbot to act as that user, then create a key with ssh-keygen\nenter the .ssh folder and cp id_rsa.pub authorized_keys so that the ssh key can be used from Comet to ssh passwordless to the server\nnow get the private key from /home/tunnelbot/.ssh/id_rsa and paste it into spawner.py\nnow make sure you set the shell of tunnelbot to /bin/false in /etc/passwd/\nfor increased security, please also follow the steps in this stackoverflow answer"
  },
  {
    "objectID": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#acknowledgments",
    "href": "posts/2018-11-07-deploy_jupyterhub_supercomputer_2018.html#acknowledgments",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks to the Jupyter and JupyterHub teams for releasing great software with outstanding documentation, in particular Yuvi Panda for the simplicity and elegance in the design of the Littlest JupyterHub deployment."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html",
    "href": "posts/2018-10-24-compute-at-sdsc.html",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "",
    "text": "I am often asked what computing resources are available at the San Diego Supercomputer Center for scientists and what is the best way to be granted access. I decided to write a blog post with an overview of all the options, consider that I’m writing this in October 2018, so please cross-check on the official websites."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html#comet",
    "href": "posts/2018-10-24-compute-at-sdsc.html#comet",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "Comet",
    "text": "Comet\nOur key resource is the Comet Supercomputer, a 2000 nodes traditional supercomputer with 72 GPU nodes, each with 4 GPUs. Comet has powerful CPUs with 24 cores and lots of memory per node (128GB) and a very fast local flash drive on each node. It is also suitable to run large amounts of single node jobs, so you can exploit it even if you don’t have a multi-node parallel software.\nComet is a XSEDE resource, XSEDE is basically a consortium of many large US supercomputers dedicated to Science, it reviews applications from US scientists and grants them supercomputing resources for free. It is funded by National Science Foundation.\n\nHow to request resources on Comet\nOrdered from the lowest to the largest amount of resources needed, which means they are ordered by the amount of effort it takes to get each type of allocation.\nThe amount of resources on Comet are billed in core hours (sometimes named SUs), if you request a Comet node for 1 hour you are charged 24 hours, comet GPUs are billed 14 core hours for each hour on each GPU. The newest Comet GPU nodes have P100 instead of K80, those are billed 1.5 times the older GPU nodes, i.e. 21 core hours per hour. Comet also has a shared queue where you can request and be charged for a portion of a Comet node (you also get the proportional amount of memory), i.e. you can request 6 cores and pay only 6 core hours per hour and get access to 32GB of RAM.\n\nTrial allocation\nAnybody can request a trial allocation on Comet with a quick 1 paragraph justification and be approved within a day for 1000 core hours to be used within 6 month. This is useful to try Comet out, run some test jobs. See the trial allocation page on the XSEDE website.\n\n\nCampus champions\nMost US universities have a reference person that facilitates access to XSEDE supercomputers, it is often somebody in the Information Technology office or in a Chancellor of Research or a professor. This person is given a large amount of supercomputing hours on all XSEDE resources and local professors, postdocs and graduate students can request to be added to this allocation and use many thousands of core hours, depending on availability. Campus champions are currently available in 241 (!!) US institutions, see the list on the XSEDE website.\n\n\nHPC@UC\nIf you are at any of the University of California campuses, you have an expedited way of getting resources at SDSC. You can submit a request for up to 1 million core hours (more often ~500K core hours) on Comet on the HPC@UC page. It just requires a 3 page justification and is answered within 10 business days. You are not eligible if your research group has an active XSEDE allocation.\n\n\nStartup allocation\nStartup allocations are really quick to prepare, they just require a 1 page justification and CV of the Principal Investigator and grant up to 50K core hours on Comet, if your research is funded by NSF/NASA/NIH remember to specify that. See the startup page on XSEDE for more details.\nThey are reviewed continously so you should be approved within a few days. Generally you are supposed to utilize the amount of hours within 1 year, but if your science project is funded for a longer period, you can request a multi-year allocation.\n\n\nXRAC allocation\nXRAC allocations are full fledged proposals, you can request up to a few million hours on Comet, here you must provide a detailed justification of the resources requested, demonstrate that your software is able to efficiently scale up in parallel, i.e. if in production you want to run on 100 nodes, you should run it on 5/10/50/100 nodes and check that performance does not degrade too much with increased parallelism. You should have performed those tests in a startup allocation. The XRAC requests are reviewed quarterly, see the Research allocations page, there is also a recorded webinar."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html#triton-shared-computing-cluster",
    "href": "posts/2018-10-24-compute-at-sdsc.html#triton-shared-computing-cluster",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "Triton Shared Computing Cluster",
    "text": "Triton Shared Computing Cluster\nThe Triton Shared Computing Cluster is a supercomputer at SDSC with specifications a bit lower than Comet and that is not allocated through XSEDE, resources are paid by the users. XSEDE resources are always oversubscribed and often only a portion of the resources requested is granted, scientific groups that do not get enough resources through XSEDE can complement it with an allocation on TSCC.\nThe easiest way to get computational hours on TSCC is a pay-as-you-go option where you buy an amount of core-hours at $6c / core-hour (academics have a lower rate based on affiliation).\nBut the most cost-effective way is to buy a node to be added to the cluster for 3 years with full hardware warranty plus 1 extra year with no warranty, so if it breaks it needs to be removed. You pay a fixed price to buy the node (~$6K) plus yearly operations (~\\(1.8K if not subsidized by your University, in UC campuses this is generally subsidized and is ~\\).5K), see the updated costs on the TSCC page, also get in touch with them directly for more details. You can also buy a node with GPUs.\nThen, instead of having direct access to that node, you are given an allocation as big as the computing hours that your node provides to the cluster. This is great because it allows you to not be penalized for incosistent usage patterns. You can pay for 1 node and then use tens of nodes together once in a while. If you have the yearly operations subsidized by campus, the cost per core hour is about $2c, which is quite competitive, and the cluster is in SDSC machine room and professionally managed, updated, backed up."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html#colocation",
    "href": "posts/2018-10-24-compute-at-sdsc.html#colocation",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "Colocation",
    "text": "Colocation\nLarger collaborations might need dedicated resources, it is possible to buy your own nodes, in units of entire racks (48 Rack Units), which depending on the type of blades can be 12 or 24 nodes and colocate it in SDSC’s machine room. See the detailed cost on the colocation page, this is a custom solution and it is not easy to give a simple cost estimate, better write and ask for a quote."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html#cloud-resources-virtual-machines",
    "href": "posts/2018-10-24-compute-at-sdsc.html#cloud-resources-virtual-machines",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "Cloud resources (Virtual Machines)",
    "text": "Cloud resources (Virtual Machines)\nSDSC also manages a OpenStack deployment, which is especially suitable for running services, for example websites, databases, APIs but it is also suitable to run long-running single node jobs or interactive data analysis (think Jupyter Notebooks). And Kubernetes, of course! (see my tutorial for Jetstream, which works also on SDSC Cloud. This is also equivalent to Amazon Elastic Cloud Compute (EC2), here you pay for what you use, within UC you provide a funding index and that is charged for each hour used, see the full pricing on the SDSC cloud page, roughly you are charged $8c an hour for a Virtual Machine with 1 core and 4GB of RAM."
  },
  {
    "objectID": "posts/2018-10-24-compute-at-sdsc.html#feedback",
    "href": "posts/2018-10-24-compute-at-sdsc.html#feedback",
    "title": "Bring your computing to the San Diego Supercomputer Center",
    "section": "Feedback",
    "text": "Feedback\nIf you have questions please email me at zonca on the sdsc.edu domain or tweet @andreazonca."
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html",
    "title": "Explore a Kubernetes deployment on Jetstream with Kubespray 2/3",
    "section": "",
    "text": "This is the second part of the tutorial on deploying Kubernetes with kubespray and JupyterHub on Jetstream.\nIn the first part, we installed Kubernetes on Jetstream with kubespray.\nIt is optional, its main purpose is to familiarize with the Kubernetes deployment on Jetstream and how the different components play together before installing JupyterHub. If you are already familiar with Kubernetes you can skip to next part where we will be installing Jupyterhub using the zerotojupyterhub helm recipe.\nAll the files for the examples below are available on Github, first SSH to the master node (or do this locally if you setup kubectl locally):"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html#test-persistent-storage-with-cinder",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html#test-persistent-storage-with-cinder",
    "title": "Explore a Kubernetes deployment on Jetstream with Kubespray 2/3",
    "section": "Test persistent storage with cinder",
    "text": "Test persistent storage with cinder\nThe most important feature that brought me to choose kubespray as method for installing Kubernetes is that it automatically sets up persistent storage exploiting Jetstream Volumes. The Jetstream team already does a great job in providing a persistent storage solution with adequate redundancy via the Cinder project, part of OpenStack.\nkubespray sets up a Kubernetes provisioner so that when a container requests persistent storage, it talks to the Openstack API and have a dedicated volume (the same type you can create with the Jetstream Horizon Web interfaces) automatically created and exposed to Kubernetes.\nThis is achieved through a storageclass:\nkubectl get storageclass\nNAME                 PROVISIONER            AGE\nstandard (default)   kubernetes.io/cinder   1h\nSee the file alpine-persistent-volume.yaml in the repository on how we can request a Cinder volume to be created and attached to a pod.\nkubectl create -f alpine-persistent-volume.yaml\nWe can test it by getting a terminal inside the container (alpine has no bash):\nkubectl exec -it alpine -- /bin/sh\nlook into df -h, check that there is a 5GB mounted file system which is persistent.\nAlso, back to the machine with openstack access, see how an Openstack volume was dynamically created and attached to the running instance:\nopenstack volume list\nopenstack volume list\n+--------------------------------------+-------------------------------------------------------------+--------+------+--------------------------------------------------+\n| ID                                   | Name                                                        | Status | Size | Attached to                                      |\n+--------------------------------------+-------------------------------------------------------------+--------+------+--------------------------------------------------+\n| 508f1ee7-9654-4c84-b1fc-76dd8751cd6e | kubernetes-dynamic-pvc-e83ec4d6-bb9f-11e8-8344-fa163eb22e63 | in-use |    5 | Attached to kubespray-k8s-node-nf-1 on /dev/sdb  |\n+--------------------------------------+-------------------------------------------------------------+--------+------+--------------------------------------------------+"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html#test-replicasets-services-and-ingress",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray_explore.html#test-replicasets-services-and-ingress",
    "title": "Explore a Kubernetes deployment on Jetstream with Kubespray 2/3",
    "section": "Test ReplicaSets, Services and Ingress",
    "text": "Test ReplicaSets, Services and Ingress\nIn this section we will explore how to build redundancy and scale in a service with a simple example included in the book Kubernetes in Action, which by the way I highly recommend to get started with Kubernetes.\nFirst let’s deploy a service in our Kubernetes cluster, this service just answers to HTTP requests on port 8080 with the message “You’ve hit kubia-manual”:\ncd kubia_test_ingress\nkubectl create -f kubia-manual.yaml\nWe can test it by checking at which IP Kubernetes created the pod:\nkubectl get pods -o wide\nand assign it to the KUBIA_MANUAL_IP variable, then on one of the nodes:\n$ curl $KUBIA_MANUAL_IP:8080\nYou've hit kubia-manual\nFinally close it:\nkubectl delete -f kubia-manual.yaml\n\nLoad balancing with ReplicaSets and Services\nNow we want to scale this service up and provide a set of 3 pods instead of just 1:\nkubectl create -f kubia-replicaset.yaml\nNow we could access those services on 3 different IP addresses, but we would like to have a single entry point and automatic load balancing across those instances, so we create a Kubernetes “Service” resource:\nkubectl create -f kubia-service.yaml\nAnd test it:\n$ kubectl get service\nNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.233.0.1      &lt;none&gt;        443/TCP   22h\nkubia        ClusterIP   10.233.28.205   &lt;none&gt;        80/TCP    45m\ncurl $KUBIA_SERVICE_IP\nThis is on port 80 so we don’t need :8080 in the URL. Run many times and check different kubia services answer.\n\n\nPublish service externally with ingress\nTry to open browser and access the hostname of your master node at:\nhttp://js-XXX-YYY.jetstream-cloud.org\nWhere XXX-YYY are the last 2 groups of digits of the floating IP of the master instance, i.e. AAA.BBB.XXX.YYY, each of them could also be 1 or 2 digits instead of 3.\nThe connection should respond with 404.\nAt this point, edit the kubia-ingress.yaml file and replace the host value with the master node domain name you just derived.\nNow:\nkubectl create -f kubia-ingress.yaml\nkubectl get ingress\nTry again in the browser. You should now see something like:\n“You’ve hit kubia-jqwwp”\nForce reload the browser page a few times and you will see you are hitting a different kubia service.\nFinally,\nkubectl delete -f kubia-ingress.yaml"
  },
  {
    "objectID": "posts/2018-07-23-pearc18_paper_deploy_jupyterhub_xsede.html",
    "href": "posts/2018-07-23-pearc18_paper_deploy_jupyterhub_xsede.html",
    "title": "PEARC18 Paper on Deploying Jupyterhub at scale on XSEDE",
    "section": "",
    "text": "Bob Sinkovits and I are presenting a paper at PEARC18 about:\n“Deploying Jupyter Notebooks at scale on XSEDE resources for Science Gateways and workshops”\nSee the pre-print on Arxiv: https://arxiv.org/abs/1805.04781\nJupyter Notebooks provide an interactive computing environment well suited for Science. JupyterHub is a multi-user Notebook environment developed by the Jupyter team.\nIn order to provide adequate amount of memory and CPU to many users for example during workshops, it is necessary to leverage a distributed system, either leveraging multiple Jetstream instances or interfacing with a traditional HPC system.\nIn this work we present 3 strategies for deploying JupyterHub on XSEDE resources to support a large number of users, each is linked to the step-by-step tutorial with all necessary configuration files:\n\ndeploy Jupyterhub on a single Jetstream instance and spawn Jupyter Notebook servers for each user on a computing node of a Supercomputer (for example Comet)\ndeploy Jupyterhub on Jetstream using Docker Swarm to distributed the user’s containers across many instances and providing persistent storage with quotas through a NFS share\ndeploy Jupyterhub on top of Kubernetes across Jetstream instances with persistent storage provided by the Ceph distributed filesystem\n\nPresentation slides\nIf are an author at PEARC18, you can follow my instructions on how to publish your preprint to Arxiv"
  },
  {
    "objectID": "posts/2018-07-19-docker-auto-build.html",
    "href": "posts/2018-07-19-docker-auto-build.html",
    "title": "Create DockerHub auto build",
    "section": "",
    "text": "It is very convenient to create Autobuild repositories on DockerHub linked to a Github repository with a Dockerfile. Then every time you commit to Github, Dockerhub is going to build the image on their service and make it available on https://hub.docker.com and can quickly be pulled to any other system that supports Docker or Singularity.\nUnfortunately if you have many Github organizations and repositories, the process to set a new repository up gets stuck.\nFortunately we can bypass the issue by directly accessing the right URL, as suggested on StackOverflow.\nI created a simple page to make this quicker, add the right parameters and it automatically builds the right URL, see:\nhttps://zonca.github.io/docker-auto-build"
  },
  {
    "objectID": "posts/2018-06-07-private-dask-kubernetes-jetstream.html",
    "href": "posts/2018-06-07-private-dask-kubernetes-jetstream.html",
    "title": "Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream",
    "section": "",
    "text": "In this post we will leverage software made available by the Pangeo community to allow each user of a Jupyterhub instance deployed on Jetstream on top of Kubernetes to launch a set of dask workers as containers running inside Kubernetes itself and use them for distributed computing.\nPangeo also maintains a deployment of this environment on Google Cloud freely accessible at pangeo.pydata.org.\nSecurity considerations: This deployment grants each user administrative access to the Kubernetes API, so each user could use this privilege to terminate other users’ pods or dask workers. Therefore it is suitable only for a community of trusted users. There is discussion about leveraging namespaces to limit this but it hasn’t been implemented yet."
  },
  {
    "objectID": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#deploy-kubernetes",
    "href": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#deploy-kubernetes",
    "title": "Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Deploy Kubernetes",
    "text": "Deploy Kubernetes\nWe need to first create Jetstream instances and deploy Kubernetes on them. We can follow the first part of the tutorial at https://zonca.github.io/2017/12/scalable-jupyterhub-kubernetes-jetstream.html. I also tested with Ubuntu 18.04 instead of Ubuntu 16.04 and edited the install-kubeadm.bash accordingly, I also removed version specifications in order to pickup the latest Kubernetes version, currently 1.10. See my install-kubeadm-18.04.bash. Notice that for the http://apt.kubernetes.io/ don’t have yet Ubuntu 18.04 packages, so I left xenial, this should be updated in the future.\nIn order to simplify the setup we will just be using ephemeral storage, later we can update the deployment using either Rook following the steps in my original tutorial or a NFS share (I’ll write a tutorial soon about that)."
  },
  {
    "objectID": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#deploy-pangeo",
    "href": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#deploy-pangeo",
    "title": "Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Deploy Pangeo",
    "text": "Deploy Pangeo\nDeployment is just a single step because Pangeo published a Helm recipe that depends on the Zero-to-JupyterHub recipe and deploys both in a single step, therefore we should not have deployed JupyterHub beforehand.\nFirst we need to create a yaml configuration file for the package. Checkout the Github repository with all the configuration files on the master node of Kubernetes:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\nin the pangeo_helm folder, there is already a draft of the configuration file.\nWe need to:\n\nrun openssl as instructed inside the file and paste the output tokens to the specified location\nedit the hostname in the ingress section to the hostname of the Jetstream master node\ncustomize the memory and CPU requirements, currently they are very low so that this can be tested also in a single small instance\n\nWe can then deploy with:\nsudo helm install pangeo/pangeo -n pangeo --namespace pangeo -f config_pangeo_no_storage.yaml --version=v0.1.1-95ab292\nYou can optionally check if there are newer versions of the chart at https://pangeo-data.github.io/helm-chart/.\nThen check that the pods start checking their status with:\nsudo kubectl -n pangeo get pods\nIf any is stuck in Pending, check with:\nsudo kubectl -n pangeo describe &lt;pod-name&gt;\nOnce the hub pod is running, you should be able to connect with your browser to js-xxx-xxx.Jetstream-cloud.org, by default it runs with a dummy authenticator, at the login form, just type any username and leave the password empty to login."
  },
  {
    "objectID": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#launch-a-dask-cluster",
    "href": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#launch-a-dask-cluster",
    "title": "Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Launch a dask cluster",
    "text": "Launch a dask cluster\nOnce you get the Jupyter Notebook instance, you should see a file named worker-template.yaml in your home folder, this is a template for the configuration and the allocated resources for the pod of each dask worker. The default workers for Pangeo are beefy, for testing we can reduce their requirements, see for example my worker-template.yaml that works on a small Jetstream VM.\nThen inside examples/ we have several example notebooks that show how to use dask for distributed computing. dask-array.ipynb shows basic functionality for distributed multi-dimensional arrays.\nThe most important piece of code is the creation of dask workers:\nfrom dask_kubernetes import KubeCluster\ncluster = KubeCluster(n_workers=2)\ncluster\nIf we execute this cell dask_kubernetes contacts the Kubernetes API using the serviceaccount daskkubernetes mounted on the pods by the Helm chart and requests new pods to be launched. In fact we can check on the terminal again with:\nsudo kubectl -n pangeo get pods\nthat new pods should be about to run. It also provides buttons to change the number of running workers, either manually or adaptively based on the required resources.\nThis also runs the dask scheduler on the pod that is running the Jupyter Notebook and we can connect to it with:\nfrom dask.distributed import Client\nclient = Client(cluster)\nclient\nFrom now on all dask commands will automatically execute commands on the dask cluster."
  },
  {
    "objectID": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#customize-the-jupyterhub-deployment",
    "href": "posts/2018-06-07-private-dask-kubernetes-jetstream.html#customize-the-jupyterhub-deployment",
    "title": "Setup private dask clusters in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Customize the JupyterHub deployment",
    "text": "Customize the JupyterHub deployment\nWe can then customize the JupyterHub deployment for example to add authentication or permanent storage. Notice that all configuration options inside the config_pangeo_no_storage.yaml are inside the jupyterhub: tag, this is due to the fact that jupyterhub is another Helm package which we are configuring through the pangeo Helm package. Therefore make sure that any configuration option found in my previous tutorials or on the Zero-to-Jupyterhub documentation needs to be indented accordingly.\nThen we can either run:\nsudo helm delete --purge pangeo\nand then install it from scratch again or just update the running cluster with:\nsudo helm upgrade pangeo -f config_pangeo_no_storage.yaml"
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "",
    "text": "Let’s assume we have already a Kubernetes deployment and have installed JupyterHub, see for example my previous tutorial on Jetstream. Now that users can login and access a Jupyter Notebook, we would also like to provide them more computing power for their interactive data exploration. The easiest way is through dask, we can launch a scheduler and any number of workers as containers inside Kubernetes so that users can leverage the computing power of many Jetstream instances at once.\nThere are 2 main strategies, we can give each user their own dask cluster with exclusive access and this would be more performant but cause quick spike of usage of the Kubernetes cluster, or just launch a shared cluster and give all users access to that.\nIn this tutorial we cover the second scenario, we’ll cover the first scenario in a following tutorial.\nWe will deploy first Jupyterhub through the Zero-to-JupyterHub guide, then launch via Helm a fixed size dask clusters and show how users can connect, submit distributed Python jobs and monitor their execution on the dashboard.\nThe configuration files mentioned in the tutorial are available in the Github repository zonca/jupyterhub-deploy-kubernetes-jetstream."
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#deploy-jupyterhub",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#deploy-jupyterhub",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Deploy JupyterHub",
    "text": "Deploy JupyterHub\nFirst we start from Jupyterhub on Jetstream with Kubernetes at https://zonca.github.io/2017/12/scalable-jupyterhub-kubernetes-jetstream.html\nOptionally, for testing purposes, we can simplify the deployment by skipping permanent storage, if this is an option, see the relevant section below.\nWe want to install Jupyterhub in the pangeo namespace with the name jupyter, replace the helm install line in the tutorial with:\nsudo helm install --name jupyter jupyterhub/jupyterhub -f config_jupyterhub_pangeo_helm.yaml --namespace pangeo\nThe pangeo configuration file is using a different single user image which has the right version of dask for this tutorial."
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#optional-simplify-deployment-using-ephemeral-storage",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#optional-simplify-deployment-using-ephemeral-storage",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "(Optional) Simplify deployment using ephemeral storage",
    "text": "(Optional) Simplify deployment using ephemeral storage\nInstead of installing and configuring rook, we can temporarily disable permanent storage to make the setup quicker and easier to maintain.\nIn the JupyterHub configuration yaml set:\nhub:\n   db:\n     type: sqlite-memory\n\nsingleuser:\n   storage:\n      type: none\nNow every time a user container is killed and restarted, all data are gone, this is good enough for testing purposes."
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#configure-github-authentication",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#configure-github-authentication",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Configure Github authentication",
    "text": "Configure Github authentication\nFollow the instructions on the Zero-to-Jupyterhub documentation, at the end you should have in the YAML:\nauth:\n  type: github\n  admin:\n    access: true\n    users: [zonca, otherusername]\n  github:\n    clientId: \"xxxxxxxxxxxxxxxxxxxx\"\n    clientSecret: \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n    callbackUrl: \"https://js-xxx-xxx.jetstream-cloud.org/hub/oauth_callback\""
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#test-jupyterhub",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#test-jupyterhub",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Test Jupyterhub",
    "text": "Test Jupyterhub\nConnect to the master node with your browser at: https://js-xxx-xxx.jetstream-cloud.org Login with your Github credentials, you should get a Jupyter Notebook.\nYou can also check that your pod is running:\nsudo kubectl get pods -n pangeo\nNAME                                  READY     STATUS    RESTARTS   AGE\njupyter-zonca                         1/1       Running   0          2m\n......other pods"
  },
  {
    "objectID": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#install-dask",
    "href": "posts/2018-05-04-shared-dask-kubernetes-jetstream.html#install-dask",
    "title": "Launch a shared dask cluster in Kubernetes alongside JupyterHub on Jetstream",
    "section": "Install Dask",
    "text": "Install Dask\nWe want to deploy a single dask cluster that all the users can submit jobs to.\nCustomize the dask_shared/dask_config.yaml file available in the repository, for testing purposes I set just 1 GB RAM and 1 CPU limits on each of 3 workers. We can change replicas of the workers to add more.\nsudo helm install stable/dask --name=dask --namespace=pangeo -f dask_config.yaml\nThen check that the dask instances are running:\n$ sudo kubectl get pods --namespace pangeo\nNAME                              READY     STATUS    RESTARTS   AGE\ndask-jupyter-647bdc8c6d-mqhr4     1/1       Running   0          22m\ndask-scheduler-5d98cbf54c-4rtdr   1/1       Running   0          22m\ndask-worker-6457975f74-dqhsh      1/1       Running   0          22m\ndask-worker-6457975f74-lpvk4      1/1       Running   0          22m\ndask-worker-6457975f74-xzcmc      1/1       Running   0          22m\nhub-7f75b59fc5-8c2pg              1/1       Running   0          6d\njupyter-zonca                     1/1       Running   0          10m\nproxy-6bbf67f6bd-swt7f            2/2       Running   0          6d\n\nAccess the scheduler and launch a distributed job\nkube-dns gives a name to each service and automatically propagates it to each pod, so we can connect by name\nfrom dask.distributed import Client\nclient = Client(\"dask-scheduler:8786\")\nclient\nNow we can access the 3 workers that we launched before:\nClient\nScheduler: tcp://dask-scheduler:8786\nDashboard: http://dask-scheduler:8787/status\nCluster\nWorkers: 3\nCores: 6\nMemory: 12.43 GB\nWe can run an example computation with dask array:\nimport dask.array as da\nx = da.random.random((20000, 20000), chunks=(2000, 2000)).persist()\nx.sum().compute()\n\n\nAccess the Dask dashboard for monitoring job execution\nWe need to setup ingress so that a path points to the Dask dashboard instead of Jupyterhub,\nCheckout the file dask_shared/dask_webui_ingress.yaml in the repository, it routes the path /dask to the dask-scheduler service.\nCreate the ingress resource with:\nsudo kubectl create ingress -n pangeo -f dask_webui_ingress.yaml\nAll users can now access the dashboard at:\n\nhttps://js-xxx-xxx.jetstream-cloud.org/dask/status\n\nMake sure to use /dask/status/ and not only /dask. Currently this is not authenticated, so this address is publicly available. A simple way to hide it is to choose a custom name instead of /dask and edit the ingress accordingly with:\nsudo kubectl edit ingress dask -n pangeo"
  },
  {
    "objectID": "posts/2018-03-03-zarr.html",
    "href": "posts/2018-03-03-zarr.html",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "",
    "text": "Zarr is a pretty new file format designed for cloud computing, see documentation and a webinar for more details.\nZarr is also supported by dask, the parallel computing framework for Dask, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2018-03-03-zarr.html#zarr",
    "href": "posts/2018-03-03-zarr.html#zarr",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "",
    "text": "Zarr is a pretty new file format designed for cloud computing, see documentation and a webinar for more details.\nZarr is also supported by dask, the parallel computing framework for Dask, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2018-03-03-zarr.html#use-openstack-swift-on-jetstream-for-object-storage",
    "href": "posts/2018-03-03-zarr.html#use-openstack-swift-on-jetstream-for-object-storage",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Use OpenStack swift on Jetstream for object storage",
    "text": "Use OpenStack swift on Jetstream for object storage\nJetstream also offers (currently in beta) access to object storage via OpenStack Swift. This is a separate service from the Jetstream Virtual Machines, so you do not need to spin any Virtual Machine dedicated to storing the data but just use the object storage already provided by Jetstream."
  },
  {
    "objectID": "posts/2018-03-03-zarr.html#read-zarr-files-from-object-store",
    "href": "posts/2018-03-03-zarr.html#read-zarr-files-from-object-store",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Read Zarr files from object store",
    "text": "Read Zarr files from object store\nIf somebody else has already made available some files on object store and set their visibility to “public”, anybody can read them.\nSee this notebook\nNeed openstack RC file version 3 from: https://iu.jetstream-cloud.org/project/api_access/\npip install python-openstackclient\nsource the openstackRC file, put the password, this is the TACC password, NOT the XSEDE Password. I know.\nnow create ec2 credentials with:\nopenstack ec2 credentials create -f json &gt; ec2.json\ntest if we can access this.\nI installed this on js-169-169\nactually we can skip ec2 credentials and just use openstack:\nopenstack object list zarr_pangeo\nsave credentials in ~/.aws/config\n[default]\nregion=RegionOne\naws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\naws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nimport s3fs\nfs = s3fs.S3FileSystem(client_kwargs=dict(endpoint_url=\"https://iu.jetstream-cloud.org:8080\"))\nfs.ls(\"zarr_pangeo\")\nZarr with dask on 1 node works fine\nhttps://gist.github.com/zonca/071bbd8cbb9d15b1789865acb9e66de8\nNeed to test: * access from multiple nodes with distributed * test read-only access without authentication"
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "",
    "text": "I described how to deploy JupyterHub with each user session running on a different node of a Supercomputer in my paper for PEARC18, however things are moving fast in the space and I am employing a different strategy this year, in particular relying on the littlest JupyterHub project for the initial deployment."
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#initial-deployment-of-jupyterhub",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#initial-deployment-of-jupyterhub",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Initial deployment of JupyterHub",
    "text": "Initial deployment of JupyterHub\nThe littlest JupyterHub project has great documentation on how to deploy JupyterHub working on a single server on a wide array of providers.\nIn my case I logged in to the dashboard of SDSC Cloud, a OpenStack deployment at the San Diego Supercomputer Center, and requested an instance with 16 GB of RAM and 6 vCPUs with Ubuntu 18.04. Make sure you attach a floating public IP to the instance and open up ports 22 for SSH and 80,443 for HTTP/HTTPS.\nThen I followed the installation tutorial for custom servers, just make sure that you first create in the virtual machine the admin user you specify in the installation script, also make sure to use the same username of your Github account, as we will later setup Github Authentication.\nYou can connect to the instance and check JupyterHub is working and you can login with your user and access the admin panel, for SDSC Cloud the address is http://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu, filled in with the instance floating IP address.\n\nSetup HTTPS\nFollow the Littlest JupyterHub documentation on how to get a SSL certificate through Letsencrypt automatically, after this you should be able to access JupyterHub from https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu or a custom domain you pointed there."
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#authentication-with-github",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#authentication-with-github",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Authentication with Github",
    "text": "Authentication with Github\nFollow the Littlest JupyterHub documentation, just make sure to set the http address and not the https address."
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#interface-with-comet-via-batchspawner",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#interface-with-comet-via-batchspawner",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Interface with Comet via batchspawner",
    "text": "Interface with Comet via batchspawner\nWe want all users to run on Comet as a single “Gateway” user, as JupyterHub executes as the root user on the server, we want to create a SSH key for the root user and copy the public key to the home folder of the gateway user on Comet so that we can SSH without a password.\nInstead, if you would like each user to utilize their own XSEDE account, you need them to authenticate via XSEDE and get a certificate from the XSEDE API that can be used to login to Comet on behalf of the user, see an example deployment of this.\nFirst install batchspawner with pip in the Python environment of the hub, this is different than the Python environment of the user, you can have access to it logging in with the root user and running:\nexport PATH=/opt/tljh/hub/bin:${PATH}\nSet the configuration file, see spawner.py on this Gist and copy it into the /opt/tljh/config/jupyterhub_config.d folder, then add the private SSH key of the tunnelbot user, which is a user on the Virtual Machine with no shell (set /bin/false in /etc/passwd) but that can setup a SSH tunnel from Comet back to the Hub.\nAlso customize all paths and usernames in the file.\nReload the Jupyterhub configuration with:\ntljh-config reload\nYou can then check the Hub logs with sudo journalctl -r -u jupyterhub\nThe most complicated part is making sure that the environment variables defined by JupyterHub, the most important is the token which allows the singleuser server to authenticate itself with the Hub, are correctly propagated through SSH. See in spawner.py how I explicitely pass the variables over SSH.\nAlso, as all workshop participants access Comet with the same user account, I automatically create a folder with their Github username and checkout the Notebooks for the workshop in that folder. Then start JupyterLab in that folder, so that the users do not interfere, we are not worrying about security here, with the current setup a user can open a terminal inside JupyterLab and access the folder of another person."
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#how-to-setup-the-tunnelbot-user",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#how-to-setup-the-tunnelbot-user",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "How to setup the tunnelbot user",
    "text": "How to setup the tunnelbot user\n\nOn the JupyterHub virtual machine, create a user named tunnelbot\nsudo su tunnelbot to act as that user, then create a key with ssh-keygen\nenter the .ssh folder and cp id_rsa.pub authorized_keys so that the ssh key can be used from Comet to ssh passwordless to the server\nnow get the private key from /home/tunnelbot/.ssh/id_rsa and paste it into spawner.py\nnow make sure you set the shell of tunnelbot to /bin/false in /etc/passwd/\nfor increased security, please also follow the steps in this stackoverflow answer"
  },
  {
    "objectID": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#acknowledgments",
    "href": "posts/2018-01-01-deploy_jupyterhub_supercomputer_2018.html#acknowledgments",
    "title": "Deploy JupyterHub on a Supercomputer for a workshop or tutorial 2018 edition",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks to the Jupyter and JupyterHub teams for releasing great software with outstanding documentation, in particular Yuvi Panda for the simplicity and elegance in the design of the Littlest JupyterHub deployment."
  },
  {
    "objectID": "posts/2017-12-19-ecss-symposium.html",
    "href": "posts/2017-12-19-ecss-symposium.html",
    "title": "ECSS Symposium about Jupyterhub deployments on XSEDE",
    "section": "",
    "text": "ECSS Symposium, 19 December 2017, Web presentation to the XSEDE Extended Collaborative Support Services.\nOverview on deployment options for Jupyter Notebooks at scale on XSEDE resources."
  },
  {
    "objectID": "posts/2017-12-19-ecss-symposium.html#jupyter-notebooks-at-scale-for-gateways-and-workshops",
    "href": "posts/2017-12-19-ecss-symposium.html#jupyter-notebooks-at-scale-for-gateways-and-workshops",
    "title": "ECSS Symposium about Jupyterhub deployments on XSEDE",
    "section": "",
    "text": "ECSS Symposium, 19 December 2017, Web presentation to the XSEDE Extended Collaborative Support Services.\nOverview on deployment options for Jupyter Notebooks at scale on XSEDE resources."
  },
  {
    "objectID": "posts/2017-12-19-ecss-symposium.html#presentation",
    "href": "posts/2017-12-19-ecss-symposium.html#presentation",
    "title": "ECSS Symposium about Jupyterhub deployments on XSEDE",
    "section": "Presentation",
    "text": "Presentation\n\nGoogle doc slides\nRecording of the talk on Youtube"
  },
  {
    "objectID": "posts/2017-12-19-ecss-symposium.html#tutorials",
    "href": "posts/2017-12-19-ecss-symposium.html#tutorials",
    "title": "ECSS Symposium about Jupyterhub deployments on XSEDE",
    "section": "Tutorials",
    "text": "Tutorials\nStep-by-step tutorials and configuration files to deploy JupyterHub on XSEDE resources:\n\nspawn Notebooks on a traditional HPC system\nsetup a distributed scalable system on Jetstream instances via Docker Swarm\nsetup a distributed scalable system on Jetstream instances via Kubernetes"
  },
  {
    "objectID": "posts/2017-12-19-ecss-symposium.html#publication",
    "href": "posts/2017-12-19-ecss-symposium.html#publication",
    "title": "ECSS Symposium about Jupyterhub deployments on XSEDE",
    "section": "Publication",
    "text": "Publication\nPaper in preparation: “Deploying Jupyter Notebooks at scale on XSEDE for Science Gateways and workshops”, Andrea Zonca and Robert Sinkovits, PEARC18"
  },
  {
    "objectID": "posts/2017-12-04-store-conda-environment-inside-notebook.html",
    "href": "posts/2017-12-04-store-conda-environment-inside-notebook.html",
    "title": "Store a conda environment inside a Notebook",
    "section": "",
    "text": "Last August, during the Container Analysis Environments Workshop held at Urbana-Champaign, we had discussion about reproducibility in the Jupyter Notebooks. There came out the idea of storing all the details about the Python environment inside the Notebook, in the metadata.\nI released an experimental package on Github (and PyPI):\nhttps://github.com/zonca/nbenv\nFor simplicity it only supports conda environment, but it also supports having pip-installed packages inside those environments.\nIt automatically saves the conda environment as metadata inside the .ipynb document and then provides a command line tool to inspect it and create a new conda environment based on it.\nI am not sure this is the best design, please open Issues on Github to send me feedback!"
  },
  {
    "objectID": "posts/2017-10-25-cudak8s.html",
    "href": "posts/2017-10-25-cudak8s.html",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "",
    "text": "Start from the CUDA 8 image from NVIDIA:\nFROM nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\nSetup the environment:\nRUN echo \"/usr/local/cuda-8.0/lib64/\" &gt;/etc/ld.so.conf.d/cuda.conf\n\n# For CUDA profiling, TensorFlow requires CUPTI.\nRUN echo \"/usr/local/cuda/extras/CUPTI/lib64/\" &gt;&gt;/etc/ld.so.conf.d/cuda.conf\n\n# make sure we have a way to bind host provided libraries\n# see https://github.com/singularityware/singularity/issues/611\nRUN mkdir -p /host-libs && \\\n    echo \"/host-libs/\" &gt;/etc/ld.so.conf.d/000-host-libs.conf\nThen build the container"
  },
  {
    "objectID": "posts/2017-10-25-cudak8s.html#setup-docker-image",
    "href": "posts/2017-10-25-cudak8s.html#setup-docker-image",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "",
    "text": "Start from the CUDA 8 image from NVIDIA:\nFROM nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\nSetup the environment:\nRUN echo \"/usr/local/cuda-8.0/lib64/\" &gt;/etc/ld.so.conf.d/cuda.conf\n\n# For CUDA profiling, TensorFlow requires CUPTI.\nRUN echo \"/usr/local/cuda/extras/CUPTI/lib64/\" &gt;&gt;/etc/ld.so.conf.d/cuda.conf\n\n# make sure we have a way to bind host provided libraries\n# see https://github.com/singularityware/singularity/issues/611\nRUN mkdir -p /host-libs && \\\n    echo \"/host-libs/\" &gt;/etc/ld.so.conf.d/000-host-libs.conf\nThen build the container"
  },
  {
    "objectID": "posts/2017-10-25-cudak8s.html#launch-a-test-pod",
    "href": "posts/2017-10-25-cudak8s.html#launch-a-test-pod",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Launch a test pod",
    "text": "Launch a test pod\napiVersion: v1\nkind: Pod\nmetadata:\n  name: testcuda\n  namespace: jup\nspec:\n  restartPolicy: Never\n  containers:\n    - image: your_image\n      name: ml\n      resources:\n        limits:\n          alpha.kubernetes.io/nvidia-gpu: 1 # requesting 1 GPU\n      args: [ \"nvidia-smi\" ]\n\n      volumeMounts:\n        - name: nvidia-driver\n          mountPath: /host-libs\n          readOnly: true\n  volumes:\n      - name: nvidia-driver\n        hostPath:\n          path: /var/lib/nvidia-docker/volumes/nvidia_driver/384.90/lib64"
  },
  {
    "objectID": "posts/2017-10-25-cudak8s.html#configure-jupyterhub",
    "href": "posts/2017-10-25-cudak8s.html#configure-jupyterhub",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Configure Jupyterhub",
    "text": "Configure Jupyterhub"
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "",
    "text": "It is good practice in software development to implement extensive testing of the codebase in order to catch quickly any bug introduced into the code when implementing new features.\nThe suite of tests should be easy to execute (possibly one single command, for example with the py.test runner) and quick to run (more than 1 minute would make it tedious to run).\nThe developers should run the unit test suite every time they implement a change to the codebase to make sure anything else has not been broken.\nHowever, once a commit has been pushed to Github, it is also useful to have automated testing executed automatically, at least for 2 reasons:\n\nRun tests in all the environments that need to be supported by the software, for example run with different versions of Python or different versions of a key required external dependancy\nRun tests in a clean environment that has less risks of being contaminated by some mis-configuration on one of the developers’ environments"
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#introduction",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#introduction",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "",
    "text": "It is good practice in software development to implement extensive testing of the codebase in order to catch quickly any bug introduced into the code when implementing new features.\nThe suite of tests should be easy to execute (possibly one single command, for example with the py.test runner) and quick to run (more than 1 minute would make it tedious to run).\nThe developers should run the unit test suite every time they implement a change to the codebase to make sure anything else has not been broken.\nHowever, once a commit has been pushed to Github, it is also useful to have automated testing executed automatically, at least for 2 reasons:\n\nRun tests in all the environments that need to be supported by the software, for example run with different versions of Python or different versions of a key required external dependancy\nRun tests in a clean environment that has less risks of being contaminated by some mis-configuration on one of the developers’ environments"
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#travis-ci",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#travis-ci",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Travis-CI",
    "text": "Travis-CI\nTravis is a free web based service that allows to register a trigger on Github so that every time a commit is pushed to Github or a Pull Request is opened, it launches an isolated Ubuntu (even if it also supports Mac OS) container for each of the configurations that we want to test, builds the software (if needed) and then runs the test.\nThe only requirement is that the Github project needs to be public for the free service. Otherwise there are paid plans for private repositories."
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#setup-on-travis-ci",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#setup-on-travis-ci",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Setup on Travis-CI",
    "text": "Setup on Travis-CI\n\nGo to http://travis-ci.org and login with a Github account\nIn order to automatatically configure the hook on Github, Travis requests writing privileges to your Github account, annoying but convenient\nLeave all default options, just make sure that Pull Requests are automatically tested\nIf you have the repository both under an organization and a fork under your account, you can choose either to test both or just the organization repository, anyway your pull requests will be tested before merging."
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#preparation-of-the-test-scripts",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#preparation-of-the-test-scripts",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Preparation of the test scripts",
    "text": "Preparation of the test scripts\nIn order to automate running the test scripts on Travis-CI, it is important that the test scripts return a exit code different from zero to signal that the tests failed.\nIf you are using a test running tool like pytest, this is automatically done for you. If you are using bash scripts instead, make sure that if the script detects an error it calls exit 1. In order to automate running the test scripts on Travis-CI, it is important that the test scripts return a exit code different from zero to signal that the tests failed.\nIf you are using a test running tool like pytest, this is automatically done for you. If you are using bash scripts instead, make sure that if the script detects an error it calls exit 1."
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#configuration-of-the-repository",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#configuration-of-the-repository",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Configuration of the repository",
    "text": "Configuration of the repository\n\nCreate a new branch on your repository:\n  git checkout -b test_travis\nAdd a .travis.yml (mind that it starts with a dot) configuration file\nInside this file you can configure how your project is built and tested, for the simple case of bash or perl scripts you can just write:\n  dist: trusty\n  language: bash\n\n  script:\n      - cd $TRAVIS_BUILD_DIR/tests; bash run_test.sh\nCheck the Travis-CI documentation for advanced configuration options\nNow push these changes to your fork of the main repository and then create a Pull Request to the main repository\nGo to https://travis-ci.org/YOUR_ORGANIZATION/YOUR_REPO to check the build status and the log\nOnce your Pull Request passes the tests, merge it to the main repository so that also the master branch will be tested for all future commits."
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#python-example",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#python-example",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Python example",
    "text": "Python example\nIn the following example, Travis-CI will create 8 builds, each of the 4 versions of Python will be tested with the 2 versions of numpy:\nlanguage: python\npython:\n  - \"2.7\"\n  - \"3.4\"\n  - \"3.5\"\n  - \"3.6\"\nenv:\n  - NUMPY_VERSION=1.12.1\n  - NUMPY_VERSION=1.13.1\n# command to install dependencies, requirements.txt should NOT include numpy\ninstall:\n  - pip install -r requirements.txt numpy==$NUMPY_VERSION\n# command to run tests\nscript:\n  - pytest # or py.test for Python versions 3.5 and below"
  },
  {
    "objectID": "posts/2017-09-06-continous-integration-travis-ci-github.html#badge-in-readme",
    "href": "posts/2017-09-06-continous-integration-travis-ci-github.html#badge-in-readme",
    "title": "Setup automated testing on a Github repository with Travis-ci",
    "section": "Badge in README",
    "text": "Badge in README\nAestetic touch, left click on the “Build Passing” image on the Travis-CI page for your repository, choose “Markdown” and paste the code to the README.md of your repository on Github. This will show in real time if the last version of the code is passing the tests or not."
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html",
    "title": "How to create pull requests on Github",
    "section": "",
    "text": "Pull Requests are the web-based version of sending software patches via email to code maintainers. They allow athat has no access to a code repository to submit a code change to the repository administrator that can review and mr I cannot find a guide to making pull requests on Github with the Last year I wrote some tutorials on simple deployments of Jupyterhub on Ubuntu 16.04 on the OpenStack deployment SDSC Cloud, even if most of the steps would also be suitable on other resources like Amazon EC2.\nIn more detail:\nThe Jupyter team has released an automated script to deploy Jupyterhub on a single server, see Jupyterhub-deploy-teaching.\nIn this tutorial we will use this script to deploy Jupyterhub to SDSC Cloud using:"
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#setup-a-virtual-machine-to-run-jupyterhub",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#setup-a-virtual-machine-to-run-jupyterhub",
    "title": "How to create pull requests on Github",
    "section": "Setup a Virtual Machine to run Jupyterhub",
    "text": "Setup a Virtual Machine to run Jupyterhub\nCreate first a Ubuntu 16.04 Virtual Machine, a default server image works fine.\nIn case you are deploying on SDSC Cloud, follow the steps in “Create a Virtual Machine in OpenStack” on my first tutorial at https://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html.\nYou will also need a DNS entry pointing to the server to create a SSL certificate with Let’s Encrypt. Either ask your institution to provide a DNS A entry, e.g. test-jupyterhub.ucsd.edu, that points to the Public IP of the server. SDSC Cloud already provides a DNS entry in the form xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu.\nIf you plan on using nbgrader, you need to create the home folder for the instructor beforehand, so SSH into the server and create a user with your Github username, i.e. I had to execute sudo adduser zonca"
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#setup-your-local-machine-to-run-the-automation-scripts",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#setup-your-local-machine-to-run-the-automation-scripts",
    "title": "How to create pull requests on Github",
    "section": "Setup your local machine to run the automation scripts",
    "text": "Setup your local machine to run the automation scripts\nAutomation of the server setup is provided by the Ansible software tool, it allows to describe a server configuration in great detail (a “playbook”) and then connects via SSH to a Virtual Machine and runs Python to install and setup all the required software.\nOn your local machine, install Ansible, at least version 2.1, see Ansible docs, for Ubuntu just add the Ansible PPA repository. I tested this with Ansible version 2.2.1.0\nThen you need to configure passwordless SSH connection to your Virtual Machine. Download your SSH key from the OpenStack dashboard, copy it to your ~/.ssh folder and then add an entry to .ssh/config for the server:\nHost xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu\n    HostName xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu\n    User ubuntu\n    IdentityFile \"~/.ssh/sdsccloud.key\"\nAt this point you should be able to SSH into the machine without typing any password with ssh xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu."
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#configure-and-run-the-ansible-script",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#configure-and-run-the-ansible-script",
    "title": "How to create pull requests on Github",
    "section": "Configure and run the Ansible script",
    "text": "Configure and run the Ansible script\nFollow the Jupyterhub-deploy-teaching documentation to checkout the script, configure and run it.\nThe only modification you need to do if you are on SDSC Cloud is that the remote user is ubuntu and not root, so modify ansible.cfg in the root of the repository, replace remote_user=root with remote_user=ubuntu.\nAs an example, see the configuration I used, just:\n\ncopy it into host_vars\nrename it to your public DNS record\nfill in proxy_auth_token, Github OAuth credentials for authentication\nreplace zonca with your Github username everywhere\n\nThe exact version of the jupyterhub-deploy-teaching code I used for testing is on the sdsc_cloud_jan_17 tag on Github"
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#test-the-deployment",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#test-the-deployment",
    "title": "How to create pull requests on Github",
    "section": "Test the deployment",
    "text": "Test the deployment\nConnect to https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu on your browser, you should be redirected to Github for authentication and then access a Jupyter Notebook instance with the Python 3, R and bash kernels running locally on the machine."
  },
  {
    "objectID": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#optional-docker",
    "href": "posts/2017-07-12-xstream-tutorial-notes-pearc17.html#optional-docker",
    "title": "How to create pull requests on Github",
    "section": "Optional: Docker",
    "text": "Optional: Docker\nIn order to provide isolation and resource limits to the users, it is useful to run single user Jupyter Notebooks inside Docker containers.\nYou will need to SSH into the Virtual Machine and follow the next steps.\n\nInstall Docker\nFirst of all we need to install and configure Docker on the machine, see:\n\nhttps://docs.docker.com/engine/installation/linux/ubuntu/\nhttps://docs.docker.com/engine/installation/linux/linux-postinstall/\n\n\n\nInstall dockerspawner\nThen install the Jupyterhub plugin dockerspawner that handles launching single user Notebooks inside Docker containers, we want to install from master instead of pypi to avoid an error setting the memory limit.\npip install git+https://github.com/jupyterhub/dockerspawner\n\n\nSetup the Docker container to run user Notebooks\nWe can first get the standard systemuser container, this Docker container mounts the home folder of the users inside the container, this way we can have persistent data even if the container gets deleted.\ndocker pull jupyterhub/systemuser\nIf you do not need nbgrader this image is enough, otherwise we have to build our own image, first checkout my Github repository in the home folder of the ubuntu user on the server with:\ngit clone https://github.com/zonca/systemuser-nbgrader\nthen edit the nbgrader_config.py file to set the correct course_id, and build the container image running inside the systemuser-nbgrader folder:\ndocker build -t systemuser-nbgrader .\n\n\nConfigure Jupyterhub to use dockerspawner\nThen add some configuration for dockerspawner to /etc/jupyterhub/jupyterhub_config.py:\nc.JupyterHub.spawner_class = 'dockerspawner.SystemUserSpawner'\nc.DockerSpawner.container_image = \"systemuser-nbgrader\" # delete this line if you just need `jupyterhub/systemuser`\n                                                                                                          c.Spawner.mem_limit = '500M' # or 1G for GB, probably 300M is minimum required just to run simple calculations\nc.DockerSpawner.volumes = {\"/srv/nbgrader/exchange\":\"/tmp/exchange\"} # this is necessary for nbgrader to transfer homework back and forth between students and instructor\nc.DockerSpawner.remove_containers = True\n\n# The docker instances need access to the Hub, so the default loopback port doesn't work:\nfrom IPython.utils.localinterfaces import public_ips\nc.JupyterHub.hub_ip = public_ips()[0]\n\n\nTest the deployment with Docker\nConnect to https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu on your browser, you should be redirected to Github for authentication and then access a Jupyter Notebook instance with the Python 2 or Python 3, open a Notebook and run !hostname in the first cell, you should find out that you get a Docker hash instead of the machine name, you are inside a container.\nSSH into the machine run docker ps to find the hash of a running container and then docker stat HASH to check memory usage and the current limit.\nCheck that you can connect to the nbgrader formgrade service that allows to manually grade assignments at https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu/services/formgrade-COURSEID, replace COURSEID with the course identifier you setup in the Ansible script.\n\n\nHands-on\n\nLogin only with gsissh, simplest use the XSEDE single signon hub and type gsissh xstream\nNo interactive capability, only batch, it can take a few hours or a day to get a single node\nYou can reserve just a portion of a node, gres like Comet\nSLURM uses Linux cgroups to contrain resources, so user only accesses his allocated GPU\nIf you SSH to allocated node, you are assigned to same cgroup so can access your allocated GPU\nQoS, “normal”: 2 days, “long” up to 7 days\n\n\n\nUse software\n\n**AMBER: capable of using peer-to-peer communication between GPUs, see teaching material for needed options\nTensorflow:"
  },
  {
    "objectID": "posts/2017-05-16-jupyterhub-batchspawner-ssh.html",
    "href": "posts/2017-05-16-jupyterhub-batchspawner-ssh.html",
    "title": "Deploy Jupyterhub on a Supercomputer with SSH Authentication",
    "section": "",
    "text": "The best way to deploy Jupyterhub with an interface to a Supercomputer is through the use of batchspawner. I have a sample deployment explained in an older blog post: https://zonca.github.io/2017/02/sample-deployment-jupyterhub-hpc.html\nThis setup however requires a OAUTH service, in this case provided by XSEDE, to authenticate the users via web and then provide a X509 certificate that is then used by batchspawner to connect to the Supercomputer on behalf of the user and submit the job to spawn a notebook.\nIn case an authentication service of this type is not available, another option is to use SSH authentication.\nThe starting point is a server with vanilla Jupyterhub installed, good practice would be to use an already available recipe with Ansible, like https://zonca.github.io/2017/02/automated-deployment-jupyterhub-ansible.html, that deploys Jupyterhub in a safer way, e.g. NGINX frontend with HTTPS.\nFirst we want to setup authentication, the simpler way to start would be to use the default authentication with local UNIX user accounts and possibly add Github later. In any case it is necessary that all the users have both an account on the Supercomputer and on the Jupyterhub server, with the same username, this is tedious but is the simpler way to allow them to authenticate on the Supercomputer. Then we need to save the private SSH key into each user’s .ssh folder and make sure they can SSH with no password required to the Supercomputer.\nThen we can install batchspawner and configure Jupyterhub to use it. In the batchspawner configuration in jupyterhub_config.py, you have to prefix the scheduler commands with ssh so that Jupyterhub can connect to the Supercomputer to submit the job:\nc.SlurmSpawner.batch_submit_cmd = 'ssh {username}@{host} sbatch'\nSee for example my configuration for Comet and replace gsissh with ssh.\nNow when users connect, they are authenticated with local UNIX user accounts username and password and then Jupyterhub uses their SSH key to launch a job on the Supercomputer.\nThe last issue is how to proxy the Jupyterhub running on a computing node back to the server, here one option would be to create a user on the server with no Terminal access but with the possibility of creating tunnels, then at the end of the job, setup a tunnel using a SSH Private Key pasted into the job script itself, see for example my setup on Comet."
  },
  {
    "objectID": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html",
    "href": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html",
    "title": "Sample deployment of Jupyterhub in HPC on SDSC Comet",
    "section": "",
    "text": "I have deployed an experimental Jupyterhub service (ask me privately if you would like access) installed on a SDSC Cloud virtual machine that spawns single user Jupyter notebooks on Comet computing nodes using batchspawner and then proxies the Notebook back to the user using SSH-tunneling."
  },
  {
    "objectID": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#functionality",
    "href": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#functionality",
    "title": "Sample deployment of Jupyterhub in HPC on SDSC Comet",
    "section": "Functionality",
    "text": "Functionality\nThis kind of setup is functionally equivalent to launching a job yourself on Comet, launch jupyter notebook and then SSH-Tunneling the port to your local machine, but way more convenient. You jus open your browser to the Jupyterhub instance, authenticate with your XSEDE credentials, choose queue and job length and wait for the Notebook job to be ready (generally it is a matter of minutes)."
  },
  {
    "objectID": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#rationale",
    "href": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#rationale",
    "title": "Sample deployment of Jupyterhub in HPC on SDSC Comet",
    "section": "Rationale",
    "text": "Rationale\nJupyter Notebooks have a lot of use-cases on HPC, it can be used for:\n\nIn-situ visualization\nInteractive data analysis when local resources are not enough, either in terms of RAM or disk space\nMonitoring other running jobs\nLaunch IPython Parallel jobs and distribute computation to them in parallel\nInteract with a running Spark cluster (we support Spark on Comet)\n\nMore on this on my Run Jupyterhub on a Supercomputer old blog post."
  },
  {
    "objectID": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#setup-details",
    "href": "posts/2017-02-26-sample-deployment-jupyterhub-hpc.html#setup-details",
    "title": "Sample deployment of Jupyterhub in HPC on SDSC Comet",
    "section": "Setup details",
    "text": "Setup details\nThe Jupyter team created a repository for sample HPC deployments, I added all configuration files of my deployment there, with all details about the setup:\n\nSample deployment in the jupyterhub-deploy-hpc repository\n\nPlease send feedback opening an issue in that repository and tagging @zonca."
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "",
    "text": "Last year I wrote some tutorials on simple deployments of Jupyterhub on Ubuntu 16.04 on the OpenStack deployment SDSC Cloud, even if most of the steps would also be suitable on other resources like Amazon EC2.\nIn more detail:\nThe Jupyter team has released an automated script to deploy Jupyterhub on a single server, see Jupyterhub-deploy-teaching.\nIn this tutorial we will use this script to deploy Jupyterhub to SDSC Cloud using:"
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#setup-a-virtual-machine-to-run-jupyterhub",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#setup-a-virtual-machine-to-run-jupyterhub",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "Setup a Virtual Machine to run Jupyterhub",
    "text": "Setup a Virtual Machine to run Jupyterhub\nCreate first a Ubuntu 16.04 Virtual Machine, a default server image works fine.\nIn case you are deploying on SDSC Cloud, follow the steps in “Create a Virtual Machine in OpenStack” on my first tutorial at https://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html.\nYou will also need a DNS entry pointing to the server to create a SSL certificate with Let’s Encrypt. Either ask your institution to provide a DNS A entry, e.g. test-jupyterhub.ucsd.edu, that points to the Public IP of the server. SDSC Cloud already provides a DNS entry in the form xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu.\nIf you plan on using nbgrader, you need to create the home folder for the instructor beforehand, so SSH into the server and create a user with your Github username, i.e. I had to execute sudo adduser zonca"
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#setup-your-local-machine-to-run-the-automation-scripts",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#setup-your-local-machine-to-run-the-automation-scripts",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "Setup your local machine to run the automation scripts",
    "text": "Setup your local machine to run the automation scripts\nAutomation of the server setup is provided by the Ansible software tool, it allows to describe a server configuration in great detail (a “playbook”) and then connects via SSH to a Virtual Machine and runs Python to install and setup all the required software.\nOn your local machine, install Ansible, at least version 2.1, see Ansible docs, for Ubuntu just add the Ansible PPA repository. I tested this with Ansible version 2.2.1.0\nThen you need to configure passwordless SSH connection to your Virtual Machine. Download your SSH key from the OpenStack dashboard, copy it to your ~/.ssh folder and then add an entry to .ssh/config for the server:\nHost xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu\n    HostName xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu\n    User ubuntu\n    IdentityFile \"~/.ssh/sdsccloud.key\"\nAt this point you should be able to SSH into the machine without typing any password with ssh xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu."
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#configure-and-run-the-ansible-script",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#configure-and-run-the-ansible-script",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "Configure and run the Ansible script",
    "text": "Configure and run the Ansible script\nFollow the Jupyterhub-deploy-teaching documentation to checkout the script, configure and run it.\nThe only modification you need to do if you are on SDSC Cloud is that the remote user is ubuntu and not root, so modify ansible.cfg in the root of the repository, replace remote_user=root with remote_user=ubuntu.\nAs an example, see the configuration I used, just:\n\ncopy it into host_vars\nrename it to your public DNS record\nfill in proxy_auth_token, Github OAuth credentials for authentication\nreplace zonca with your Github username everywhere\n\nThe exact version of the jupyterhub-deploy-teaching code I used for testing is on the sdsc_cloud_jan_17 tag on Github"
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#test-the-deployment",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#test-the-deployment",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "Test the deployment",
    "text": "Test the deployment\nConnect to https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu on your browser, you should be redirected to Github for authentication and then access a Jupyter Notebook instance with the Python 3, R and bash kernels running locally on the machine."
  },
  {
    "objectID": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#optional-docker",
    "href": "posts/2017-02-03-automated-deployment-jupyterhub-ansible.html#optional-docker",
    "title": "Automated deployment of Jupyterhub with Ansible",
    "section": "Optional: Docker",
    "text": "Optional: Docker\nIn order to provide isolation and resource limits to the users, it is useful to run single user Jupyter Notebooks inside Docker containers.\nYou will need to SSH into the Virtual Machine and follow the next steps.\n\nInstall Docker\nFirst of all we need to install and configure Docker on the machine, see:\n\nhttps://docs.docker.com/engine/installation/linux/ubuntu/\nhttps://docs.docker.com/engine/installation/linux/linux-postinstall/\n\n\n\nInstall dockerspawner\nThen install the Jupyterhub plugin dockerspawner that handles launching single user Notebooks inside Docker containers, we want to install from master instead of pypi to avoid an error setting the memory limit.\npip install git+https://github.com/jupyterhub/dockerspawner\n\n\nSetup the Docker container to run user Notebooks\nWe can first get the standard systemuser container, this Docker container mounts the home folder of the users inside the container, this way we can have persistent data even if the container gets deleted.\ndocker pull jupyterhub/systemuser\nIf you do not need nbgrader this image is enough, otherwise we have to build our own image, first checkout my Github repository in the home folder of the ubuntu user on the server with:\ngit clone https://github.com/zonca/systemuser-nbgrader\nthen edit the nbgrader_config.py file to set the correct course_id, and build the container image running inside the systemuser-nbgrader folder:\ndocker build -t systemuser-nbgrader .\n\n\nConfigure Jupyterhub to use dockerspawner\nThen add some configuration for dockerspawner to /etc/jupyterhub/jupyterhub_config.py:\nc.JupyterHub.spawner_class = 'dockerspawner.SystemUserSpawner'\nc.DockerSpawner.container_image = \"systemuser-nbgrader\" # delete this line if you just need `jupyterhub/systemuser`\n                                                                                                          c.Spawner.mem_limit = '500M' # or 1G for GB, probably 300M is minimum required just to run simple calculations\nc.DockerSpawner.volumes = {\"/srv/nbgrader/exchange\":\"/tmp/exchange\"} # this is necessary for nbgrader to transfer homework back and forth between students and instructor\nc.DockerSpawner.remove_containers = True\n\n# The docker instances need access to the Hub, so the default loopback port doesn't work:\nfrom IPython.utils.localinterfaces import public_ips\nc.JupyterHub.hub_ip = public_ips()[0]\n\n\nTest the deployment with Docker\nConnect to https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu on your browser, you should be redirected to Github for authentication and then access a Jupyter Notebook instance with the Python 2 or Python 3, open a Notebook and run !hostname in the first cell, you should find out that you get a Docker hash instead of the machine name, you are inside a container.\nSSH into the machine run docker ps to find the hash of a running container and then docker stat HASH to check memory usage and the current limit.\nCheck that you can connect to the nbgrader formgrade service that allows to manually grade assignments at https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu/services/formgrade-COURSEID, replace COURSEID with the course identifier you setup in the Ansible script.\n\n\nPre-built image\nI also have a saved Virtual Machine snapshot on SDSC Cloud named jupyterhub_ansible_nbgrader_coleman"
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html",
    "href": "posts/2017-01-23-singularity-hpc-comet.html",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "",
    "text": "Ever wanted to sudo apt install packages on a Supercomputer?\nEver wanted to freeze your software environment and reproduce a calculation after some time?\nEver wanted to dump your software environment to a file and move it to another Supercomputer? or wanted the same software on your laptop and on a computing node?\nIf your answer to any of those question is yes, read on! Otherwise, well, still read on, it’s awesome!"
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html#singularity",
    "href": "posts/2017-01-23-singularity-hpc-comet.html#singularity",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "Singularity",
    "text": "Singularity\nSingularity is a software project by Lawrence Berkeley Labs to provide a safe container technology for High Performance Computing, and it has been available for some time on my favorite Supercomputer, i.e. Comet at the San Diego Supercomputer Center.\nYou can read more details on their website, in summary you choose your own Operative System (any GNU/Linux distribution), describe its configuration in a standard format or even import an existing Dockerfile (from the popular Docker container technology) and Singularity is able to build an image contained in a single file. This file can then be executed on any Linux machine with Singularity installed (even on a Comet computing node), so you can run Ubuntu 16.10 or Red Hat 5 or any other flavor, your choice! It doesn’t need any deamon running like Docker, you can just execute a command inside the container by running:\nsingularity exec /path/to/your/image.img your_executable\nAnd your executable is run within the OS of the container.\nThe container technology is just sandboxing the environment, not executing a complete OS inside the host OS, so the loss of performance is minimal.\nIn summary, referring to the questions above:\n\nThis allows you to sudo apt install any package inside this environment when it is on your laptop, and then copy it to any Supercomputer and run your software inside that OS.\nYou can store this image to help reproduce your scientific results anytime in the future\nYou can develop your software inside a Singularity container and never have to worry about environment issues when you are ready for production runs on HPC or moving across different Supercomputers"
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html#build-a-singularity-image-for-sdsc-comet-with-mpi-support",
    "href": "posts/2017-01-23-singularity-hpc-comet.html#build-a-singularity-image-for-sdsc-comet-with-mpi-support",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "Build a Singularity image for SDSC Comet with MPI support",
    "text": "Build a Singularity image for SDSC Comet with MPI support\nOne of the trickiest things for such technology in HPC is support for MPI, the key stack for high speed network communication. I have prepared a tutorial on Github on how to build either a CentOS 7 or a Ubuntu 16.04 Singularity container for Comet that allows to use the mpirun command provided by the host OS on Comet but execute a code that supports MPI within the container.\n\nhttps://github.com/zonca/singularity-comet"
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html#more-complicated-setup-for-julia-with-mpi-support",
    "href": "posts/2017-01-23-singularity-hpc-comet.html#more-complicated-setup-for-julia-with-mpi-support",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "More complicated setup for Julia with MPI support",
    "text": "More complicated setup for Julia with MPI support\nFor a project that needed a setup with Julia with MPI support I built a more complicated container, see:\n\nhttps://github.com/zonca/singularity-comet/tree/master/debian_julia"
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html#prebuilt-containers",
    "href": "posts/2017-01-23-singularity-hpc-comet.html#prebuilt-containers",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "Prebuilt containers",
    "text": "Prebuilt containers\nI made also available my containers on Comet, they are located in my scratch space:\n/oasis/scratch/comet/zonca/temp_project\nand are named Centos7.img, Ubuntu.img and julia.img.\nYou can also copy those images to your local machine and customize them more."
  },
  {
    "objectID": "posts/2017-01-23-singularity-hpc-comet.html#trial-accounts-on-comet",
    "href": "posts/2017-01-23-singularity-hpc-comet.html#trial-accounts-on-comet",
    "title": "Run Ubuntu in HPC with Singularity",
    "section": "Trial accounts on Comet",
    "text": "Trial accounts on Comet\nIf you don’t have an account on Comet yet, you can request a trial allocation:\nhttps://www.xsede.org/web/xup/allocations-overview#types-trial\nEnjoy!"
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "",
    "text": "This post is part of a series on deploying Jupyterhub on OpenStack tailored at workshops, in the previous posts I showed:\nThe limitation of a single server setup is that it cannot scale beyond the resources available on that server, especially memory. Therefore for a workshop that requires to load large amount of data or with lots of students it is recommended to use a multi-server setup.\nFortunately Docker already provides that flexibility thanks to Docker Swarm. Docker Swarm allows to have a Docker interface that behaves like a normal single server instance but instead launches containers on a pool of servers. Therefore there are mininal changes on the Jupyterhub server.\nJupyterhub will interface with the Docker Swarm service running locally, Docker Swarm will take care of launching containers across the other nodes. Each container will launch a Jupyter Notebook server for a single user, then Jupyterhub will proxy the container port to the users. Users won’t connect directly to the nodes in the Docker Swarm pool."
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-the-jupyterhub-server",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-the-jupyterhub-server",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Setup the Jupyterhub server",
    "text": "Setup the Jupyterhub server\nLet’s start from the public image already available, see just the first section “Create a Virtual Machine in OpenStack with the pre-built image” in http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html for instructions on how to get the Jupyterhub single server running.\n\nSetup Docker Swarm\nFirst of all we need to have Docker accessible remotely so we need to configure it to listen on a TCP port, edit /etc/init/docker.conf and replace DOCKER_OPTS= in the start section with:\nDOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\"\nPort 2375 is not open on the OpenStack configuration, so this is not a security issue.\nThen we need to run 2 swarm services in Docker containers, first a distributed key-store listening on port 8500 that is needed for Swarm to store information about all the available nodes, Consul:\ndocker run --restart=always  -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap\nthe manager which provides the interface to Docker Swarm:\nHUB_LOCAL_IP=$(ip route get 8.8.8.8 | awk 'NR==1 {print $NF}')\ndocker run --restart=always  -d -p 4000:4000 swarm manage -H :4000 --replication --advertise $HUB_LOCAL_IP:4000 consul://$HUB_LOCAL_IP:8500\nThis sets HUB_LOCAL_IP to the internal ip of the instance, then starts the Manager container.\nWe are running both with automatic restarting, so that they are launched again in case of failure or after reboot.\nYou can check if the containers are running with:\ndocker ps -a\nand then you can check if connection works with Docker Swarm on port 4000:\ndocker -H :4000 ps -a\nCheck the Docker documentation for a more robust setup with multiple Consul services and a backup Manager.\n\n\nSetup Jupyterhub\nFollowing the work by Jess Hamrick for the compmodels Jupyterhub deployment, we can get the jupyterhub_config.py from https://gist.github.com/zonca/83d222df8d0b9eaebd02b83faa676753 and copy them into the home of the ubuntu user.\n\n\nShare users home via NFS\nWe have now a distributed system and we need a central location to store the home folders of the users, so that even if they happen to get containers on different server, they can still access their files.\nInstall NFS with the package manager:\nsudo apt-get install nfs-kernel-server\nedit /etc/exports, add:\n/home    *(rw,sync,no_root_squash)\nPorts are not open in the NFS configuration."
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-networking",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-networking",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Setup networking",
    "text": "Setup networking\nBefore preparing a node, create a new security group under Compute -&gt; Access & Security and name it swarm_group.\nWe need to be able to have open traffic between the swarmsecgroup and the group of the Jupyterhub instance, jupyterhubsecgroup in my previous tutorial. So in the new swarmsecgroup, add this rule:\n\nAdd Rule\nRule: ALL TCP\nDirection: Ingress\nRemote: Security Group\nSecurity Group: jupyterhubsecgroup\n\nAdd another rule replacing Ingress with Egress. Now open the jupyterhubsecgroup group and add the same 2 rules, just make sure to choose as target “Security Group” swarmsecgroup.\nOn the swarmsecgroup also add a Rule for SSH traffic from any source choosing CIDR and 0.0.0.0/0, you can disable this after having executed the configuration."
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-the-docker-swarm-nodes",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#setup-the-docker-swarm-nodes",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Setup the Docker Swarm nodes",
    "text": "Setup the Docker Swarm nodes\n\nLaunch a plain Ubuntu instance\nLaunch a new instance, all it swarmnode, choose the size depending on your requirements, and then choose “Boot from image” and get Ubuntu 14.04 LTS (16.04 should work as well, but I haven’t yet tested it). Remember to choose a Key Pair under Access & Security and assign the Security Group swarmsecgroup.\nTemporarily add a floating IP to this instance in order to SSH into it, see my first tutorial for more details.\n\n\nSetup Docker Swarm\nFirst install Docker engine:\nsudo apt update\nsudo apt install apt-transport-https ca-certificates\nsudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\necho \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo tee /etc/apt/sources.list.d/docker.list \nsudo apt update\nsudo apt install -y docker-engine\nsudo usermod -aG docker ubuntu\nThen make the same edit we did on the hub, edit /etc/init/docker.conf and replace DOCKER_OPTS= in the start section with:\nDOCKER_OPTS=\"-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock\"\nRestart Docker with:\nsudo service docker restart\nThen run the container that interfaces with Swarm:\nHUB_LOCAL_IP=10.XX.XX.XX\nNODE_LOCAL_IP=$(ip route get 8.8.8.8 | awk 'NR==1 {print $NF}')\ndocker run --restart=always -d swarm join --advertise=$NODE_LOCAL_IP:2375 consul://$HUB_LOCAL_IP:8500    \nCopy the address of the Jupyterhub server in the HUB_LOCAL_IP variable.\n\n\nSetup mounting the home filesystem\nsudo apt-get install autofs\nadd in /etc/auto.master:\n/home         /etc/auto.home\ncreate /etc/auto.home:\necho \"* $HUB_LOCAL_IP:/home/&\" | sudo tee /etc/auto.home\nusing the internal IP of the hub.\nsudo service autofs restart\nverify by doing:\nls /home/ubuntu\nor\nls /home/training01\nyou should see the same files that were on the Jupyterhub server.\n\n\nCreate users\nAs we are using system users and mounting the home filesystem it is important that users have the same UID on all nodes, so we are going to run on the node the same script we ran on the Jupyterhub server:\n bash create_users.sh\n \n\n\nTest Jupyterhub\nLogin on the Jupyterhub instance with 2 or more different users, then check on the console of the Hub that the containers were launched on the swarmnode instance:\n docker -H :4000 ps -a"
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#create-more-nodes",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#create-more-nodes",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Create more nodes",
    "text": "Create more nodes\nNow that we created a fully functioning node we can clone it to create more to accomodate more users.\n\nCreate a snapshot of the node\nFirst we need to delete all Docker containers, ssh into the swarmnode and execute:\n docker rm -f $(docker ps -a -q)\nDocker has a unique identifying key, we need to remove that so that it will be regenerated by the clones.\nsudo service docker stop\nsudo rm /etc/docker/key.json\nThen from Compute-&gt;Instances choose “Create Snapshot”, call it swarmnodeimage.\n\n\nLaunch other nodes\nClick on Launch instance-&gt;“Boot from Snapshot”-&gt;swarmnodeimage, choose the swarmnodesecgroup Security Group. Choose any number of instances you need.\nEach node will need to launch the Swarm container with its own local ip, not the same as our first node. Therefore we need to use the “Post Creation”-&gt;“Direct Input” and add this script:\n#!/bin/bash\nHUB_LOCAL_IP=10.XX.XX.XX\nNODE_LOCAL_IP=$(ip route get 8.8.8.8 | awk 'NR==1 {print $NF}')\ndocker run --restart=always -d swarm join --advertise=$NODE_LOCAL_IP:2375 consul://$HUB_LOCAL_IP:8500\nHUB_LOCAL_IP is the internal network IP address of the Jupyterhub instance and NODE_LOCAL_IP will be filled with the IP of the OpenStack image just created.\nSee for example Jupyterhub with 3 remote Swarm nodes running containers for 4 training users:\n$ docker -H :4000 ps -a\nCONTAINER ID        IMAGE                                     COMMAND                  CREATED              STATUS              PORTS                         NAMES\n60189f208df2        zonca/jupyterhub-datascience-systemuser   \"tini -- sh /srv/sing\"   11 seconds ago       Up 7 seconds        10.128.1.28:32769-&gt;8888/tcp   swarmnodes-1/jupyter-training04\n1d7b05caedb1        zonca/jupyterhub-datascience-systemuser   \"tini -- sh /srv/sing\"   36 seconds ago       Up 32 seconds       10.128.1.27:32768-&gt;8888/tcp   swarmnodes-2/jupyter-training03\n733c5ff0a5ed        zonca/jupyterhub-datascience-systemuser   \"tini -- sh /srv/sing\"   58 seconds ago       Up 54 seconds       10.128.1.29:32768-&gt;8888/tcp   swarmnodes-3/jupyter-training02\n282abce201dd        zonca/jupyterhub-datascience-systemuser   \"tini -- sh /srv/sing\"   About a minute ago   Up About a minute   10.128.1.28:32768-&gt;8888/tcp   swarmnodes-1/jupyter-training01\n29b2d394fab9        swarm                                     \"/swarm join --advert\"   13 minutes ago       Up 13 minutes       2375/tcp                      swarmnodes-2/romantic_easley\n8fd3d32fe849        swarm                                     \"/swarm join --advert\"   13 minutes ago       Up 13 minutes       2375/tcp                      swarmnodes-3/clever_mestorf\n1ae073f7b78b        swarm                                     \"/swarm join --advert\"   13 minutes ago       Up 13 minutes       2375/tcp                      swarmnodes-1/jovial_goldwasser"
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#where-to-go-from-here",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#where-to-go-from-here",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Where to go from here",
    "text": "Where to go from here\nAt this level the deployment is quite complicated, so it is probably worth automating it with an ansible playbook, that will be the subject of the next blog post, I think the result will be a simplified version of Jess Hamrick’s compmodels deployment. Still, I recommend starting with a manual setup to understand how the different pieces work."
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#troubleshooting",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#troubleshooting",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf docker -H :4000 ps -a gives the error:\nError response from daemon: No elected primary cluster manager\nit means the Consul container is broken, remove it and create it again."
  },
  {
    "objectID": "posts/2016-05-24-jupyterhub-docker-swarm.html#acknowledgments",
    "href": "posts/2016-05-24-jupyterhub-docker-swarm.html#acknowledgments",
    "title": "Jupyterhub deployment on multiple nodes with Docker Swarm",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks to Jess Hamrick for sharing the setup of her compmodel class on Github, the Jupyter team for releasing such great tools and Kevin Coakley and the rest of the SDSC Cloud team for OpenStack support and resources."
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "",
    "text": "This tutorial describes the steps to install a Jupyterhub instance on a single machine suitable for hosting a workshop, suitable for having people login with training accounts on Jupyter Notebooks running Python 2/3, R, Julia with also Terminal access on Docker containers. Details about the setup:\nI am using the OpenStack deployment at the San Diego Supercomputer Center, SDSC Cloud, AWS deployments should just replace the first section on Creating a VM and setting up Networking, see the Jupyterhub wiki.\nIf you intend to run on SDSC Cloud, I have a pre-built image of this deployment you can setup and run quickly, see see my followup tutorial."
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#network-setup",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#network-setup",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Network setup",
    "text": "Network setup\nJupyterhub will be proxied to the standard HTTPS port by NGINX and we also want to redirect HTTP to HTTPS, so we open those ports, then SSH for the administrators to login and a custom TCP rule in order for the Docker containers to be able to connect to the Jupyterhub hub running on port 8081, so we are opening that port just to the subnet that is running the Docker containers.\n\nCompute -&gt; Access & Security -&gt; Security Groups -&gt; Create Security Group and name it jupyterhubsecgroup\nClick on Manage Rules\nClick on add rule, choose the HTTP rule and click add\nRepeat the last step with HTTPS and SSH\nClick on add rule again, choose Custom TCP Rule, set port 8081 and set CIDR 172.17.0.0/24 (this is needed so that the containers can connect to the hub)"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#create-a-new-virtual-machine",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#create-a-new-virtual-machine",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Create a new Virtual Machine",
    "text": "Create a new Virtual Machine\nWe choose Ubuntu here, also other distributions should work fine.\n\nCompute -&gt; Access & Security -&gt; Key Pairs -&gt; Create key pair, name it jupyterhub and download it to your local machine\nInstances -&gt; Launch Instance, Choose a name, Choose “Boot from image” in Boot Source and Ubuntu as Image name, Choose any size, depending on the number of users (TODO add link to Jupyterhub docs)\nUnder “Access & Security” choose Key Pair jupyterhub and Security Groups jupyterhubsecgroup\nClick Launch to create the instance"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#give-public-ip-to-the-instance",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#give-public-ip-to-the-instance",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Give public IP to the instance",
    "text": "Give public IP to the instance\nBy default in SDSC Cloud machines do not have a public IP.\n\nCompute -&gt; Access & Sewcurity -&gt; Floating IPs -&gt; Allocate IP To Project, “Allocate IP” to request a public IP\nClick on the “Associate” button of the IP just requested and under “Port to be associated” choose the instance just created"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#setup-jupyterhub",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#setup-jupyterhub",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Setup Jupyterhub",
    "text": "Setup Jupyterhub\n wget --no-check-certificate https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n bash Miniconda3-latest-Linux-x86_64.sh\nuse all defaults, answer “yes” to modify PATH\nsudo apt-get install npm nodejs-legacy\nsudo npm install -g configurable-http-proxy\nconda install traitlets tornado jinja2 sqlalchemy \npip install jupyterhub\nFor authentication to work, the ubuntu user needs to be able to read the /etc/shadow file:\nsudo adduser ubuntu shadow"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#setup-the-web-server",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#setup-the-web-server",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Setup the web server",
    "text": "Setup the web server\nWe will use the NGINX web server to proxy Jupyterhub and handle HTTPS for us, this is recommended for deployments on the public internet.\nsudo apt install nginx\nSSL Certificate: Optionally later, once we have assigned a domain to the Virtual Machine, we can install letsencrypt and get a real certificate, see my followup tutorial, for simplicity here we are just using self-signed certificates that will give warnings on the first time users connect to the server, but still will keep the traffic encrypted.\nsudo mkdir /etc/nginx/ssl\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt\n\nGet /etc/nginx/nginx.conf from https://gist.github.com/zonca/08c413a37401bdc9d2a7f65a7af44462"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#install-docker",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#install-docker",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Install Docker",
    "text": "Install Docker\n\nSource: https://docs.docker.com/engine/installation/linux/ubuntulinux/#prerequisites\n\nsudo apt update\nsudo apt install apt-transport-https ca-certificates\nsudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D\necho \"deb https://apt.dockerproject.org/repo ubuntu-trusty main\" | sudo tee /etc/apt/sources.list.d/docker.list \nsudo apt update\nsudo apt install docker-engine\nsudo usermod -aG docker ubuntu\nLogout and login again for the group to take effect"
  },
  {
    "objectID": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#install-and-configure-dockerspawner",
    "href": "posts/2016-04-16-jupyterhub-sdsc-cloud.html#install-and-configure-dockerspawner",
    "title": "Deploy Jupyterhub on a Virtual Machine for a Workshop",
    "section": "Install and configure DockerSpawner",
    "text": "Install and configure DockerSpawner\npip install dockerspawner\ndocker pull jupyter/systemuser\nconda install ipython jupyter\nCreate jupyterhub_config.py in the home folder of the ubuntu user with this content:\nc.JupyterHub.confirm_no_ssl = True\nc.JupyterHub.spawner_class = 'dockerspawner.SystemUserSpawner'\n\n# The docker instances need access to the Hub, so the default loopback port doesn't work:\nfrom IPython.utils.localinterfaces import public_ips\nc.JupyterHub.hub_ip = public_ips()[0]"
  },
  {
    "objectID": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html",
    "href": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html",
    "title": "IPython/Jupyter notebook setup on NERSC Edison",
    "section": "",
    "text": "This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Edison at NERSC and forward its port encrypted with SSH to the browser on a local laptop. This setup is a bit more complicated than other supercomputers, i.e. see my tutorial for Comet for 2 reasons:\n\nEdison’s computing nodes run a stripped down OS, with no support for SSH, unless you activate Cluster Compatibility Mode (CCM)\nOn edison you generally don’t have direct access to a computing node, even if you request an interactive node you actually have access to an intermediary node (MOM node), from there aprun sends a job for execution on the computing node."
  },
  {
    "objectID": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#introduction",
    "href": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#introduction",
    "title": "IPython/Jupyter notebook setup on NERSC Edison",
    "section": "",
    "text": "This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Edison at NERSC and forward its port encrypted with SSH to the browser on a local laptop. This setup is a bit more complicated than other supercomputers, i.e. see my tutorial for Comet for 2 reasons:\n\nEdison’s computing nodes run a stripped down OS, with no support for SSH, unless you activate Cluster Compatibility Mode (CCM)\nOn edison you generally don’t have direct access to a computing node, even if you request an interactive node you actually have access to an intermediary node (MOM node), from there aprun sends a job for execution on the computing node."
  },
  {
    "objectID": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#quick-reference",
    "href": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#quick-reference",
    "title": "IPython/Jupyter notebook setup on NERSC Edison",
    "section": "Quick reference",
    "text": "Quick reference\n\nInstall IPython notebook and make sure it is in the path, I recommend to install Anaconda 64bit in your home folder or on scratch.\nMake sure you can ssh passwordless within Edison, i.e. ssh edison from Edison login node works without password\nCreate a folder notebook in your home, get notebook_job.pbs and launch_notebook_and_tunnel_to_login.sh from https://gist.github.com/zonca/357d36347fd5addca8f0\nChange the port number and customize options (duration)\nqsub notebook_job.pbs\nFrom laptop, launch bash tunnel_laptop_edisonlogin.sh ## from https://gist.github.com/zonca/5f8b5ccb826a774d3f89, where ## is the edison login number in 2 digits, like 03. First you need to modify the port number.\nFrom laptop, open browser and connect to http://localhost:YOURPORT"
  },
  {
    "objectID": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#detailed-walkthrough",
    "href": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#detailed-walkthrough",
    "title": "IPython/Jupyter notebook setup on NERSC Edison",
    "section": "Detailed walkthrough",
    "text": "Detailed walkthrough\n\nOne time setup on Edison\nMake sure that ipython notebook works on a login node, one option is to install Anaconda 64bit from http://continuum.io/downloads#py34. Choose Python 3.\nYou need to be able to SSH from a node to another node on Edison with no need of a password. Create a new SSH certificate with ssh-keygen, hit enter to keep all default options, DO NOT ENTER A PASSWORD. Then use ssh-copy-id edison.nersc.gov, enter your password to make sure the key is copied in the authorized hosts. Now you can check it works by executing:\nssh edison.nersc.gov\nfrom the login node and make sure you are NOT asked for your password.\n\n\nConfigure the script for TORQUE and submit the job\nCreate a notebook folder on your home on Edison.\nCopy notebook_job.pbs and launch_notebook_and_tunnel_to_login.sh from https://gist.github.com/zonca/357d36347fd5addca8f0 to the notebook folder.\nChange the port number in the launch_notebook_and_tunnel_to_login.sh script to a port of your choosing between 7000 and 9999, referenced as YOURPORT in the rest of the tutorial. Two users on the same login node on the same port would not be allowed to forward, so try to avoid common port numbers as 8000, 9000, 8080 or 8888.\nChoose a duration of your job, for initial testing better keep 30 minutes so your job starts sooner.\nSubmit the job to the scheduler:\nqsub notebook_job.pbs\nWait for the job to start running, you should see R in:\nqstat -u $USER\nThe script launches an IPython notebook on a computing node and tunnels its port to the login node.\nYou can check that everything worked by checking that no errors show up in the notebook.log file, and that you can access the notebook page with wget:\nwget localhost:YOURPORT\nshould download a index.html file in the current folder, and NOT give an error like “Connection refused”.\n\n\nTunnel the port to your laptop\n\nLinux / MAC\nDownload the tunnel_laptop_edisonlogin.sh script from https://gist.github.com/zonca/357d36347fd5addca8f0.\nCustomize the script with your port number and your username.\nLaunch bash tunnel_laptop_edisonlogin.sh ## where ## is the Edison login node you launched the job from in 2 digits, e.g. 03.\nThe script forwards the port from the login node of Edison to your laptop.\n\n\nWindows\nInstall putty.\nFollow tutorial for local port forwarding on http://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/\n\nset edison##-eth5.nersc.gov as remote host, where ## is the Edison login node you launched the job from in 2 digits, e.g. 03 and set 22 as SSH port\nset YOURPORT as tunnel port, replace both 8080 and 80 in the tutorial with your port number.\n\n\n\n\nConnect to the Notebook\nOpen a browser and type http://localhost:YOURPORT in the address bar.\nSee in the screenshot from my local browser, the hostname is one of Edison’s computing node:\n\n\n\ntest_edison_screenshot.png"
  },
  {
    "objectID": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#acknowledgements",
    "href": "posts/2015-09-24-ipython-jupyter-notebook-nersc-edison.html#acknowledgements",
    "title": "IPython/Jupyter notebook setup on NERSC Edison",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks Lisa Gerhardt from NERSC user support to help me understand Edison’s configuration."
  },
  {
    "objectID": "posts/2015-04-02-jupyterhub-hpc.html",
    "href": "posts/2015-04-02-jupyterhub-hpc.html",
    "title": "Run Jupyterhub on a Supercomputer",
    "section": "",
    "text": "Summary: I developed a plugin for Jupyterhub: RemoteSpawner, it has a proof-of-concept interface with the Supercomputer Gordon at UC San Diego to spawn IPython Notebook instances as jobs throught the queue and tunnel the interface back to the Jupyterhub instance.\nThe IPython (recently renamed Jupyter) Notebook is a powerful tool for analyzing and visualizing data in Python and other programming languages. A key feature is that a single document contains code, figures, text and equations. Everything is saved in a single .ipynb file that can be shared, executed and modified. See an example Notebook on integration of partial differential equations.\nThe Jupyter Notebook is a Python application with a web frontend, i.e. the interface runs in the user browser. This setup makes it suitable for any kind of remote computing, in particular running the Jupyter Notebook on a computing node of a Supercomputer, and exporting the interface HTTP port to a local browser. Setting up tunneling via SSH is tedious, in particular if the user does not have a public IP address.\nJupyterhub, developed by the Jupyter team, comes to the rescue by providing a web application that manages and proxies multiple instances of the Jupyter Notebook for any number of users. Jupyterhub natively only spawns local processes, but supports plugins to extend its functionality.\nI have been developing a proof-of-concept plugin (RemoteSpawner) designed to work on a web server and once a user is authenticated, connect to the login node of a Supercomputer and submit a Jupyter Notebook job. As soon as the job starts execution, it sets up SSH tunneling with the Jupyterhub host so that Jupyterhub can provide the Notebook interface to the user. This setup allows users to simply access a Supercomputer via browser, accessing all their Python environment and data.\nI am looking for interested parties either as users or as collaborators to help further development. See more information about the project below."
  },
  {
    "objectID": "posts/2015-04-02-jupyterhub-hpc.html#test-it-yourself",
    "href": "posts/2015-04-02-jupyterhub-hpc.html#test-it-yourself",
    "title": "Run Jupyterhub on a Supercomputer",
    "section": "Test it yourself",
    "text": "Test it yourself\nIn order to have a feeling on how Jupyterhub works, you can test in your browser at:\n\nhttp://tmpnb.org\n\nThis service by Rackspace creates temporary Jupyter Notebooks on the fly. If you click on Welcome.ipynb, you can see an example Notebook.\nThe purpose of my project is to have a web interface to access Jupyter Notebooks that are running on computing nodes of a Supercomputer. So that users can access the environment and data on a Supercomputer from their browser and run data-intensive processing."
  },
  {
    "objectID": "posts/2015-04-02-jupyterhub-hpc.html#tour-of-jupyterhub-on-the-gordon-supercomputer",
    "href": "posts/2015-04-02-jupyterhub-hpc.html#tour-of-jupyterhub-on-the-gordon-supercomputer",
    "title": "Run Jupyterhub on a Supercomputer",
    "section": "Tour of Jupyterhub on the Gordon Supercomputer",
    "text": "Tour of Jupyterhub on the Gordon Supercomputer\nI’ll show some screenshots to display how a test Jupyterhub installation on my machine is integrated with Gordon thanks to the plugin.\nJupyterhub is accessed publicly via browser and the user can login. Jupyterhub supports authentication for PAM/LDAP so it could be integrated with XSEDE credential, at the moment I am testing with local authentication.\n\n\n\njupyterhub-hpc-login.png\n\n\nOnce the user is authenticated, Jupyterhub connects via SSH to a login node on Gordon and submits a batch serial job using qsub. The web interface waits for the job to start running. A dedicated queue with a quick turnaround would be useful for this kind of jobs.\n \nWhen the job starts running, it first sets up SSH tunneling between the Jupyterhub host and the computing node, then starts the Jupyter Notebook. As soon as the web interface detects that the job is running, proxies the tunneled HTTP port for the user. From this point the Jupyter Notebook works exactly like it would on a local machine.\nSee an example Notebook printing the hostname of the computing node:\n\n\n\njupyterhub-hpc-testnotebook.png\n\n\nOther two useful features of the Jupyter Notebook are a terminal:\n\n\n\njupyterhub-hpc-terminal.png\n\n\nand an editor that run in the browser:\n\n\n\njupyterhub-hpc-editor.png"
  },
  {
    "objectID": "posts/2015-04-02-jupyterhub-hpc.html#launch-jupyterhub-parallel-to-access-hundreds-of-computing-engines",
    "href": "posts/2015-04-02-jupyterhub-hpc.html#launch-jupyterhub-parallel-to-access-hundreds-of-computing-engines",
    "title": "Run Jupyterhub on a Supercomputer",
    "section": "Launch Jupyterhub parallel to access hundreds of computing engines",
    "text": "Launch Jupyterhub parallel to access hundreds of computing engines\nThe Notebook also supports using Torque to run Python computing engines and send them computationally intensive serial functions for load-balanced execution.\nIn the Notebook interface, in the Clusters tab, is it possible to choose the number of engines and click start to submit a job to the queue system:\n\n\n\njupyterhub-hpc-clusterlaunch.png\n\n\nThis will pack 16 jobs per node (Gordon has 16-cores CPUs) and make them available from the notebook, see an example usage where I process 1000 files with 128 engines running on a different job on Gordon:\n\nExample of Jupyterhub Parallel"
  },
  {
    "objectID": "posts/2015-02-10-software-carpentry-setup-chromebook.html",
    "href": "posts/2015-02-10-software-carpentry-setup-chromebook.html",
    "title": "Software Carpentry setup for Chromebook",
    "section": "",
    "text": "In this post I’ll provide instructions on how to install the main requirements of a Software Carpentry workshop on a Chromebook. Bash, git, IPython notebook and R."
  },
  {
    "objectID": "posts/2015-02-10-software-carpentry-setup-chromebook.html#switch-the-chromebook-to-developer-mode",
    "href": "posts/2015-02-10-software-carpentry-setup-chromebook.html#switch-the-chromebook-to-developer-mode",
    "title": "Software Carpentry setup for Chromebook",
    "section": "Switch the Chromebook to Developer mode",
    "text": "Switch the Chromebook to Developer mode\nChromeOS is very restrictive on what users can install on the machine. The only way to get around this is to switch to developer mode.\nSwitching to Developer mode wipes all the data on the local disk and may void warranty, do it at your own risk.\nInstructions are available on the ChromeOS wiki, you need to click on your device name and follow instructions. For most devices you need to switch the device off, then hold down ESC and Refresh and poke the Power button, then press Ctrl-D at the Recovery screen (there is no prompt, you have to know to do it). This will wipe the device and activate Developer mode.\nOnce you reboot and enter your Google credentials, the Chromebook will copy back from Google servers all of your settings.\nNow you are in Developer mode, the main feature is that you have a root (superuser) shell you can activate using Ctrl-Alt-T.\nThe worst issue of Developer mode is that at each boot the system will display a scary screen warning that OS verification is off and asks you if you would like to leave Developer mode. If you either press Ctrl-D or wait 30 seconds, it will boot ChromeOS in Developer mode, if you instead hit the Space, it will wipe everything and switch back to Normal mode."
  },
  {
    "objectID": "posts/2015-02-10-software-carpentry-setup-chromebook.html#install-ubuntu-with-crouton",
    "href": "posts/2015-02-10-software-carpentry-setup-chromebook.html#install-ubuntu-with-crouton",
    "title": "Software Carpentry setup for Chromebook",
    "section": "Install Ubuntu with crouton",
    "text": "Install Ubuntu with crouton\nYou can now install Ubuntu using crouton, you can read the instructions on the page, in summary:\n\nFirst you need to install the Crouton Chrome extension on ChromeOS\nDownload the last release from https://goo.gl/fd3zc\nOpen the ChromeOS shell using Ctrl-Alt-t, digit shell at the prompt and hit enter\nRun sudo sh ~/Downloads/crouton -t xfce,xiwi -r trusty, this instlls Ubuntu Trutyty with xfce desktop and uses kiwi to be able to run in a window.\n\nNow you can have Ubuntu running in a window of the Chromebook browser by:\n\nPress Ctrl-Alt-T\ndigit shell at the prompt and hit enter\ndigit sudo startxfce4\n\nWhat is great about crouton is that it is not like a Virtual Machine, Ubuntu runs at full performance on the same linux kernel of ChromeOS."
  },
  {
    "objectID": "posts/2015-02-10-software-carpentry-setup-chromebook.html#install-scientific-computing-stack",
    "href": "posts/2015-02-10-software-carpentry-setup-chromebook.html#install-scientific-computing-stack",
    "title": "Software Carpentry setup for Chromebook",
    "section": "Install scientific computing stack",
    "text": "Install scientific computing stack\nYou can now follow the instructions for Linux at http://software-carpentry.org/v5/setup.html, summary of commands to run in a terminal:\n\nsudo apt install nano\nsudo apt install git\nIn order to install R sudo apt install r-base\nDownload Anaconda Python 3 64bit for Linux from http://continuum.io/downloads and execute it\n\nAnaconda will run under Ubuntu but when you open an IPython notebook, it will automatically open a new tab in the main browser of ChromeOS, not inside the Ubuntu window."
  },
  {
    "objectID": "posts/2015-02-10-software-carpentry-setup-chromebook.html#final-note",
    "href": "posts/2015-02-10-software-carpentry-setup-chromebook.html#final-note",
    "title": "Software Carpentry setup for Chromebook",
    "section": "Final note",
    "text": "Final note\nI admit it looks scary, I personally followed this procedure successfully on 2 chromebooks: Samsung Chromebook 1 and Toshiba Chromebook 2.\nSee a screenshot on my Chromebook with the Ubuntu window on the right with git, nano and IPython notebook running, the IPython notebook window opens in Chrome, see the left window (click to enlarge).\n\nIt is also possible to switch the Chromebook to Developer mode and install Anaconda and git directly there, however I think that in order to have a complete platform for scientific computing is a lot better to have all of the packages provided by Ubuntu."
  },
  {
    "objectID": "posts/2014-09-30-unit-tests-ipython-notebook.html",
    "href": "posts/2014-09-30-unit-tests-ipython-notebook.html",
    "title": "Write unit tests as cells of IPython notebooks",
    "section": "",
    "text": "Plugin for py.test to write unit tests as cells in IPython notebooks:\n\nHomepage on Github: https://github.com/zonca/pytest-ipynb\nPyPi : https://pypi.python.org/pypi/pytest-ipynb/\nInstall with pip install pytest-ipynb"
  },
  {
    "objectID": "posts/2014-09-30-unit-tests-ipython-notebook.html#what",
    "href": "posts/2014-09-30-unit-tests-ipython-notebook.html#what",
    "title": "Write unit tests as cells of IPython notebooks",
    "section": "",
    "text": "Plugin for py.test to write unit tests as cells in IPython notebooks:\n\nHomepage on Github: https://github.com/zonca/pytest-ipynb\nPyPi : https://pypi.python.org/pypi/pytest-ipynb/\nInstall with pip install pytest-ipynb"
  },
  {
    "objectID": "posts/2014-09-30-unit-tests-ipython-notebook.html#why",
    "href": "posts/2014-09-30-unit-tests-ipython-notebook.html#why",
    "title": "Write unit tests as cells of IPython notebooks",
    "section": "Why?",
    "text": "Why?\nMany unit testing fromeworks in Python, first of all the unittest package in the standard library, work very well for automating unit tests, but make it very difficult to debug interactively any failed test.\npy.test alleviates this problem by allowing to write just plain Python functions with assert statements (no boilerplate code), discover them automatically in any file that starts with test and write a useful report.\nI wrote a plugin for py.test, pytest-ipynb, that goes a step further and runs unit tests written as cells of any IPython notebook named test*.ipynb.\nThe advantage is that it is easy to create and debug interactively any issue by opening the testing notebook interactively, then clean the notebook outputs and add it to the software repository.\nMore details on Github: https://github.com/zonca/pytest-ipynb\nSuggestions welcome as comments or github issues.\n(Yes, works with Python 3)"
  },
  {
    "objectID": "posts/2014-08-19-github-for-research-groups.html",
    "href": "posts/2014-08-19-github-for-research-groups.html",
    "title": "Create a Github account for your research group with free private repositories",
    "section": "",
    "text": "See the updated version at https://zonca.github.io/2019/08/github-for-research-groups.html\nGithub allows a research group to create their own webpage where they can host, share and develop their software using the git version control system and the powerful Github online issue-tracking interface.\nSince February 2014 Github also offers 20 private repositories to research groups and classrooms, plus unlimited public repositories. Private repositories are useful for early stages of development or if it is necessary to keep software secret before publication, at publication they can easily switched to public repositories and free up their slot.\nHere the steps to set this up:\n\nCreate a user account on Github and choose the free plan, use your .edu email address\nCreate an organization account for your research group\nGo to https://education.github.com/ and click on “Request a discount”\nChoose what is your position, e.g. Researcher and select you want a discount for an organization\nChoose the organization you created earlier and confirm that it is a “Research group”\nAdd details about your Research group\nFinally you need to upload a picture of your University ID card and write how you plan on using the repositories\nWithin a week at most, but generally in less than 24 hours, you will be approved for 20 private repositories.\n\nOnce the organization is created, you can add key team members to the “Owners” group, and then create another group for students and collaborators.\nConsider also that is not necessary for every collaborator to have write access to your repositories. My recommendation is to ask a more experienced team member to administer the central repository, ask the students to fork the repository under their user accounts (forks of private repositories are always private, free and don’t use any slot), and then send a pull request to the central repository for the administrator to review, discuss and merge.\nSee for example the organization account of the “Genomics, Evolution, and Development” at Michigan State U led by Dr. C. Titus Brown where they share code, documentation and papers. Open Science!!\nOther suggestions on the setup very welcome!"
  },
  {
    "objectID": "posts/2014-03-20-python-on-gordon.html",
    "href": "posts/2014-03-20-python-on-gordon.html",
    "title": "Python on Gordon",
    "section": "",
    "text": "Gordon has already a python environment setup which can be activated by loading the python module:\nmodule load python # add this to .bashrc to load it at every login\n\nInstall virtualenv\nThen we need to setup a sandboxed local environment to install other packages, by using virtualenv, get the link to the latest version from https://pypi.python.org/pypi/virtualenv, then download it on gordon and unpack it, e.g.\nwget --no-check-certificate https://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.11.2.tar.gz\ntar xzvf virtualenv*tar.gz\nThen create your own virtualenv and load it:\nmkdir ~/venv\npython virtualenv-*/virtualenv.py ~/venv/py\nsource ~/venv/py/bin/activate # add this to .bashrc to load it at every login\nyou can restore your previous environment by deactivating the virtualenv:\ndeactivate # from your bash prompt\n\n\nInstall IPython\nUsing pip you can install IPython and all dependencies for the notebook and parallel tools running:\npip install ipython pyzmq tornado jinja\n\n\nConfigure the IPython notebook\nFor interactive data exploration, you can run the IPython notebook in a computing node on Gordon and export the web interface to your local machine, which also embeds all the plots. Configuring the tunnelling over SSH is complicated, so I created a script, takes a little time to setup but then is very easy to use, see https://github.com/pyHPC/ipynbhpc.\n\n\nConfigure IPython parallel\nIPython parallel on Gordon allows to launch a PBS job with tens (or hundreds) of Python engines and then easily submit hundreds (or thousands) of serial jobs to be executed with automatic load balancing. First of all create the default configuration files:\nipython profile create --parallel \nThen, in ~/.ipython/profile_default/ipcluster_config.py, you need to setup:\nc.IPClusterStart.controller_launcher_class = 'LocalControllerLauncher' \nc.IPClusterStart.engine_launcher_class = 'PBS' \nc.PBSLauncher.batch_template_file = u'/home/REPLACEWITHYOURUSER/.ipython/profile_default/pbs.engine.template' # \"~\" does not work\nYou also need to allow connections to the controller from other hosts, setting in ~/.ipython/profile_default/ipcontroller_config.py:\nc.HubFactory.ip = '*'\nc.HubFactory.engine_ip = '*'\nFinally create the PBS template ~/.ipython/profile_default/pbs.engine.template:\n#!/bin/bash\n#PBS -q normal\n#PBS -N ipcluster\n#PBS -l nodes={n/16}:ppn={n}:native\n#PBS -l walltime=01:00:00\n#PBS -o ipcluster.out\n#PBS -e ipcluster.err\n#PBS -m abe\n#PBS -V\nmpirun_rsh -np {n} -hostfile $PBS_NODEFILE ipengine\nHere we chose to run 16 IPython engines per Gordon node, so each has access to 4GB of ram, if you need more just change 16 to 8 for example.\n\n\nRun IPython parallel\nYou can submit a job to the queue running, n is equal to the number of processes you want to use, so it needs to be a multiple of the ppn chosen in the PBS template:\nipcluster start --n=32 &\nin this case we are requesting 2 nodes, with 16 IPython engines each, check with:\nqstat -u $USER\nbasically ipcluster runs an ipcontroller on the login node and submits a job to PBS for running the ipengines on the computing nodes.\nOnce the PBS job is running, check that the engines are connected by opening a IPython on the login node and print the ids:\nIn [1]: from IPython.parallel import Client\nIn [2]: rc = Client()\nIn [3]: rc.ids\nYou can stop the cluster (kills ipcontroller and runs qdel on the PBS job) either by sending CTRL-c to ipcluster or running:\nipcluster stop # from bash console\n\n\nSubmit jobs to IPython parallel\nAs soon as ipcluster is executed, ipcontroller is ready to queue jobs up, which will be then consumed by the engines once they will be running. The easiest method to submit jobs with automatic load balancing is to create a load balanced view:\nIn [1]: from IPython.parallel import Client\nIn [2]: rc = Client()\nIn [3]: lview = rc.load_balanced_view() # default load-balanced view\nand then use its map method:\ndef exp_10(x):\n    return x**10\n    \nlist_of_args = range(100)\nresult = lview.map(exp_10, list_of_args)\nIn this code IPython will distribute uniformly the list of arguments to the engines and the function will be evalutated for each of them and the result copied back to the connecting client running on the login node.\n\n\nSubmit non-python jobs to IPython parallel\nLet’s assume you have a list of commands you want to run in a text file, one command per line, those could be implemented in any programming language, e.g.:\ndate &&gt; date.log\nhostname &&gt; hostname.log\nThen you create a function that executes one of those commands:\ndef run_command(command):\n    import subprocess\n    subprocess.Popen(command, shell = True)\nThen apply this function to the list of commands:\nlist_of_commands = open(\"commands.txt\").readlines()\nlview.map(run_command, list_of_commands)\nI created a script that automates this process, see https://gist.github.com/zonca/8994544, you can run as:\n./ipcluster_run_commands.py commands.txt"
  },
  {
    "objectID": "posts/2014-02-26-build-software-carpentry-with-pelican.html",
    "href": "posts/2014-02-26-build-software-carpentry-with-pelican.html",
    "title": "Build Software Carpentry lessons with Pelican",
    "section": "",
    "text": "Software Carpentry offers bootcamps for scientist to teach basic programming skills. All the material, mainly about bash, git, Python and R is available on Github under Creative Commons.\nThe content is either in Markdown or in IPython notebook format, and is currently built using Jekyll, nbconvert and Pandoc. Basicly the requirement is to make it easy for bootcamp instructors to setup their own website, modify the content, and have the website updated.\nI created a fork of the Software Carpentry repository and configured Pelican for creating the website:\nPelican handles fenced code blocks, see http://swcarpentry-pelican.github.io/ and conversion of IPython notebooks, see http://swcarpentry-pelican.github.io/lessons/numpy-notebook.html"
  },
  {
    "objectID": "posts/2014-02-26-build-software-carpentry-with-pelican.html#how-to-setup-the-repositories-for-a-new-bootcamp",
    "href": "posts/2014-02-26-build-software-carpentry-with-pelican.html#how-to-setup-the-repositories-for-a-new-bootcamp",
    "title": "Build Software Carpentry lessons with Pelican",
    "section": "How to setup the repositories for a new bootcamp",
    "text": "How to setup the repositories for a new bootcamp\n\ncreate a new Organization on Github and add all the other instructors, name it: swcarpentry-YYYY-MM-DD-INST where INST is the institution name, e.g. NYU\nFork the bootcamp-pelican repository under the organization account\nCreate a new repository in your organization named swcarpentry-YYYY-MM-DD-INST.github.io that will host the HTML of the website, also tick initialize with README, it will help later.\n\nNow you can either prepare the build environment on your laptop or have the web service travis-ci automatically update the website whenever you update the repository (even from the Github web interface!)."
  },
  {
    "objectID": "posts/2014-02-26-build-software-carpentry-with-pelican.html#buildupdate-the-website-from-your-laptop",
    "href": "posts/2014-02-26-build-software-carpentry-with-pelican.html#buildupdate-the-website-from-your-laptop",
    "title": "Build Software Carpentry lessons with Pelican",
    "section": "Build/Update the website from your laptop",
    "text": "Build/Update the website from your laptop\n\nClone the bootcamp-pelican repository of your organization locally\nCreate a Python virtual environment and install requirements with:\n cd bootcamp-pelican\n virtualenv swcpy\n . swcpy/bin/activate\n pip install -r requirements.txt\nClone the swcarpentry-YYYY-MM-DD-INST.github.io in the output folder as:\n git clone git@github.com:swcarpentry-YYYY-MM-DD-INST.github.io.git output\nBuild or Update the website with Pelican running\n fab build\nYou can display the website in your browser locally with:\n fab serve\nFinally you can publish it to Github with:\n cd output\n git add .\n git push origin master"
  },
  {
    "objectID": "posts/2014-02-26-build-software-carpentry-with-pelican.html#configure-travis-ci-to-automatically-build-and-publish-the-website",
    "href": "posts/2014-02-26-build-software-carpentry-with-pelican.html#configure-travis-ci-to-automatically-build-and-publish-the-website",
    "title": "Build Software Carpentry lessons with Pelican",
    "section": "Configure Travis-ci to automatically build and publish the website",
    "text": "Configure Travis-ci to automatically build and publish the website\n\nGo to http://travis-ci.org and login with Github credentials\nUnder https://travis-ci.org/profile click on the organization name on the left and activate the webhook setting ON on your bootcamp-pelican repository\nNow it is necessary to setup the credentials for travis-ci to write to the repository\nGo to https://github.com/settings/tokens/new, create a new token with default permissions\nInstall the travis tool (in debian/ubuntu sudo gem install travis) and run from any machine (not necessary to have a clone of the repository):\n travis encrypt -r swcarpentry-YYYY-MM-DD-INST/bootcamp-pelican GH_TOKEN=TOKENGOTATTHEPREVIOUSSTEP\notherwise I’ve setup a web application that does the encryption in your browser, see: http://travis-encrypt.github.io\nOpen .travis.yml on the website and replace the string under env: global: secure: with the string from travis encrypt\nPush the modified .travis.yml to trigger the first build by Travis, and then check the log on http://travis-ci.org\n\nNow any change on the source repository will be picked up automatically by Travis and used to update the website."
  },
  {
    "objectID": "posts/2014-01-31-wget-file-from-google-drive.html",
    "href": "posts/2014-01-31-wget-file-from-google-drive.html",
    "title": "wget file from google drive",
    "section": "",
    "text": "Sometimes it is useful, even more if you have a chromebook, to upload a file to Google Drive and then use wget to retrieve it from a server remotely.\nIn order to do this you need to make the file available to “Anyone with the link”, then click on that link from your local machine and get to the download page that displays a Download button. Now right-click and select “Show page source” (in Chrome), and search for “downloadUrl”, copy the url that starts with https://docs.google.com, for example:\nhttps://docs.google.com/uc?id\\u003d0ByPZe438mUkZVkNfTHZLejFLcnc\\u0026export\\u003ddownload\\u0026revid\\u003d0ByPZe438mUkZbUIxRkYvM2dwbVduRUxSVXNERm0zZFFiU2c0PQ\nThis is unicode, so open Python and do:\ndownload_url = \"PASTE HERE\"\nprint download_url.decode(\"unicode_escape\")\nu'https://docs.google.com/uc?id=0ByPZe438mUkZVkNfTHZLejFLcnc&export=download&revid=0ByPZe438mUkZbUIxRkYvM2dwbVduRUxSVXNERm0zZFFiU2c0PQ'\nThe last url can be pasted into a terminal and used with wget."
  },
  {
    "objectID": "posts/2013-12-10-joining-sandiego-supercomputer-center.html",
    "href": "posts/2013-12-10-joining-sandiego-supercomputer-center.html",
    "title": "Joining San Diego Supercomputer Center",
    "section": "",
    "text": "TL;DR Left UCSB after 4 years, got staff position at San Diego Supercomputer Center within UCSD, will be helping research groups analyze their data on Gordon and more. Still 20% on Planck.\nI spent 4 great years at UCSB with Peter Meinhold working on analyzing Cosmic Microwave Background data from the ESA Planck space mission. Cosmology is fascinating, also I enjoyed working with a very open minded team, that always left large freedom in choosing the techniques and the software tools for the job.\nMy work has been mainly focused on understanding and characterizing large amount of data using Python (and C++) on NERSC supercomputers. I was neither interested nor fit for a traditional academic career, and I was looking for a job that allowed me to focus on doing research/data analysis full time.\nThe perfect opportunity showed up, as the San Diego Supercomputer Center was looking for a computational scientist with a strong scientific background in any field of science to help research teams jump into supercomputing, specifically newcomers. This involves having the opportunity to collaborate with groups in any area of science, the first projects I am going to work on will be in Astrophysics, Quantum Chemistry and Genomics!\nI also have the opportunity to continue my work on calibration and mapmaking of Planck data in collaboration with UCSB for 20% of my time."
  },
  {
    "objectID": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html",
    "href": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html",
    "title": "Jiffylab multiuser IPython notebooks",
    "section": "",
    "text": "jiffylab is a very interesting project by Preston Holmes to provide sandboxed IPython notebooks instances on a server using docker. There are several user cases, for example:"
  },
  {
    "objectID": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#how-to-install-jiffylab-on-ubuntu-12.04",
    "href": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#how-to-install-jiffylab-on-ubuntu-12.04",
    "title": "Jiffylab multiuser IPython notebooks",
    "section": "How to install jiffylab on Ubuntu 12.04",
    "text": "How to install jiffylab on Ubuntu 12.04\n\nInstall docker on Ubuntu Precise\nCopy-paste each line of linux-setup.sh to a terminal, to check what is going on step by step\nTo start the application, change user to jiffylabweb:\n\nsudo su jiffylabweb\ncd /usr/local/etc/jiffylab/webapp/\npython app.py #run in debug mode\n\nPoint your browser to the server to check debugging messages, if any.\nFinally start the application in production mode:\n\npython server.py #run in production mode"
  },
  {
    "objectID": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#how-jiffylab-works",
    "href": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#how-jiffylab-works",
    "title": "Jiffylab multiuser IPython notebooks",
    "section": "How jiffylab works",
    "text": "How jiffylab works\nEach users gets a sandboxed IPython notebook instance, the user can save the notebooks and reconnect to the same session later. Main things missing:\n\nNo real authentication system / no HTTPS connection, easy workaround would be to allow access only from local network/VPN/SSH tunnel\nNo scientific packages preinstalled, need to customize the docker image to have numpy, matplotlib, pandas…\nNo access to common filesystem, read-only, this I think is the most pressing feature missing, issue already on Github\n\nI think that just adding the common filesystem would be enough to make the project already usable to provide students a way to easily get started with python."
  },
  {
    "objectID": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#few-screenshots",
    "href": "posts/2013-10-14-jiffylab-multiuser-ipython-notebooks.html#few-screenshots",
    "title": "Jiffylab multiuser IPython notebooks",
    "section": "Few screenshots",
    "text": "Few screenshots\n\nLogin page\n\n\n\nIPython notebook dashboard\n\n\n\nIPython notebook"
  },
  {
    "objectID": "posts/2013-09-27-google-plus-comments-plugin-for-pelican.html",
    "href": "posts/2013-09-27-google-plus-comments-plugin-for-pelican.html",
    "title": "Google Plus comments plugin for Pelican",
    "section": "",
    "text": "There has been recently several discussions about whether comments are any useful on blogs I think it is important to find better ways to connect blogs to social networks. In my opinion the most suitable social network for this is Google+, because there is space for larger discussion, without Twitter’s character limit.\nSo, for my small blog I’ve decided to implement the Google+ commenting system, which Google originally implemented just for Blogger but that works on any website.\nSee it in action below.\nThe plugin is available in the googleplus_comments branch in:\nhttps://github.com/zonca/pelican-plugins/tree/googleplus_comments/googleplus_comments"
  },
  {
    "objectID": "posts/2013-09-17-clviewer-interactive-plot-of-CMB-spectra.html",
    "href": "posts/2013-09-17-clviewer-interactive-plot-of-CMB-spectra.html",
    "title": "clviewer, interactive plot of CMB spectra",
    "section": "",
    "text": "Today it was HackDay at .Astronomy, so I felt compelled to hack something around myself, creating something I have been thinking for a while after my previous work on Interactive CMB power spectra in the browser\nThe idea is to get text files from a user and load it in a browser-based interactive display built on top of the d3.js and rickshaw libraries.\nSimilar to nbviewer, I think it is very handy to load data from Github gists, because then there is no need of uploading files and it is easier to circulate links.\nSo I created a small web app, in Python of course, using Flask and deployed on Heroku. It just gets a gist number, calls the Github APIs to load the files, and displays them in the browser:\n\nApplication website: http://clviewer.herokuapp.com\nExample input data: https://gist.github.com/zonca/6599016\nExample interactive plot: http://clviewer.herokuapp.com/6599016\nSource: https://github.com/zonca/clviewer"
  },
  {
    "objectID": "posts/2013-09-02-run-hadoop-python-jobs-on-amazon-with-mrjob.html",
    "href": "posts/2013-09-02-run-hadoop-python-jobs-on-amazon-with-mrjob.html",
    "title": "Run Hadoop Python jobs on Amazon with MrJob",
    "section": "",
    "text": "First we need to install mrjob with: \n\npip install mrjob\n\nI am starting with a simple example of word counting. Previously I implemented this directly using the hadoop streaming interface, therefore mapper and reducer were scripts that read from standard input and print to standard output, see mapper.py and reducer.py in:    https://github.com/zonca/python-wordcount-hadoop    With MrJob instead the interface is a little different, we implement the mapper  method of our subclass of MrJob that already gets a “line” argument and yields the output as a tuple like (“word”, 1). \n\nMrJob makes the implementation of the reducer particularly simple. Using hadoop-streaming directly, we needed also to first parse back the output of the mapper into python objects, while MrJob does it for you and gives directly the key and the list of count, that we just need to sum.\n\n\n  \n\n\nThe code is pretty simple:  \n\nhttps://github.com/zonca/python-wordcount-hadoop/blob/master/mrjob/word_count_mrjob.py\n\n\n\n\nFirst we can test locally with 2 different methods, either:  \n\npython word_count_mrjob.py gutemberg/20417.txt.utf-8\n\n or:  \n\npython word_count_mrjob.py –runner=local gutemberg/20417.txt.utf-8\n\n The first is a simple local test, the seconds sets some hadoop variables and uses multiprocessing to run the mapper in parallel. \n\n\n\n Run on Amazon Elastic Map Reduce   \n\n\nNext step is submitting the job to EMR.  First get an account on Amazon Web Services from  aws.amazon.com  .   Setup MrJob with Amazon:    https://mrjob.readthedocs.io/en/latest/guides/emr-quickstart.html   \n\nThen we just need to choose the “emr” runner for MrJob to take care of:\n\n\n\n\nCopy the python module to Amazon S3, with requirements\n\n\nCopy the input data to S3\n\n\nCreate a small EC2 instance (of course we could set it up to run 1000 instead)\n\n\nRun Hadoop to process the jobs\n\n\nCreate a local web service that allows easy monitoring of the cluster\n\n\nWhen completed, copy the results back (this can be disabled to just leave the results on S3.\n\n\n\n\ne.g.:\n\n\npython word_count_mrjob.py –runner=emr –aws-region=us-west-2 gutemberg/20417.txt.utf-8\n\n\nIt is important to make sure that the aws-region used by MrJob is the same we used for creating the SSH key on the EC2 console in the MrJob configuration step, i.e. SSH keys are region-specific.    Logs and output of the run    MrJob copies the needed files to S3: \n\n. runemr.sh  using configs in /home/zonca/.mrjob.conf  using existing scratch bucket mrjob-ecd1d07aeee083dd  using s3://mrjob-ecd1d07aeee083dd/tmp/ as our scratch dir on S3  creating tmp directory /tmp/mrjobjob.zonca.20130901.192250.785550  Copying non-input files into s3://mrjob-ecd1d07aeee083dd/tmp/mrjobjob.zonca.20130901.192250.785550/files/  Waiting 5.0s for S3 eventual consistency  Creating Elastic MapReduce job flow  Job flow created with ID: j-2E83MO9QZQILB  Created new job flow j-2E83MO9QZQILB\n\nCreates the instances: \n\nJob launched 30.9s ago, status STARTING: Starting instances  Job launched 123.9s ago, status BOOTSTRAPPING: Running bootstrap actions  Job launched 250.5s ago, status RUNNING: Running step (mrjobjob.zonca.20130901.192250.785550: Step 1 of 1)\n\nCreates an SSH tunnel to the tracker: \n\nOpening ssh tunnel to Hadoop job tracker  Connect to job tracker at: http://localhost:40630/jobtracker.jsp\n\n\nTherefore we can connect to that address to check realtime information about the cluster running on EC2, for example:  \n\n  \n\n Once the job completes, MrJob copies the output back to the local machine, here are few lines from the file: \n\n“maladies”   1  “malaria”   5  “male”   18  “maleproducing”   1  “males”   5  “mammal”   10  “mammalInstinctive”   1  “mammalian”   4  “mammallike”   1  “mammals”   87  “mammoth”   5  “mammoths”   1  “man”   152\n\nI’ve been positively impressed that it is so easy to implement and run a MapReduce job with MrJob without need of managing directly EC2 instances or the Hadoop installation.  This same setup could be used on GB of data with hundreds of instances."
  },
  {
    "objectID": "posts/2013-08-20-planck-ctp-angular-power-spectrum-ell.html",
    "href": "posts/2013-08-20-planck-ctp-angular-power-spectrum-ell.html",
    "title": "Planck CTP angular power spectrum ell binning",
    "section": "",
    "text": "Planck released a binning of the angular power spectrum in the Explanatory supplement,  unfortunately the file is in PDF format, non easily machine-readable:    http://www.sciops.esa.int/wikiSI/planckpla/index.php?title=Frequency_maps_angular_power_spectra&instance=Planck_Public_PLA    So here is a csv version:   https://gist.github.com/zonca/6288439    Follows embedded gist."
  },
  {
    "objectID": "posts/2013-08-04-export-google-analytics-data-via-api.html",
    "href": "posts/2013-08-04-export-google-analytics-data-via-api.html",
    "title": "Export google analytics data via API with Python",
    "section": "",
    "text": "Fun weekend hacking project: export google analytics data using the google APIs.   Clone the latest version of the API client from:    https://code.google.com/p/google-api-python-client    there is an example for accessing analytics APIs in the samples/analytics folder,  but you need to fill in client_secrets.json.   You can get the credentials from the APIs console:    https://code.google.com/apis/console    In SERVICES: activate google analytics  In API Access: Create a “Client ID for installed applications” choosing “Other” as a platform   Copy the client id and the client secret to client_secrets.json.      Now you only need the profile ID of the google analytics account, it is in the google analytics web interface, just choose the website, then click on Admin, then on the profile name in the profile tab, and then on profile settings.   You can then run:  \n\n\npython core_reporting_v3_reference.py ga:PROFILEID\n\nThe first time you run it, it will open a browser for authentication, but then the auth token is saved and used for future requests.   This retrieves from the APIs the visits to the website from search, with keywords and the number of visits, for example for my blog:  \n\nTotal Metrics For All Results:  This query returned 25 rows.  But the query matched 30 total results.  Here are the metric totals for the matched total results.  Metric Name  = ga:visits  Metric Total = 174  Rows:  google   (not provided)   121  google   andrea zonca   17  google   butterworth filter python   4  google   andrea zonca blog   2  google   healpix for ubuntu   2  google   healpy install ubuntu   2  google   python butterworth filter   2  google   zonca andrea   2  google   andrea zonca buchrain luzern   1  google   andrea zonca it   1  google   astrofisica in pillole   1  google   bin data healpy   1  google   ellipticity fwhm   1  google   enthought and healpy   1  google   fwhm   1  google   healpix apt-get   1  google   healpix repository ubuntu   1  google   healpix ubuntu 12.04 install   1  google   healpy ubuntu   1  google   install healpix ubuntu   1  google   ipython cluster task output   1  google   numpy pink noise   1  google   pink noise numpy   1  google   python 1/f noise   1  google   python apply mixin   1"
  },
  {
    "objectID": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html",
    "href": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html",
    "title": "How to use the IPython notebook on a small computing cluster",
    "section": "",
    "text": "The IPython notebook is a powerful and easy to use interface for using Python and particularly useful when running remotely, because it allows the interface to run locally in your browser, while the computing kernel runs remotely on the cluster."
  },
  {
    "objectID": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#configure-ipython-notebook",
    "href": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#configure-ipython-notebook",
    "title": "How to use the IPython notebook on a small computing cluster",
    "section": "1) Configure IPython notebook:",
    "text": "1) Configure IPython notebook:\nFirst time you use the notebook you need to follow this configuration steps:\n\nLogin to the cluster\nLoad the python environment, for example:\n  module load pythonEPD\nCreate the profile files:\n  ipython profile create # creates the configuration files\n  vim .ipython/profile_default/ipython_notebook_config.py\nset a password, see instructions in the file.\nChange the port to something specific to you, please change this to avoid conflict with other users:\n  c.NotebookApp.port = 8900\nSet a certificate to serve the notebook over https:\n  c.NotebookApp.certfile = u'/home/zonca/mycert.pem'\nor create a new certificate, see the documentation\nSet:\n  c.NotebookApp.open_browser = False"
  },
  {
    "objectID": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#run-the-notebook-for-testing-on-the-login-node.",
    "href": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#run-the-notebook-for-testing-on-the-login-node.",
    "title": "How to use the IPython notebook on a small computing cluster",
    "section": "2) Run the notebook for testing on the login node.",
    "text": "2) Run the notebook for testing on the login node.\nYou can use IPython notebook on the login node if you do not use much memory, e.g. &lt; 300MB. ssh into the login node, at the terminal run:\nipython notebook --pylab=inline\nopen the browser on your local machine and connect to (always use https, replace 8900 with your port):\nhttps://LOGINNODEURL:8900\nDismiss all the browser complaints about the certificate and go ahead."
  },
  {
    "objectID": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#run-the-notebook-on-a-computing-node",
    "href": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#run-the-notebook-on-a-computing-node",
    "title": "How to use the IPython notebook on a small computing cluster",
    "section": "3) Run the notebook on a computing node",
    "text": "3) Run the notebook on a computing node\nYou should always use a computing node whenever you need a large amount of resources.\nCreate a folder notebooks/ in your home, just copy this script in runipynb.pbs in your that folder:\n\nreplace LOGINNODEURL with the url of the login node of your cluster.\nNOTICE: you need to ask the sysadmin to set GatewayPorts yes in sshd_config on the login node to allow access externally to the notebook.\nSubmit the job to the queue running:\nqsub runipynb.pbs\nThen from your local machine connect to (replace 8900 with your port):\nhttps://LOGINNODEURL:8900"
  },
  {
    "objectID": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#other-introductory-python-resources",
    "href": "posts/2013-06-22-how-to-use-ipython-notebook-on-small.html#other-introductory-python-resources",
    "title": "How to use the IPython notebook on a small computing cluster",
    "section": "Other introductory python resources",
    "text": "Other introductory python resources\n\nScientific computing with Python, large and detailed introduction to Python, Numpy, Matplotlib, Scipy\nMy Python for High performance computing: slides and few ipython notebook examples, see the README\nMy short Python and healpy tutorial"
  },
  {
    "objectID": "posts/2013-04-08-simple-mixin-usage-in-python.html",
    "href": "posts/2013-04-08-simple-mixin-usage-in-python.html",
    "title": "Simple Mixin usage in python",
    "section": "",
    "text": "One situation where Mixins are useful in Python is when you need to modify  a method of similar classes that you are importing from a package. \n\n\n\n\n\nFor just a single class, it is easier to just create a derived class, but if the same modification must be applied to several classes, then it is cleaner to implement this modification once in a Mixin and then apply it to all of them.\n\n\n  \n\n\nHere an example in Django:\n\n\n\n\n\nDjango has several generic view classes that allow to pull objects from the database and feed them to the html templates.\n\n\n\n\n\nOne for example shows the detail of a specific object:\n\n\n\n\n\n from django.views.generic.detail import DetailView \n\n\n\n\n\n\nThis class has a get_object method that gets an object from the database given a primary key.\n\n\nWe need to modify this method to allow access to an object only to the user that owns them.\n\n\n\n\n\nWe first implement a Mixin, i.e. an independent class that only implements the method we wish to override:\n\n\n\n\n\n class OwnedObjectMixin(object): \n\n\n def get_object(self, *args, **kwargs): \n\n\n obj = super(OwnedObjectMixin, self).get_object(*args, **kwargs) \n\n\n if not obj.user == self.request.user: \n\n\n raise Http404 \n\n\n return obj \n\n\n\n\n\n\n Then we create a new derived class which inherits both from the Mixin and from the class we want to modify. \n\n\n  \n\n\n\n\n class ProtectedDetailView(OwnedObjectMixin, DetailView): \n\n\n pass \n\n\n\n\n  \n\n\nThis overrides the get_object method of DetailView with the get_object method of OwnedObjectMixin, and the call to super calls the get_object method of DetailView, so has the same effect of subclassing DetailView and override the get_object method, but we can be apply the same Mixin to other classes."
  },
  {
    "objectID": "posts/2013-04-06-basic-forkpull-git-workflow.html",
    "href": "posts/2013-04-06-basic-forkpull-git-workflow.html",
    "title": "Basic fork/pull git workflow",
    "section": "",
    "text": "Typical simple workflow for a (github) repository with few users.\n\n\n  \n\n\n Permissions configuration: \n\n\nMain developers have write access to the repository, occasional contributor are supposed to fork and create pull requests.\n\n\n\n\n  \n\n Main developer:  Small bug fix just go directly in master:\n\n\n  \n\n\n git  checkout master  # update from repository, better use rebase in case there are unpushed commits  git pull –rebase  git commit -m “commit message”  git push \n\n\n\n\n\nMore complex feature, better use a branch:\n\n\n  \n\n\n git checkout -b featurebranch  git commit -am “commit message”  # work and make several commits  # backup and share to github  git push origin featurebranch \n\n\n  \n\n\n When ready to merge (cannot push cleanly anymore after any rebasing):        \n\n\n # reorder, squash some similar commits, better commit msg    git rebase -i HEAD~10    # before merging move commits all together to the end of history    git rebase master    git checkout master    git merge featurebranch    git push    # branch is fully merged, no need to keep it    git branch -d featurebranch    git push origin –delete featurebranch \n\n\n\n\n\nOptional, if the feature requires discussing within the team, better create a pull request.  After cleanup and rebase, instead of merging to master:    \n\n\n # create new branch  git checkout -b readyfeaturebranch  git push origin readyfeaurebranch \n\n\nConnect to github and create a pull request from the new branch to master (now github has a shortcut for creating a pull request from the last branch pushed).\n\n\n\n\n\nDuring the discussion on the pull request, any commit to the readyfeaturebranch is added to the pull request.  When ready either automatically merge on github, or do it manually as previously.\n\n\n\n\n\n For occasional developers:  Just fork the repo on github to their account, work on a branch there, and then create a pull request on the github web interface from the branch to master on the main repository."
  },
  {
    "objectID": "posts/2013-02-27-how-to-cite-hdf5-in-bibtex.html",
    "href": "posts/2013-02-27-how-to-cite-hdf5-in-bibtex.html",
    "title": "How to cite HDF5 in bibtex",
    "section": "",
    "text": "here the bibtex entry:  \n\n reference:   http://www.hdfgroup.org/HDF5-FAQ.html#gcite"
  },
  {
    "objectID": "posts/2013-01-18-elliptic-beams-fwhm-and-ellipticity.html",
    "href": "posts/2013-01-18-elliptic-beams-fwhm-and-ellipticity.html",
    "title": "Elliptic beams, FWHM and ellipticity",
    "section": "",
    "text": "The relationship between the Full Width Half Max, FWHM (min, max, and average) and the    ellipticity is:      FWHM = sqrt(FWHM_min * FWHM_max)    e = FWHM_max/FWHM_min"
  },
  {
    "objectID": "posts/2012-10-06-butterworth-filter-with-python.html",
    "href": "posts/2012-10-06-butterworth-filter-with-python.html",
    "title": "Butterworth filter with Python",
    "section": "",
    "text": "Using IPython notebook of course:    http://nbviewer.ipython.org/3843014/"
  },
  {
    "objectID": "posts/2012-09-26-homepage-on-aboutme.html",
    "href": "posts/2012-09-26-homepage-on-aboutme.html",
    "title": "homepage on about.me",
    "section": "",
    "text": "moved my homepage to about.me:    http://about.me/andreazonca    it is quite nice, and essential, as most of it is just links to other websites, i.e. arXiv for publications, Linkedin for CV, github for code.  So I’m going to use andreazonca.com as blog, hosted on blogger."
  },
  {
    "objectID": "posts/2012-07-06-compile-python-module-with-mpi-support.html",
    "href": "posts/2012-07-06-compile-python-module-with-mpi-support.html",
    "title": "compile python module with mpi support",
    "section": "",
    "text": "CC=mpicc LDSHARED=“mpicc -shared” python setup.py build_ext -i"
  },
  {
    "objectID": "posts/2011-06-21-unit-testing-happiness.html",
    "href": "posts/2011-06-21-unit-testing-happiness.html",
    "title": "unit testing happiness",
    "section": "",
    "text": "nosetests -vtest_all_cols (pycfitsio.test.TestPyCfitsIoRead) ... oktest_colnames (pycfitsio.test.TestPyCfitsIoRead) ... oktest_move (pycfitsio.test.TestPyCfitsIoRead) ... oktest_open_file (pycfitsio.test.TestPyCfitsIoRead) ... oktest_read_col (pycfitsio.test.TestPyCfitsIoRead) ... oktest_read_hdus (pycfitsio.test.TestPyCfitsIoRead) ... oktest_create (pycfitsio.test.TestPyCfitsIoWrite) ... oktest_write (pycfitsio.test.TestPyCfitsIoWrite) ... ok----------------------------------------------------------------------Ran 8 tests in 0.016sOK"
  },
  {
    "objectID": "posts/2011-05-18-pink-noise-1f-noise-simulations-in-numpy.html",
    "href": "posts/2011-05-18-pink-noise-1f-noise-simulations-in-numpy.html",
    "title": "Pink noise (1/f noise) simulations in numpy",
    "section": "",
    "text": "https://gist.github.com/979729"
  },
  {
    "objectID": "posts/2011-04-13-set-python-logging-level.html",
    "href": "posts/2011-04-13-set-python-logging-level.html",
    "title": "set python logging level",
    "section": "",
    "text": "often using logging.basicConfig is useless because if the logging module is already configured upfront by one of the imported libraries this is ignored.   The solution is to set the level directly in the root logger:   ﻿﻿logging.root.level = logging.DEBUG"
  },
  {
    "objectID": "posts/2011-02-16-ipython-and-pytrilinos.html",
    "href": "posts/2011-02-16-ipython-and-pytrilinos.html",
    "title": "ipython and PyTrilinos",
    "section": "",
    "text": "start ipcontroller\n\n\n\nstart ipengines:   mpiexec -n 4 ipengine –mpi=pytrilinos \n\n\n\nstart ipython 0.11:   import PyTrilinos  from IPython.kernel import client  mec = client.MultiEngineClient()  %load_ext parallelmagic  mec.activate()  px import PyTrilinos  px comm=PyTrilinos.Epetra.PyComm()  px print(comm.NumProc())"
  },
  {
    "objectID": "posts/2011-01-07-memory-map-npy-files.html",
    "href": "posts/2011-01-07-memory-map-npy-files.html",
    "title": "memory map npy files",
    "section": "",
    "text": "Mem-map the stored array, and then access the second row directly from disk:    X = np.load(‘/tmp/123.npy’, mmap_mode=‘r’)"
  },
  {
    "objectID": "posts/2010-08-31-gnome-alt-f2-popup-launcher.html",
    "href": "posts/2010-08-31-gnome-alt-f2-popup-launcher.html",
    "title": "gnome alt f2 popup launcher",
    "section": "",
    "text": "gnome-panel-control –run-dialog"
  },
  {
    "objectID": "posts/2010-08-04-numpy-dtypes-and-fits-keywords.html",
    "href": "posts/2010-08-04-numpy-dtypes-and-fits-keywords.html",
    "title": "numpy dtypes and fits keywords",
    "section": "",
    "text": "bool: ‘L’,  uint8: ‘B’,  int16: ‘I’,  int32: ‘J’,  int64: ‘K’,  float32: ‘E’,  float64: ‘D’,  complex64: ‘C’,  complex128: ‘M’"
  },
  {
    "objectID": "posts/2010-06-30-change-column-name-in-fits-with-pyfits.html",
    "href": "posts/2010-06-30-change-column-name-in-fits-with-pyfits.html",
    "title": "change column name in a fits with pyfits",
    "section": "",
    "text": "no way to change it manipulating the dtype of the data array.   a=pyfits.open(‘filename.fits’)  a[1].header.update(‘TTYPE1’,‘newname’)   you need to change the header, using the update method of the right TTYPE and then write again the fits file using a.writeto."
  },
  {
    "objectID": "posts/2010-06-21-quaternions-for-python.html",
    "href": "posts/2010-06-21-quaternions-for-python.html",
    "title": "quaternions for python",
    "section": "",
    "text": "the situation is pretty problematic, I hope someday  scipy  will add a python package for rotating and interpolating quaternions, up to now: \n\n\n\n\n http://cgkit.sourceforge.net/doc2/quat.html  : slow, bad interaction with numpy, I could not find a simple way to turn a list of N quaternions to a 4xN array without a loop\n\n\n\n http://cxc.harvard.edu/mta/ASPECT/tool_doc/pydocs/Quaternion.html  : more lightweight, does not implement quaternion interpolation"
  },
  {
    "objectID": "posts/2010-03-23-change-permission-recursively-to.html",
    "href": "posts/2010-03-23-change-permission-recursively-to.html",
    "title": "change permission recursively to folders only",
    "section": "",
    "text": "find . -type d -exec chmod 777 {} ;"
  },
  {
    "objectID": "posts/2010-03-03-using-numpy-dtype-with-loadtxt.html",
    "href": "posts/2010-03-03-using-numpy-dtype-with-loadtxt.html",
    "title": "using numpy dtype with loadtxt",
    "section": "",
    "text": "Let’s say you want to read a text file like this:   \n\n\n#filename start end  fdsafda.fits 23143214 23143214  safdsafafds.fits 21423 23423432\n\n      you can use dtype to create a custom array, which is very flexible as you can work by row or columns with strings and floats in the same array:   dt=np.dtype({‘names’:[‘filename’,‘start’,‘end’],‘formats’:[‘S100’,np.float,np.float]})   [I tried also using np.str instead of S100 without success, anyone knows why?]  then give this as input to loadtxt to load the file and create the array.   a = np.loadtxt(open(‘yourfile.txt’),dtype=dt)   so each element is:   (‘dsafsadfsadf.fits’, 1.6287776249537126e+18, 1.6290301584937428e+18)    but you can get the array of start or end times using:   a[‘start’]"
  },
  {
    "objectID": "posts/2010-01-28-correlation.html",
    "href": "posts/2010-01-28-correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "Expectation value  or first moment of a random variable is the probability weighted sum of the possible values (weighted mean).  Expectation value of a 6-dice is 1+2+3+4+5+6 / 6 = 3.5    Covariance  of 2 random variables is:   COV(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY) - E(X)E(Y)   i.e. the difference between the expected value of their product and the product of their expected values.  So if the variables change together, they will have a high covariance, if they are independent, their covariance is zero.    Variance  is the covariance on the same variable, :   COV(X,X)=VAR(X)=E(X2) - E(X)2     Standard deviation  is the square root of Variance    Correlation  is:   COR(X,Y)=COV(X,Y)/STDEV(X)STDEV(Y)      http://mathworld.wolfram.com/Covariance.html"
  },
  {
    "objectID": "posts/2010-01-07-execute-bash-script-remotely-with-ssh.html",
    "href": "posts/2010-01-07-execute-bash-script-remotely-with-ssh.html",
    "title": "execute bash script remotely with ssh",
    "section": "",
    "text": "a bash script launched remotely via ssh does not load the environment, if this is an issue it is necessary to specify –login when calling bash:    ssh user@remoteserver.com ‘bash –login life_om/cronodproc’ | mail your@email.com -s cronodproc"
  },
  {
    "objectID": "posts/2009-12-15-latest-maxima-and-wxmaxima-for-ubuntu.html",
    "href": "posts/2009-12-15-latest-maxima-and-wxmaxima-for-ubuntu.html",
    "title": "Latest Maxima and WxMaxima for Ubuntu Karmic",
    "section": "",
    "text": "http://zeus.nyf.hu/~blahota/maxima/karmic/    on maxima mailing lists they suggested to install the sbcl built, so I first installed sbcl from the Ubuntu repositories and then maxima and wxmaxima f  rom this url."
  },
  {
    "objectID": "posts/2008-09-17-forcefully-unmount-disk-partition.html",
    "href": "posts/2008-09-17-forcefully-unmount-disk-partition.html",
    "title": "forcefully unmount a disk partition",
    "section": "",
    "text": "check which processes are accessing a partition:   [sourcecode language=“python”]lsof | grep ‘/opt’[/sourcecode]   kill all the processes accessing the partition (check what you’re killing, you could loose data):   [sourcecode language=“python”]fuser -km /mnt[/sourcecode]   try to unmount now:  [sourcecode language=“python”]umount /opt[/sourcecode]"
  },
  {
    "objectID": "posts/2008-03-29-decibels-db-and-dbm-in-terms-of-power.html",
    "href": "posts/2008-03-29-decibels-db-and-dbm-in-terms-of-power.html",
    "title": "Decibels, dB and dBm, in terms of Power and Amplitude",
    "section": "",
    "text": "It’s not difficult, just always having some doubts… \n\n\nPower\n\n $latex L_{dB} = 10 log_{10} ( ) $   10 dB increase for a factor 10 increase in the ratio   3 dB = doubling   40 dB = 10000 times \n\nAmplitude\n\n $latex L_{dB} = 10 log_{10} ( ) = 20 log_{10} ( ) $ \n\ndBm\n\n dBm is an absolute value obtained by a ratio with 1 mW:   $latex L_{dBm} = 10 log_{10} ( ) $ \n\n\n\n0 dBm = 1 mW\n\n\n\n3 dBm ≈ 2 mW"
  },
  {
    "objectID": "posts/2008-03-28-producing-pdf-from-xml-files.html",
    "href": "posts/2008-03-28-producing-pdf-from-xml-files.html",
    "title": "Producing PDF from XML files",
    "section": "",
    "text": "I need to produce formatted pdf from XML data input file.  The more standard way looks like to use  XSL stylesheets.   Associating a XSL sheet to an XML file permits most browsers to render them directly as HMTL, this can be used for web publishing XML sheets.   The quick and dirty way to produce PDF could be printing them from Firefox, but an interesting option is to use  xmlto  , a script for running a XSL transformation and render an XML in PDF or other formats. It would be interesting to test this script and understand if it needs just docbook XML input or any XML."
  },
  {
    "objectID": "posts/2006-10-03-using-gnu-find.html",
    "href": "posts/2006-10-03-using-gnu-find.html",
    "title": "using gnu find",
    "section": "",
    "text": "list all the directories excluding “.”: \n\n\nfind . -maxdepth 1 -type d -not -name “.*”\n\n find some string in all files matching a pattern in the subfolders (with grep -r you cannot specify the type of file) \n\nfind . -name ’*.py’ -exec grep -i pdb ‘{}’ ;"
  },
  {
    "objectID": "posts/2006-09-25-tar-quickref.html",
    "href": "posts/2006-09-25-tar-quickref.html",
    "title": "tar quickref",
    "section": "",
    "text": "compress: tar cvzf foo.tgz .cc .h  check inside: tar tzf foo.tgz | grep file.txt  extract: tar xvzf foo.tgz  extract 1 file only: tar xvzf foo.tgz path/to/file.txt"
  },
  {
    "objectID": "posts/2006-09-22-software-libero-per-il-trattamento-di.html",
    "href": "posts/2006-09-22-software-libero-per-il-trattamento-di.html",
    "title": "Software libero per il trattamento di dati scientifici",
    "section": "",
    "text": "nella ricerca del miglior ambiente per analisi di dati scientifici da leggere questi articoli:   http://www.pluto.it/files/journal/pj0501/swlibero-scie1.html   http://www.pluto.it/files/journal/pj0504/swlibero-scie2.html   http://www.pluto.it/files/journal/pj0505/swlibero-scie3.html"
  },
  {
    "objectID": "posts/2006-09-22-awk-made-easy.html",
    "href": "posts/2006-09-22-awk-made-easy.html",
    "title": "awk made easy",
    "section": "",
    "text": "awk ’/REGEX/ {print NR “ $9” $4”_“\\(5 ;}' file.txt\n&lt;/strong&gt;\n&lt;br/&gt;\nsupports extended REGEX like perl (       e.g. [:blank:]  Space or tab characters )\n&lt;br/&gt;\nNR is line number\n&lt;br/&gt;\nNF                Number of fields\n&lt;br/&gt;\\)n is the column to be printed, \\(0 is the whole row\n&lt;br/&gt;\n&lt;br/&gt;\nif it only necessary to print columns of a file it is easier to use cut:\n&lt;br/&gt;\n&lt;br/&gt;\nname -a | cut -d\" \" -f1,3,11,12\n&lt;br/&gt;\n&lt;br/&gt;\n-d: or -d\" \" is the delimiter\n&lt;br/&gt;\n-f1,3 are the fields to be displayed\n&lt;br/&gt;\nother options: -s doesnt show lines without delimiters, --complement is selfesplicative\n&lt;br/&gt;\ncondition on a specific field:\n&lt;br/&gt;\\)&lt;field&gt; ~ /&lt;string&gt;/ Search for string in specified field.   you can use awk also in pipes:  ll | awk ‘NR!=1 {s+=$5} END {print “Average:” s/(NR-1)}’  END to process al file and then print results   tutorial on using awk from the command line:   http://www.vectorsite.net/tsawk_3.html#m1"
  },
  {
    "objectID": "posts/2006-09-20-pillole-di-astrofisica.html",
    "href": "posts/2006-09-20-pillole-di-astrofisica.html",
    "title": "pillole di astrofisica",
    "section": "",
    "text": "curiosita’ ben spiegate da annibale d’ercole, interessante l’idea di avere un livello base e un livello avanzato   http://www.bo.astro.it/sait/spigolature/spigostart.html"
  },
  {
    "objectID": "posts/2006-09-22-command-line-processing.html",
    "href": "posts/2006-09-22-command-line-processing.html",
    "title": "command line processing",
    "section": "",
    "text": "Very useful summary of many linux command line processing tools (great perl onliners)   http://grad.physics.sunysb.edu/~leckey/personal/forget/"
  },
  {
    "objectID": "posts/2006-09-25-software-carpentry.html",
    "href": "posts/2006-09-25-software-carpentry.html",
    "title": "software carpentry",
    "section": "",
    "text": "basic software for scientists and engineers:  http://www.swc.scipy.org/"
  },
  {
    "objectID": "posts/2006-10-03-beginners-bash-guide.html",
    "href": "posts/2006-10-03-beginners-bash-guide.html",
    "title": "beginners bash guide",
    "section": "",
    "text": "great guide with many examples:   http://tille.xalasys.com/training/bash/"
  },
  {
    "objectID": "posts/2006-10-17-vim-costumization.html",
    "href": "posts/2006-10-17-vim-costumization.html",
    "title": "vim costumization",
    "section": "",
    "text": "it is about perl but it suggests very useful tricks for programming with vim  http://mamchenkov.net/wordpress/2004/05/10/vim-for-perl-developers/"
  },
  {
    "objectID": "posts/2008-03-28-relation-between-power-density-and.html",
    "href": "posts/2008-03-28-relation-between-power-density-and.html",
    "title": "Relation between Power density and temperature in an antenna",
    "section": "",
    "text": "Considering an antenna placed inside a blackbody enclosure at temperature T, the power received per unit bandwidth is:  \\(latex \\omega = kT\\)   where k is Boltzmann constant.   This relationship derives from considering a constant brightness \\(latex B\\) in all directions, therefore Rayleigh Jeans law tells:   \\(latex B = \\dfrac{2kT}{\\lambda^2}\\)   Power per unit bandwidth is obtained by integrating brightness over antenna beam   $latex = A_e B ( , ) P_n ( , ) d $   therefore   $latex = A_e_A $   where: \n\n\n\n\n\\(latex A_e\\) is antenna effective aperture\n\n\n\n\\(latex \\Omega_A\\) is antenna beam area\n\n\n\n $latex ^2 = A_e_A $ another post should talk about this   finally:   $latex = kT $   which is the same noise power of a resistor.   source : Kraus Radio Astronomy pag 107"
  },
  {
    "objectID": "posts/2008-04-29-netcat-quickly-send-binaries-through.html",
    "href": "posts/2008-04-29-netcat-quickly-send-binaries-through.html",
    "title": "netcat, quickly send binaries through network",
    "section": "",
    "text": "just start nc in server mode on localhost:   [sourcecode language=‘python’] nc -l -p 3333 [/sourcecode]   send a string to localhost on port 3333:   [sourcecode language=‘python’] echo “hello world” | nc localhost 3333 [/sourcecode]   you’ll see on server side appearing the string you sent.   very useful for sending binaries, see  examples  ."
  },
  {
    "objectID": "posts/2009-12-10-number-of-files-in-folder-and-subfolders.html",
    "href": "posts/2009-12-10-number-of-files-in-folder-and-subfolders.html",
    "title": "number of files in a folder and subfolders",
    "section": "",
    "text": "folders are not counted   find . -type f | wc -l"
  },
  {
    "objectID": "posts/2010-01-05-load-arrays-from-text-file-with-numpy.html",
    "href": "posts/2010-01-05-load-arrays-from-text-file-with-numpy.html",
    "title": "load arrays from a text file with numpy",
    "section": "",
    "text": "space separated text file with 5 arrays in columns:   [sourcecode language=“python”]  ods,rings,gains,offsets,rparams = np.loadtxt(filename,unpack=True)  [/sourcecode]   quite impressive…"
  },
  {
    "objectID": "posts/2010-01-07-lock-pin-hold-package-using-apt-on.html",
    "href": "posts/2010-01-07-lock-pin-hold-package-using-apt-on.html",
    "title": "lock pin hold a package using apt on ubuntu",
    "section": "",
    "text": "set hold:   echo packagename hold | dpkg –set-selections    check, should be  hi  :   dpkg -l packagename    unset hold:   echo packagename install | dpkg –set-selections"
  },
  {
    "objectID": "posts/2010-02-19-stop-ipcluster-from-script.html",
    "href": "posts/2010-02-19-stop-ipcluster-from-script.html",
    "title": "Stop ipcluster from a script",
    "section": "",
    "text": "Ipcluster is easy to start but not trivial to stop from a script, after having finished the processing, here’s the solution:   from IPython.kernel import client  mec = client.MultiEngineClient()  mec.kill(controller=True)"
  },
  {
    "objectID": "posts/2010-03-16-aptitude-search.html",
    "href": "posts/2010-03-16-aptitude-search.html",
    "title": "aptitude search ‘and’",
    "section": "",
    "text": "this is really something  really annoying  about aptitude, if you run:   aptitude search linux headers   it will make an ‘or’ search…to perform a ‘and’ search, which I need 99.9% of the time, you need quotation marks:   aptitude search ‘linux headers’"
  },
  {
    "objectID": "posts/2010-06-21-parallel-computing-python-way.html",
    "href": "posts/2010-06-21-parallel-computing-python-way.html",
    "title": "parallel computing the python way",
    "section": "",
    "text": "forget MPI:   http://showmedo.com/videotutorials/series?name=N49qyIFOh"
  },
  {
    "objectID": "posts/2010-06-23-healpix-coordinates.html",
    "href": "posts/2010-06-23-healpix-coordinates.html",
    "title": "healpix coordinates",
    "section": "",
    "text": "Healpix considers  latitude  theta from 0 on north pole to pi south pole,  so the conversion is:   theta = pi/2 - latitude    longitude  and phi instead are consistently from 0 to 2*pi with \n\n\n\n\nzero on vernal equinox (for  ecliptic  ).\n\n\n\nzero in the direction from Sun to galactic center (for  galactic  )"
  },
  {
    "objectID": "posts/2010-07-23-count-hits-with-numpy.html",
    "href": "posts/2010-07-23-count-hits-with-numpy.html",
    "title": "count hits with numpy",
    "section": "",
    "text": "I have an array where I record hits   a=np.zeros(5)   and an array with the indices of the hits, for example I have 2 hits on index 2   hits=np.array([2,2])   so I want to increase index 2 of a by 2     I tried:   a[hits]+=1   but it gives array([ 0., 0., 1., 0., 0.])  does someone have a suggestion?   bins=np.bincount(hits)  a[:len(bins)] += bins  a  array([ 0., 0., 2., 0., 0.])"
  },
  {
    "objectID": "posts/2010-08-21-switch-to-interactive-backend-with.html",
    "href": "posts/2010-08-21-switch-to-interactive-backend-with.html",
    "title": "switch to interactive backend with ipython -pylab",
    "section": "",
    "text": "objective: \n\n\n\n\nwhen running ipython without pylab or executing scripts you want to use an image matplotlib backend like Agg\n\n\n\njust when calling ipython -pylab you want to use an interactive backend like GTKAgg or TKAgg\n\n\n\n     you need first to setup as default backend on .matplotlib/matplotlibrc  Agg  :   backend : Agg   then setup you ipython to switch to interactive, in ipython file Shell.py, in the class MatplotlibShellBase, at about line 516, add:   matplotlib.use(‘GTKAgg’)   after the first import of matplotlib"
  },
  {
    "objectID": "posts/2010-12-03-force-local-install-of-python-module.html",
    "href": "posts/2010-12-03-force-local-install-of-python-module.html",
    "title": "force local install of python module",
    "section": "",
    "text": "python setup.py install –prefix FOLDER     creates lib/python2.6/site-packages, to force a local install you should use:    python setup.py install –install-lib FOLDER"
  },
  {
    "objectID": "posts/2011-02-02-git-make-local-branch-tracking-origin.html",
    "href": "posts/2011-02-02-git-make-local-branch-tracking-origin.html",
    "title": "git make local branch tracking origin",
    "section": "",
    "text": "git branch –set-upstream master origin/master    you obtain the same result as initial cloning"
  },
  {
    "objectID": "posts/2011-03-28-pyfits-memory-leak-in-newtable.html",
    "href": "posts/2011-03-28-pyfits-memory-leak-in-newtable.html",
    "title": "pyfits memory leak in new_table",
    "section": "",
    "text": "I found a memory leakage issue in pyfits.new_table, data were NOT deleted when the table was deleted, I prepared a test on github, using  objgraph  , which shows that data are still in memory:     https://gist.github.com/884298    the issue was solved by Erik Bray of STSCI on March 28th, 2011 , see bug report:   http://trac6.assembla.com/pyfits/ticket/49   and changeset:   http://trac6.assembla.com/pyfits/changeset/844"
  },
  {
    "objectID": "posts/2011-04-29-vim-regular-expressions.html",
    "href": "posts/2011-04-29-vim-regular-expressions.html",
    "title": "Vim regular expressions",
    "section": "",
    "text": "very good reference of the usage of regular expressions in VIM:    http://www.softpanorama.org/Editors/Vimorama/vim_regular_expressions.shtml"
  },
  {
    "objectID": "posts/2011-06-21-cfitsio-wrapper-in-python.html",
    "href": "posts/2011-06-21-cfitsio-wrapper-in-python.html",
    "title": "cfitsio wrapper in python",
    "section": "",
    "text": "After several issues with pyfits, and tired of it being so overengineered, I’ve wrote my own fits I/O package in python, wrapping the C library cfitsio with ctypes.   Pretty easy, first version completely developed in 1 day.    https://github.com/zonca/pycfitsio"
  },
  {
    "objectID": "posts/2011-11-01-some-python-resources.html",
    "href": "posts/2011-11-01-some-python-resources.html",
    "title": "some python resources",
    "section": "",
    "text": "python tutorial:   http://docs.python.org/tutorial/   numpy tutorial [arrays]:   http://www.scipy.org/Tentative_NumPy_Tutorial   plotting tutorial:   http://matplotlib.sourceforge.net/users/pyplot_tutorial.html    free online books:   http://diveintopython.org/toc/index.html    http://www.ibiblio.org/swaroopch/byteofpython/read/    install enthought python:   http://www.enthought.com/products/edudownload.php    video tut:  http://www.youtube.com/watch?v=YW8jtSOTRAU&feature=channel"
  },
  {
    "objectID": "posts/2012-08-16-doctests-and-unittests-happiness-2.html",
    "href": "posts/2012-08-16-doctests-and-unittests-happiness-2.html",
    "title": "doctests and unittests happiness 2",
    "section": "",
    "text": "nosetests -v –with-doctest  Doctest: healpy.pixelfunc.ang2pix … ok  Doctest: healpy.pixelfunc.get_all_neighbours … ok  Doctest: healpy.pixelfunc.get_interp_val … ok  Doctest: healpy.pixelfunc.get_map_size … ok  Doctest: healpy.pixelfunc.get_min_valid_nside … ok  Doctest: healpy.pixelfunc.get_neighbours … ok\n\n   \n\nDoctest: healpy.pixelfunc.isnpixok … ok  Doctest: healpy.pixelfunc.isnsideok … ok  Doctest: healpy.pixelfunc.ma … ok  Doctest: healpy.pixelfunc.maptype … ok  Doctest: healpy.pixelfunc.mask_bad … ok  Doctest: healpy.pixelfunc.mask_good … ok  Doctest: healpy.pixelfunc.max_pixrad … ok  Doctest: healpy.pixelfunc.nest2ring … ok  Doctest: healpy.pixelfunc.npix2nside … ok  Doctest: healpy.pixelfunc.nside2npix … ok  Doctest: healpy.pixelfunc.nside2pixarea … ok  Doctest: healpy.pixelfunc.nside2resol … ok  Doctest: healpy.pixelfunc.pix2ang … ok  Doctest: healpy.pixelfunc.pix2vec … ok  Doctest: healpy.pixelfunc.reorder … ok  Doctest: healpy.pixelfunc.ring2nest … ok  Doctest: healpy.pixelfunc.ud_grade … ok  Doctest: healpy.pixelfunc.vec2pix … ok  Doctest: healpy.rotator.Rotator … ok  test_write_map_C (test_fitsfunc.TestFitsFunc) … ok  test_write_map_IDL (test_fitsfunc.TestFitsFunc) … ok  test_write_alm (test_fitsfunc.TestReadWriteAlm) … ok  test_write_alm_256_128 (test_fitsfunc.TestReadWriteAlm) … ok  test_ang2pix_nest (test_pixelfunc.TestPixelFunc) … ok  test_ang2pix_ring (test_pixelfunc.TestPixelFunc) … ok  test_nside2npix (test_pixelfunc.TestPixelFunc) … ok  test_nside2pixarea (test_pixelfunc.TestPixelFunc) … ok  test_nside2resol (test_pixelfunc.TestPixelFunc) … ok  test_inclusive (test_query_disc.TestQueryDisc) … ok  test_not_inclusive (test_query_disc.TestQueryDisc) … ok  test_anafast (test_sphtfunc.TestSphtFunc) … ok  test_anafast_iqu (test_sphtfunc.TestSphtFunc) … ok  test_anafast_xspectra (test_sphtfunc.TestSphtFunc) … ok  test_synfast (test_sphtfunc.TestSphtFunc) … ok  test_cartview_nocrash (test_visufunc.TestNoCrash) … ok  test_gnomview_nocrash (test_visufunc.TestNoCrash) … ok  test_mollview_nocrash (test_visufunc.TestNoCrash) … ok   ———————————————————————-  Ran 43 tests in 19.077s   OK"
  },
  {
    "objectID": "posts/2012-09-27-ipythonparallel-for-planck-data.html",
    "href": "posts/2012-09-27-ipythonparallel-for-planck-data.html",
    "title": "IPython.parallel for Planck data analysis at NERSC",
    "section": "",
    "text": "Planck  is a Space mission for high precision measurements of the  Cosmic Microwave Background  (CMB), data are received as timestreams of output voltages from the 2 instruments on-board, the Low and High Frequency Instruments [LFI / HFI].   The key phase in data reduction is map-making, where data are binned to a map of the microwave emission of our galaxy, the CMB, and extragalactic sources. This phase is intrinsically parallel and requires simultaneous access to all the data, so requires a fully parallel MPI-based software.   However, preparing the data for map-making requires several tasks that are serial, but are data and I/O intensive, therefore need to be parallelized.      IPython.parallel offers the easiest solution for managing a large amount of trivially parallel jobs.   The first task is pointing reconstruction, where we interpolate and apply several rotations and corrections to low-sampled satellite quaternions stored on disk and then write the output dense detector pointing to disk.  The disk quota of pointing files is about 2.5TB split in about 3000 files, those files can be processed independently, therefore we implement a function that processes 1 file, to be used interactively for debugging and testing.  Then launch an IPython cluster, typically between 20 and 300 engines on Carver (NERSC), and use the exact same function to process all the ~3000 files in parallel.  The IPython  BalancedView  controller automatically balances the queue therefore we get maximum efficiency, and it is possible to leave the cluster running and submit other instances of the job to be added to its queue.   Second task is calibration and dipole removal, which processes about 1.2 TB of data, but it needs to read the dense pointing from disk, so it is very I/O intensive. Also in this case we can submit the ~3000 jobs to an IPython.parallel cluster.   In a next post I’ll describe in detail my setup and how I organize my code to make it easy to swap back and forth between debugging code interactively and  running production runs in parallel."
  },
  {
    "objectID": "posts/2012-12-17-ubuntu-ppa-for-healpix-and-healpy.html",
    "href": "posts/2012-12-17-ubuntu-ppa-for-healpix-and-healpy.html",
    "title": "Ubuntu PPA for HEALPix and healpy",
    "section": "",
    "text": "HEALPix C, C++  version 3.00 and  healpy  version 1.4.1 are now available in a PPA repository for Ubuntu 12.04 Precise and Ubuntu 12.10 Quantal.   First remove your previous version of  healpy  , just find the location of the package:    &gt; python -c “import healpy; print healpy.__file__”    and remove it:    &gt; sudo rm -r /some-base-path/site-packages/healpy*  \n\n  \n\n Then add the apt repository and install the packages:  \n\n  \n\n &gt; sudo add-apt-repository ppa:zonca/healpix    &gt; sudo apt-get update    &gt; sudo apt-get install healpix-cxx libhealpix-cxx-dev     libchealpix0   libchealpix-dev python-healpy   \n\n\n &gt; which anafast_cxx \n\n\n /usr/bin/anafast_cxx \n\n\n\n  \n\n &gt; python -c “import healpy; print healpy.__version__” \n\n  \n\n 1.4.1"
  },
  {
    "objectID": "posts/2013-01-28-tag-blogger.html",
    "href": "posts/2013-01-28-tag-blogger.html",
    "title": "Compile healpix C++ to javascript",
    "section": "",
    "text": "Compile C++ -&gt; LLVM with clang   Convert LLVM -&gt; Javascript:   https://github.com/kripken/emscripten/wiki/Tutorial"
  },
  {
    "objectID": "posts/2013-03-12-interactive-3d-plot-of-sky-map.html",
    "href": "posts/2013-03-12-interactive-3d-plot-of-sky-map.html",
    "title": "Interactive 3D plot of a sky map",
    "section": "",
    "text": "update May 2024: See the new version of this tutorial\n Mayavi  is a Python package from Enthought for 3D visualization, here a simple example of creating a 3D interactive map starting from a HEALPix pixelization sky map: \n\n\n\n\n  \n\n\n\n\n    Here the code: \n\n  The output is a beautiful 3D interactive map, Mayavi allows to pan, zoom and rotate.  UPDATE 13 Mar: actually there was a bug (found by Marius Millea) in the script, there is no problem in the projection! \n\n\n\nMayavi can be installed in Ubuntu installing  python-vtk  and then  sudo pip install mayavi."
  },
  {
    "objectID": "posts/2013-04-08-noise-in-spectra-and-map-domain.html",
    "href": "posts/2013-04-08-noise-in-spectra-and-map-domain.html",
    "title": "Noise in spectra and map domain",
    "section": "",
    "text": "Spectra\n\nNET or \\(\\sigma\\) is the standard deviation of the noise, measured in mK/sqrt(Hz), typical values for microwave amplifiers are 0.2-5.  This is the natural unit of the amplitude spectra (ASD), therefore the high frequency tail of the ASD should get to the expected value of the NET.  NET can also be expressed in mKsqrt(s), which is NOT the same unit.   mK/sqrt(Hz)  refers to an integration bandwidth of 1 Hz that assumes a 6dB/octave rolloff, its integration time is only about 0.5 seconds.   mK/sqrt(s)  instead refers to integration time of 1 second, therefore assumes a top hat bandpass.  Therefore there is a factor of sqrt(2) difference between the two conventions, therefore mK/sqrt(Hz) = sqrt(2) * mK sqrt(s)  See appendix B of Noise Properties of the Planck-LFI Receivers   http://arxiv.org/abs/1001.4608  \n\nMaps\n\nTo estimate the map domain noise instead we need to integrate the sigma over the time per pixel; in this case it is easier to convert the noise to sigma/sample, therefore we need to multiply by the square root of the sampling frequency:   sigma_per_sample = NET * sqrt(sampling_freq)   Then the variance per pixel is sigma_per_sample**2/number_of_hits \n\nAngular power spectra\n\n\n\\(C_\\ell\\) of the variance map is just the variance map multiplied by the pixel area divided by the integration time.   \\[C_\\ell = \\Omega_{\\rm pix} \\langle \\frac{\\sigma^2}{\\tau} \\rangle = \\Omega_{\\rm pix} \\langle \\frac{\\sigma^2 f_{\\rm samp}}{hits} \\rangle\\]"
  },
  {
    "objectID": "posts/2013-04-11-ipython-parallell-setup-on-carver-at.html",
    "href": "posts/2013-04-11-ipython-parallell-setup-on-carver-at.html",
    "title": "IPython parallell setup on Carver at NERSC",
    "section": "",
    "text": "IPython parallel is one of the easiest ways to spawn several Python sessions on a Supercomputing cluster and process jobs in parallel.   On Carver, the basic setup is running a controller on the login node, and submit engines to the computing nodes via PBS.      First create your configuration files running:    ipython profile create –parallel    Therefore in the ~/.config/ipython/profile_default/ipcluster_config.py, just need to set:    c.IPClusterStart.controller_launcher_class = ‘LocalControllerLauncher’    c.IPClusterStart.engine_launcher_class = ‘PBS’    c.PBSLauncher.batch_template_file = u’~/.config/ipython/profile_default/pbs.engine.template’    You also need to allow connections to the controller from other hosts, setting  in ~/.config/ipython/profile_default/ipcontroller_config.py:    c.HubFactory.ip = ’*’  \n\n\n\n\nWith the path to the pbs engine template.   Next a couple of examples of pbs templates, for 2 or 8 processes per node:\n\n IPython configuration does not seem to be flexible enough to add a parameter for specifying the processes per node.  So I just created a bash script that get as parameters the processes per node and the total number of nodes:    ipc 8 2 # 2 nodes with 8ppn, 16 total engines    ipc 2 3 # 3 nodes with 2ppn, 6 total engines     Once the engines are running, jobs can be submitted opening an IPython shell on the login node and run:        from IPython.parallel import Client    rc = Client()     lview = rc.load_balanced_view() # default load-balanced view  \n\n def serial_func(argument): \n\n\n pass \n\n\n parallel_result = lview.map(serial_func, list_of_arguments) \n\n\n\n\n\n\n The serial function is sent to the engines and executed for each element of the list of arguments. \n\n\n If the function returns a value, than it is transferred back to the login node. \n\n\n In case the returned values are memory consuming, is also possible to still run the controller on the login node, but execute the interactive IPython session in an interactive job."
  },
  {
    "objectID": "posts/2013-07-15-processing-planck-sources-with-hadoop.html",
    "href": "posts/2013-07-15-processing-planck-sources-with-hadoop.html",
    "title": "Processing sources in Planck maps with Hadoop and Python",
    "section": "",
    "text": "Purpose\n\n\nThe purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software.\n\n\nHadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet.\n\n\nIt allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them (“map” step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the “reduce” algorithm, which typically aggregates them and then save them to the distributed file-system.\n\n\nHadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features.\n\n\nMost scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial.\n\n\n \n\n\nProblem description\n\n\nThe Planck collaboration (btw I’m part of it…) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources:\n\n\n http://irsa.ipac.caltech.edu/Missions/planck.html \n\n\nEach catalog contains about 1000 sources, and the collaboration released the location and flux of each source.\n\n\nThe purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source.\n\n\nIn a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy.\n\n\nSources\n\nAll files are available on github: \n\n https://github.com/zonca/planck-sources-hadoop \n\n\nHadoop setup\n\n\nI am running on the San Diego Supercomputing data intensive cluster Gordon:\n\n\n http://www.sdsc.edu/us/resources/gordon/ \n\n\nSDSC has a simplified Hadoop setup based on shell scripts,  myHadoop  , which allows running Hadoop as a regular PBS job.\n\n\nThe most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon.\n\n\nUsing Python with Hadoop-streaming\n\n\nHadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language.\n\n\nOne of the most common choices for scientific applications is Python.\n\n\nApplication design\n\n\nBest way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region.\n\n\nTherefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature (  preprocessing.py  ).\n\n\nOf course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that’s something we do not worry about, that is exactly what Hadoop takes care of.\n\n\nImplementation\n\n\nInput data\n\n\nThe pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes.\n\n\nTherefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row.\n\n\nMapper\n\n\n mapper.py \n\n\n\n\n\nThe mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source.\n\n\nIn this simple scenario, the only returned key printed to stdout is “SOURCENAME_10arcminmean”.\n\n\nFor example, we can run a serial test by running:\n\n\n\n\n\n\n echo plancktest/submaps/030_045_025 | ./mapper.py \n\n\n\n  \n\n\n and the returned output is: \n\n\n  \n\n\n\n PCCS1 030 G023.00+40.77_10arcminmean   4.49202e-04 \n\n\n PCCS1 030 G023.13+42.14_10arcminmean   3.37773e-04 \n\n\n PCCS1 030 G023.84+45.26_10arcminmean   4.69427e-04 \n\n\n PCCS1 030 G024.32+48.81_10arcminmean   3.79832e-04 \n\n\n PCCS1 030 G029.42+43.41_10arcminmean   4.11600e-04 \n\n\n\n\n\n\nReducer\n\n\nThere is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file.\n\n\nHadoop call\n\n\n run.pbs \n\n\n\n\n\nThe hadoop call is:\n\n\n\n\n\n\n  $HADOOP_HOME/bin/hadoop –config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoopstreaming.jar -file $FOLDER/mapper.py -mapper \\(FOLDER/mapper.py -input /user/\\)USER/Input/* -output /user/$USER/Output  \n\n\n\n\n\n\nSo we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs.\n\n\nHadoop run and results\n\n\nFor testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes.\n\n\nProcessing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes.\n\n\nThe outputs of the mappers are then joined, sorted and written on a single file, see the output file\n\n\n output/SAMPLE_RESULT_part-00000  .\n\n\nSee the full log  sample_logs.txt  extracted running:\n\n\n /opt/hadoop/bin/hadoop job -history output \n\n\n Comparison of the results with the catalog \n\n\n Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess. \n\n\n\n\n  \n\n\nConclusion\n\n\nThe advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop.\n\n\nThe main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming."
  },
  {
    "objectID": "posts/2013-08-08-healpix-map-of-earth-using-healpy.html",
    "href": "posts/2013-08-08-healpix-map-of-earth-using-healpy.html",
    "title": "HEALPix map of the Earth using healpy",
    "section": "",
    "text": "HEALPix maps can also be used to create equal-area pixelized maps of the Earth, RGB colors are not supported in healpy, so we need to convert the image to colorscale.  The best user case is for using spherical harmonic transforms, e.g. apply a smoothing filter, in this case HEALPix/healpy tools are really efficient.  However, other tools for transforming between angles (coordinates), 3d vectors and pixels might be useful.      I’ve created an IPython notebook that provides a simple example:    http://nbviewer.ipython.org/6187504    Here is the output Mollweide projection provided by healpy:  \n\n\n  \n\n Few notes:  \n\n\n\n\n\n\nalways use  flip=“geo”  for plotting, otherwise maps are flipped East-West\n\n\nincrease the resolution of the plots (which is different from the resolution of the map array) by providing at least xsize=2000 to mollview and a reso lower than 1 to gnomview"
  },
  {
    "objectID": "posts/2013-08-30-interactive-figures-planck-power-spectra.html",
    "href": "posts/2013-08-30-interactive-figures-planck-power-spectra.html",
    "title": "Interactive figures in the browser - CMB Power Spectra",
    "section": "",
    "text": "For a long time I’ve been curious about trying out  d3.js  , the javascript plotting library which is becoming the standard for interactive plotting in the browser. \n\n\n\n\n\nWhat is really appealing is the capability of sharing with other people powerful interactive visualization simply via the link to a web page. This will hopefully be the future of scientific publications, as envisioned, for example, by  Authorea  .\n\n\n  An interesting example related to my work on Planck is a plot of the high number of Angular Power Spectra of the anisotropies of the Cosmic Microwave Background Temperature.\n\n\nThe CMB Power spectra describe how the temperature fluctuations were distributed in the sky as a function of the angular scale, for example the largest peak at about 1 degree means that the brightest cold/warm spots of the CMB have that angular size, see  The Universe Simulator in the browser  .\n\n\nThe  Planck Collaboration released  a combined spectrum, which aggregates several channels to give the best result, spectra frequency by frequency (for some frequencies split in detector-sets) and a best-fit spectrum given a Universe Model.\n\n\nIt is also interesting to compare to the latest release spectrum by WMAP with 9 years of data.\n\n\n\n\n\nThe plan is to create a visualization where it is easier to zoom to different angular scales on the horizontal axis and quickly show/hide each curve.\n\n\nFor this I used  rickshaw  , a library based on  d3.js   which makes it easier to create time-series plots. \n\n\n In fact most of the features are already implemented, it is just a matter of configuring them, see the code on github:   https://github.com/zonca/visualize-planck-cl \n\n\nThe most complex task is actually to load all the data, previously converted to JSON, in the background from the server and push them in a data structure which is understood by rickshaw.\n\n\n\n\n\nCheck out the result:\n\n\n  http://bit.ly/planck-spectra"
  },
  {
    "objectID": "posts/2013-09-10-Planck-CMB-map-at-high-resolution.html",
    "href": "posts/2013-09-10-Planck-CMB-map-at-high-resolution.html",
    "title": "Planck CMB map at high resolution",
    "section": "",
    "text": "Prompted by a colleague, I created a high-resolution version of the Cosmic Microwave Background map in MollWeide projection released by the Planck collaboration, available on the Planck Data Release Website in FITS format.\nThe map is a PNG at a resolution of 17469x8796 pixels, which is suitable for printing at 300dpi up to 60x40 inch, or 150x100 cm, file size is about 150MB.\nUpdate: now with Planck color scale\nUpdate: previous version had grayed out pixels in the galactic plane represents the fraction of the sky that is not possible to reconstruct due to bright galactic sources. The last version uses inpainting to create a constrained CMB realization with the same statistics as the observed CMB to fill the unobserved pixels, more details in the Planck Explanatory Supplement.\n\nHigh Resolution image on FigShare\nSmall size preview:\n\n\n\n\nPreview of Planck CMB map\n\n\n\nPython code:"
  },
  {
    "objectID": "posts/2013-09-26-automatically-build-pelican-and-publish-to-github-pages.html",
    "href": "posts/2013-09-26-automatically-build-pelican-and-publish-to-github-pages.html",
    "title": "How to automatically build your Pelican blog and publish it to Github Pages",
    "section": "",
    "text": "Something I like a lot about Jekyll, the Github static blog generator, is that you just push commits to your repository and Github takes care of re-building and publishing your website. Thanks to this, it is possible to create a quick blog post from the Github web interface, without the need to use a machine with Python environment.\nThe Pelican developers have a method for building and deploying Pelican on Heroku, which is really useful, but I would like instead to use Github Pages.\nI realized that the best way to do this is to rely on Travis-CI, as the build/deploy workflow is pretty similar to install/unit-testing Travis is designed for."
  },
  {
    "objectID": "posts/2013-09-26-automatically-build-pelican-and-publish-to-github-pages.html#how-to-setup-pelican-to-build-on-travis",
    "href": "posts/2013-09-26-automatically-build-pelican-and-publish-to-github-pages.html#how-to-setup-pelican-to-build-on-travis",
    "title": "How to automatically build your Pelican blog and publish it to Github Pages",
    "section": "How to setup Pelican to build on Travis",
    "text": "How to setup Pelican to build on Travis\nI suggest to use 2 separate git repositories on Github for the source and the built website, let’s first only create the repository for the source:\n\ncreate the yourusername.github.io-source repository for Pelican and add it as origin in your Pelican folder repository\n\nadd a requirements.txt file in your Pelican folder:\ngithub:zonca/zonca.github.io-source/requirements.txt\nadd a .travis.yml file to your repository:\ngithub:zonca/zonca.github.io-source/.travis.yml\nIn order to create the encrypted token under env, you can login to the Github web interface to get an Authentication Token, and then install the travis command line tool with:\n# on Ubuntu you need ruby dev\nsudo apt-get install ruby1.9.1-dev\nsudo gem install travis\nand run from inside the repository:\ntravis encrypt GH_TOKEN=LONGTOKENFROMGITHUB --add env.global\nThen add also the deploy.sh script and update the global variable with yours:\ngithub:zonca/zonca.github.io-source/deploy.sh\nThen we can create the repository that will host the actual blog:\n\ncreate the yourusername.github.io repository for the website (with initial readme, so you can clone it)\n\nFinally we can connect to Travis-CI, connect our Github profile and activate Continous Integration on our yourusername.github.io-source repository.\nNow, you can push a new commit to your source repository and check on Travis if the build and deploy is successful, hopefully it is (joking, no way it is going to work on the first try!)."
  },
  {
    "objectID": "posts/2013-10-01-how-to-log-exceptions-in-python.html",
    "href": "posts/2013-10-01-how-to-log-exceptions-in-python.html",
    "title": "How to log exceptions in Python",
    "section": "",
    "text": "Sometimes it is useful to just catch any exception, write details to a log file and continue execution.\nIn the Python standard library, it is possible to use the logging and exceptions modules to achieve this. First of all, we want to catch any exception, but also being able to access all information about it:\ntry:\n    my_function_1()\nexcept exception.Exception as e:\n    print e.__class__, e.__doc__, e.message\nThen we want to write those to a logging file, so we need to setup the logging module:\nimport logging\nlogging.basicConfig( filename=\"main.log\",\n                     filemode='w',\n                     level=logging.DEBUG,\n                     format= '%(asctime)s - %(levelname)s - %(message)s',\n                   )\nIn the following gist everything together, with also function name detection from Alex Martelli:\n\nHere the output log:\n2013-10-01 11:32:56,466 - ERROR - Function my_function_1() raised &lt;type 'exceptions.IndexError'&gt; (Sequence index out of range.): Some indexing error\n2013-10-01 11:32:56,466 - ERROR - Function my_function_2() raised &lt;class 'my_module.MyException'&gt; (This is my own Exception): Something went quite wrong\n2013-10-01 11:32:56,466 - ERROR - Function my_function_1_wrapper() raised &lt;type 'exceptions.IndexError'&gt; (Sequence index out of range.): Some indexing error"
  },
  {
    "objectID": "posts/2013-11-20-published-paper-destriping-CMB-polarimeter.html",
    "href": "posts/2013-11-20-published-paper-destriping-CMB-polarimeter.html",
    "title": "Published paper on Destriping Cosmic Microwave Background Polarimeter data",
    "section": "",
    "text": "TL;DR version:\n\nPreprint on arxiv: Destriping Cosmic Microwave Background Polarimeter data\nDestriping python code on github: dst\nOutput maps and sample input data on figshare: BMachine 40GHz CMB Polarimeter sky maps\n(Paywalled published paper: Destriping Cosmic Microwave Background Polarimeter data)\n\nMy last paper was published by Astronomy and Computing.\nThe paper is focused on Cosmic Microwave Background data destriping, a map-making tecnique which exploits the fast scanning of instruments in order to efficiently remove correlated low frequency noise, generally caused by thermal fluctuations and gain instability of the amplifiers.\nThe paper treats in particular the case of destriping data from a polarimeter, i.e. an instrument which directly measures the polarized signal from the sky, which allows some simplification compared to the case of a simply polarization-sensitive radiometer.\nI implemented a fully parallel python implementation of the algorithm based on:\n\nPyTrilinos for Distributed Linear Algebra via MPI\nHDF5 for I/O\ncython for improving the performance of the inner loops\n\nThe code is available on Github under GPL.\nThe output maps for about 30 days of the UCSB B-Machine polarimeter at 37.5 GHz are available on FigShare.\nThe experience of publishing with ASCOM was really positive, I received 2 very helpful reviews that drove me to work on several improvements on the paper."
  },
  {
    "objectID": "posts/2013-12-18-run-ipython-notebook-on-HPC-cluster-via-PBS.html",
    "href": "posts/2013-12-18-run-ipython-notebook-on-HPC-cluster-via-PBS.html",
    "title": "Run IPython Notebook on a HPC Cluster via PBS",
    "section": "",
    "text": "The IPython notebook is a great tool for data exploration and visualization. It is suitable in particular for analyzing a large amount of data remotely on a computing node of a HPC cluster and visualize it in a browser that runs on a local machine. In this configuration, the interface is local, it is very responsive, but the amount of memory and CPU horsepower is provided by a HPC computing node.\nAlso, it is possible to keep the notebook server running, disconnect and reconnect later from another machine to the same session.\nI created a script which is very general and can be used on most HPC cluster and published it on Github:\nhttps://github.com/pyHPC/ipynbhpc\nOnce the script is running, it is possible to connect to localhost:PORT and visualize the IPython notebook, see the following screenshot of Chromium running locally on my machine connected to a IPython notebook running on a Gordon computing node:"
  },
  {
    "objectID": "posts/2014-02-13-openproceedings.html",
    "href": "posts/2014-02-13-openproceedings.html",
    "title": "openproceedings - Github/FigShare based publishing platform for conference proceedings",
    "section": "",
    "text": "Github provides a great interface for gathering, peer reviewing and accepting papers for conference proceedings, the second step is to publish them on a website either in HTML or PDF form or both. The Scipy conference is at the forefront on this and did great work in peer reviewing on Github, see: https://github.com/scipy-conference/scipy_proceedings/pull/61.\nI wanted to develop a system to make it easier to continously publish updated versions of the papers and also leverage FigShare to provide a long term repository, a sharing interface and a DOI.\nI based it on the blog engine Pelican, developed a plugin figshare_pdf to upload a PDF of an article via API and configured Travis-ci as building platform.\nSee more details on the project page on Github: https://github.com/openproceedings/openproceedings-buildbot"
  },
  {
    "objectID": "posts/2014-03-20-machine-learning-at-scale-with-python.html",
    "href": "posts/2014-03-20-machine-learning-at-scale-with-python.html",
    "title": "Machine learning at scale with Python",
    "section": "",
    "text": "My talk for the San Diego Data Science meetup: http://www.meetup.com/San-Diego-Data-Science-R-Users-Group/events/170967362/\nAbout:\n\nSetup StarCluster to launch EC2 instances\nRunning IPython Notebook on Amazon EC2\nRunning single node Machine Learning jobs using multiple cores\nDistributing jobs with IPython parallel to multiple EC2 instances\nSee HTML5 slides: http://bit.ly/ml-ec2\nSee the IPython notebook sources of the slides: http://bit.ly/ml-ec2-ipynb\n\nFinally the Github repository with additional material, under MIT license: https://github.com/zonca/machine-learning-at-scale-with-python\nAny feedback is appreciated, google+, twitter or email."
  },
  {
    "objectID": "posts/2014-06-05-career_as_a_computational_scientist.html",
    "href": "posts/2014-06-05-career_as_a_computational_scientist.html",
    "title": "Thoughts on a career as a computational scientist",
    "section": "",
    "text": "Recently I’ve been asked what are the prospects of a wannabe computational scientist, both in terms of training and in terms of job opportunities.\nSo I am writing this blog post about my personal experience."
  },
  {
    "objectID": "posts/2014-06-05-career_as_a_computational_scientist.html#what-is-a-computational-scientist",
    "href": "posts/2014-06-05-career_as_a_computational_scientist.html#what-is-a-computational-scientist",
    "title": "Thoughts on a career as a computational scientist",
    "section": "What is a computational scientist?",
    "text": "What is a computational scientist?\nIn my understanding, a computational scientist is a scientist with strong skills in scientific computing who most of the day is building software.\nUsually there are 2 main areas, in any field of science:\n\nData analysis: historically only few fields of science had to deal with large amount of experimental data, e.g. Astrophysics, nowadays instead every field can generate extremely large amounts of data thanks to modern technology. The task of the computational scientist is generally to analyze the data, i.e. cleanup, check systematic effects, calibrate, understand and reduce to a form to be used for scientific exploitation. Generally a second phase of data analysis involves model fitting, i.e. check which theoretical models best fit the data and estimate their parameters with error bars, this requires knowledge of Statistics and Bayesian techniques, like Markov Chain Monte Carlo (MCMC).\nSimulations: production of artificial data used for their own good in the understanding of scientific models or by trying to reproduce experimental data in order to characterize the response of a scientific instrument."
  },
  {
    "objectID": "posts/2014-06-05-career_as_a_computational_scientist.html#skills-of-a-computational-scientist",
    "href": "posts/2014-06-05-career_as_a_computational_scientist.html#skills-of-a-computational-scientist",
    "title": "Thoughts on a career as a computational scientist",
    "section": "Skills of a computational scientist",
    "text": "Skills of a computational scientist\nStarting out as a computational scientist nowadays is quite easy; with a background in any field of science, it is possible to improve computational skills thanks to several learning resources, for example:\n\nFree online video classes on Coursera, Udacity and others\nSoftware Carpentry runs bootcamps for scientists to improve their computational skills\nOnline tutorials on Python for scientific computing\nBooks, e.g. Python for Data Analysis\n\nBasically it is important to have a good experience with at least one programming language, Python is the safest option because:\n\nit is well enstabilished in many fields of science\nits syntax is easier to learn than most other common programming languages\nit has the largest number of scientific libraries\nit is easy to interface with other languages, i.e. we can reuse legacy code implemented in C/C++/FORTRAN\nit can be used also when developing something unusual for a computational scientist, like web development (django) or interfacing with hardware (pyserial).\n\nPython performance is comparable to C/C++/Java when we make use of optimized libraries like numpy, pandas, scipy, which have Python frontends to highly optimized C or Fortran code; therefore is necessary to avoid explicit for loops and learn to write “vectorized” code, that allows entire arrays and matrices to be processed in one step.\nSome important Python tools to learn are:\n\nIPython notebooks to write documents with code, documentatin and plots embedded\nnumpy and pandas for data management\nmatplotlib for plotting\nh5py or pytables, HDF5 binary files manipulation\nhow to publish a Python package\nemcee for MCMC\nscipy for signal processing, FFT, optimization, integration, 2d array processing\nscikit-learn for Machine Learning\nscikit-image for image processing\nObject oriented programming\n\nFor parallel programming:\n\nIPython parallel for distributing large amount of serial and independent job on a cluster\nPyTrilinos for distributed linear algebra (high level operations with data distributed across nodes, automatic MPI communication)\nmpi4py for manually create communication of data via MPI\n\nOn top of Python is also useful to learn a bit about shell scripting with bash, which for simple automation tasks is better suited, and it is fundamental to learn version control with git or mercurial."
  },
  {
    "objectID": "posts/2014-06-05-career_as_a_computational_scientist.html#my-experience",
    "href": "posts/2014-06-05-career_as_a_computational_scientist.html#my-experience",
    "title": "Thoughts on a career as a computational scientist",
    "section": "My experience",
    "text": "My experience\nI trained as Aerospace Engineer for my Master degree, and then moved to a PhD in Astrophysics, in Milano, where I worked in the Planck collaboration and took care of simulating the inband response of the Low Frequency Instrument detectors. During my PhD I developed a good proficiency with Python, mainly using it for task automation and plotting. My previous programming experience was very low, only some Matlab during last year of my Master degree, but I found Python really easy to use, and learned it myself with books and online tutorials. With no formal education in Computer Science, the most complicated concept to grasp is Object Oriented programming; at the time I was moonlighting as a web developer and I familiarized with OO using Django models. After my PhD I got a PostDoc position at the University of California, Santa Barbara, there I had for the first time access to supercomputers and my job involved analyzing large amount of data. During 4 years at UCSB I had the great opportunity of choosing my own tools, implementing my own software for data processing, so I immediately saw the value of improving my understanding of software development best practices.\nUnfortunately in science there is usually a push toward hacking around a quick and dirty solution to get out results and go forward, I instead focused on learning how to build easily-maintenable libraries that I could re-use in the future. This involved learning more advanced Python, version control, unit testing and so on. I learned these tools by reading tutorials and documentation on the web, answers on StackOverflow, blog posts. It also helped that I became one of the core developers of healpy, a Python package for pixelized sky maps processing.\nIn 2013, at the 4th year of my PostDoc and with the Planck mission near to the end in 2015, I was looking for a position as a computational scientist, mainly as a research scientist (i.e. doing research/data analysis full time, with a long term contract) at research labs like Berkeley Lab or Jet Propulsion Laboratory, or in a research group in Cosmology/Astrophysics or in High Performance Computing.\nI got hired at the San Diego Supercomputer Center in December 2013 as a permanent staff, mainly thanks to my experience with data analysis, Python and parallel programming, here I collaborate with research groups in any field of Science and help them deploy and optimize their software on supercomputers here at SDSC or in other XSEDE centers."
  },
  {
    "objectID": "posts/2014-06-05-career_as_a_computational_scientist.html#thoughts-about-a-career-as-a-computational-scientist",
    "href": "posts/2014-06-05-career_as_a_computational_scientist.html#thoughts-about-a-career-as-a-computational-scientist",
    "title": "Thoughts on a career as a computational scientist",
    "section": "Thoughts about a career as a computational scientist",
    "text": "Thoughts about a career as a computational scientist\nAfter a PhD program, a computational scientist with experience either in data analysis or simulation, especially if has experience in parallel programming, should quite easily find a position as a PostDoc, lots of research groups have huge amount of data and need software development skilled labor.\nI believe what is complicated is the next step, faculty jobs favour scientists with the best scientific publications, and software development generally is not recognized as a first class scientific product. Very interesting opportunities in Academia are Research Scientist positions either at research facilities, for example Lawrence Berkeley Labs and NASA Jet Propulsion Laboratory, or supercomputer centers. These jobs are often permament positions, unless the institution runs out of funding, and allow to work 100% on research. Another opportunity is to work as Research Scientist in a specific research group in a University, this is less common, and depends on their availability of long-term funding.\nStill, the total number of available positions in Academia is not very high, therefore it is very important to also keep open the opportunity of a job in Industry. Fortunately nowadays most skills of a computational scientist are very well recognized in Industry, so I recommend to choose, whenever possible, to learn and use tools that are widely used also outside of Academia, for example Python, version control with Git, shell scripting, unit testing, databases, multi-core programming, parallel programming, GPU programming and so on.\nAcknowledgement: thanks to Priscilla Kelly for discussion on this topic and review of the post\nComments/feedback: comment on the blog using Google+ or tweet to @andreazonca"
  },
  {
    "objectID": "posts/2014-08-28-code-review-for-scientific-computing.html",
    "href": "posts/2014-08-28-code-review-for-scientific-computing.html",
    "title": "How to perform code review for scientific software",
    "section": "",
    "text": "Code review is the formal process where a programmer inspects in detail a piece of software developed by somebody else in order to improve code quality by catching bugs, improve readibility and usability. It is used extensively in industry, not much in academia.\nThere has been some discussion about this lately, see: * A few thoughts on code review of scientific code by Titus Brown * Code review for science: What we learned by Kaitlin Thaney\nI participated in the second code review pilot study of Software Carpentry where I was paired to a research group in Genomics and I reviewed some of their analysis code. In this blog post I’d like to write about some guidelines and best practices on how to perform code review of scientific code.\nBest use of code review is on libraries, prior to publication, because an improvement in code quality can help future users of the code. One-off analysis scripts benefit less from the process."
  },
  {
    "objectID": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-do-a-code-review-of-a-large-codebase",
    "href": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-do-a-code-review-of-a-large-codebase",
    "title": "How to perform code review for scientific software",
    "section": "How to do a code review of a large codebase",
    "text": "How to do a code review of a large codebase\nThe code review process should be performed on ~200-400 lines of code at a time. First thing is to ask the code author if she can identify different functionalities of the code that could be packaged and distributed separately. Modularity really helps maintaining software in the long term.\nThen the author should follow these steps to get ready for the code review:\n\nFor each of the packages identified previously, the code author should create a separate repository, generally on Github, possibly under an organization account (see Github for research groups).\nCreate a blank project in the programming language of choice (hopefully Python!) using a pre-defined standard template, I recommend using CookieCutter.\nWrite a README.md file explaining exactly the functionality of the code in general\nClone the repository locally, add, commit and push the blank project with README.md to the master branch on Github\nIdentify a portion of the software of about ~200-400 lines that has a defined functionality and that could be reviewed together. It doesn’t necessarily need to be in a runnable state, at the beginning we can start the code review without running the code.\nCreate a new branch locally and copy, add, commit this file or this set of files to the repository and push to Github\nAccess the web interface of Github, it should have detected that you just pushed a new branch and asked if you want to create a pull request. Create a pull request with a few details on the code under review.\nPoint the reviewer to the pull request"
  },
  {
    "objectID": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-review-an-improvement-to-the-software",
    "href": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-review-an-improvement-to-the-software",
    "title": "How to perform code review for scientific software",
    "section": "How to review an improvement to the software",
    "text": "How to review an improvement to the software\nThe implementation of a feature should be performed on a separate branch, then it is straightforward to push it to Github, create a pull request and ask reviewers to look at the set of changes."
  },
  {
    "objectID": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-perform-the-actual-code-review",
    "href": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-perform-the-actual-code-review",
    "title": "How to perform code review for scientific software",
    "section": "How to perform the actual code review",
    "text": "How to perform the actual code review\nCoding style should not be the main focus of the review, the most important feedback for the author are high-level comments on software organization. The reviewer should focus on what makes the software more usable and more maintenable.\nA few examples:\n\ncan some parts of the code be simplified?\nis there any functionality that could be replaced by an existing library?\nis it clear what each part of the software is doing?\nis there a more straightforward way of splitting the code into files?\nis documentation enough?\nare there some function arguments or function names that could be easily misinterpreted by a user?\n\nThe purpose is to improve the code, but also to help the code author to improve her coding skills.\nOn the Github pull requests interface, it is possible both to write general comments, and to click on a single line of code and write an inline comment."
  },
  {
    "objectID": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-implement-reviewers-recommendations",
    "href": "posts/2014-08-28-code-review-for-scientific-computing.html#how-to-implement-reviewers-recommendations",
    "title": "How to perform code review for scientific software",
    "section": "How to implement reviewer’s recommendations",
    "text": "How to implement reviewer’s recommendations\nThe author can improve the code locally on the same branch used in the pull request, then commit and push the changes to Github, the changes will be automatically added to the existing pull request, so the reviewer can start another iteration of the review process.\nComments and suggestions are welcome."
  },
  {
    "objectID": "posts/2014-10-22-zero-based-indexing.html",
    "href": "posts/2014-10-22-zero-based-indexing.html",
    "title": "Zero based indexing",
    "section": "",
    "text": "Dijkstra: https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html\nGuido van Rossum: https://plus.google.com/115212051037621986145/posts/YTUxbXYZyfi"
  },
  {
    "objectID": "posts/2014-10-22-zero-based-indexing.html#reads",
    "href": "posts/2014-10-22-zero-based-indexing.html#reads",
    "title": "Zero based indexing",
    "section": "",
    "text": "Dijkstra: https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html\nGuido van Rossum: https://plus.google.com/115212051037621986145/posts/YTUxbXYZyfi"
  },
  {
    "objectID": "posts/2014-10-22-zero-based-indexing.html#comment",
    "href": "posts/2014-10-22-zero-based-indexing.html#comment",
    "title": "Zero based indexing",
    "section": "Comment",
    "text": "Comment\nFor Europeans zero based indexing feels reasonable if we think of floors in a house, the lowest floor is ground floor, then 1st floor and so on.\nA house with 2 stories has ground and 1st floor. It is natural in this way to index zero-based and to count 1-based.\nWhat about slicing instead? This is a separate issue from indexing. The main problem here is that if you include the upper bound then you cannot express the empty slice. Also it is elegant to print the first n elements as a[:n]. Slicing a[i:j] excludes the upper bound, so it probably easier to understand if we express it as a[i:i+n]."
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html",
    "href": "posts/2015-03-24-numba-groupby-pixels.html",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "",
    "text": "Download the original IPython notebook"
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html#astrophysics-background",
    "href": "posts/2015-03-24-numba-groupby-pixels.html#astrophysics-background",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "Astrophysics background",
    "text": "Astrophysics background\nIt is very common in Astrophysics to work with sky pixels. The sky is tassellated in patches with specific properties and a sky map is then a collection of intensity values for each pixel. The most common pixelization used in Cosmology is HEALPix.\nMeasurements from telescopes are then represented as an array of pixels that encode the pointing of the instrument at each timestamp and the measurement output."
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html#sample-timeline",
    "href": "posts/2015-03-24-numba-groupby-pixels.html#sample-timeline",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "Sample timeline",
    "text": "Sample timeline\nimport pandas as pd\nimport numba\nimport numpy as np\nFor simplicity let’s assume we have a sky with 50K pixels:\nNPIX = 50000\nAnd we have 50 million measurement from our instrument:\nNTIME = int(50 * 1e6)\nThe pointing of our instrument is an array of pixels, random in our sample case:\npixels = np.random.randint(0, NPIX-1, NTIME)\nOur data are also random:\ntimeline = np.random.randn(NTIME)"
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html#create-a-map-of-the-sky-with-pandas",
    "href": "posts/2015-03-24-numba-groupby-pixels.html#create-a-map-of-the-sky-with-pandas",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "Create a map of the sky with pandas",
    "text": "Create a map of the sky with pandas\nOne of the most common operations is to sum all of our measurements in a sky map, so the value of each pixel in our sky map will be the sum of each individual measurement. The easiest way is to use the groupby operation in pandas:\ntimeline_pandas = pd.Series(timeline, index=pixels)\n\ntimeline_pandas.head()\n46889    0.407097\n3638     1.300001\n6345     0.174931\n15742   -0.255958\n34308    1.147338\ndtype: float64\n\n%time m = timeline_pandas.groupby(level=0).sum()\n\nCPU times: user 4.09 s, sys: 471 ms, total: 4.56 s\nWall time: 4.55 s"
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html#create-a-map-of-the-sky-with-numba",
    "href": "posts/2015-03-24-numba-groupby-pixels.html#create-a-map-of-the-sky-with-numba",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "Create a map of the sky with numba",
    "text": "Create a map of the sky with numba\nWe would like to improve the performance of this operation using numba, which allows to produce automatically C-speed compiled code from pure python functions.\nFirst we need to develop a pure python version of the code, test it, and then have numba optimize it:\ndef groupby_python(index, value, output):\n    for i in range(index.shape[0]):\n        output[index[i]] += value[i]\n\nm_python = np.zeros_like(m)\n\n\n%time groupby_python(pixels, timeline, m_python)\n\nCPU times: user 37.5 s, sys: 0 ns, total: 37.5 s\nWall time: 37.6 s\n\nnp.testing.assert_allclose(m_python, m)\nPure Python is slower than the pandas version implemented in cython.\n\nOptimize the function with numba.jit\nnumba.jit gets an input function and creates an compiled version with does not depend on slow Python calls, this is enforced by nopython=True, numba would throw an error if it would not be possible to run in nopython mode.\ngroupby_numba = numba.jit(groupby_python, nopython=True)\n\nm_numba = np.zeros_like(m)\n\n%time groupby_numba(pixels, timeline, m_numba)\nCPU times: user 274 ms, sys: 5 ms, total: 279 ms\nWall time: 278 ms\n\nnp.testing.assert_allclose(m_numba, m)\nPerformance improvement is about 100x compared to Python and 20x compared to Pandas, pretty good!"
  },
  {
    "objectID": "posts/2015-03-24-numba-groupby-pixels.html#use-numba.jit-as-a-decorator",
    "href": "posts/2015-03-24-numba-groupby-pixels.html#use-numba.jit-as-a-decorator",
    "title": "Accelerate groupby operation on pixels with Numba",
    "section": "Use numba.jit as a decorator",
    "text": "Use numba.jit as a decorator\nThe exact same result is obtained if we use numba.jit as a decorator:\n@numba.jit(nopython=True)\ndef groupby_numba(index, value, output):\n    for i in range(index.shape[0]):\n        output[index[i]] += value[i]"
  },
  {
    "objectID": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html",
    "href": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html",
    "title": "IPython/Jupyter notebook setup on SDSC Comet",
    "section": "",
    "text": "This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Comet at the San Diego Supercomputer Center and forward the port encrypted with SSH to the browser on a local laptop."
  },
  {
    "objectID": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#introduction",
    "href": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#introduction",
    "title": "IPython/Jupyter notebook setup on SDSC Comet",
    "section": "",
    "text": "This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Comet at the San Diego Supercomputer Center and forward the port encrypted with SSH to the browser on a local laptop."
  },
  {
    "objectID": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#quick-reference",
    "href": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#quick-reference",
    "title": "IPython/Jupyter notebook setup on SDSC Comet",
    "section": "Quick reference",
    "text": "Quick reference\n\nAdd module load python scipy to .bashrc\nMake sure you can ssh passwordless within comet, i.e. ssh comet.sdsc.edu from comet login node works without password\nGet submit_slurm_comet.sh from https://gist.github.com/zonca/5f8b5ccb826a774d3f89\nChange the port number and customize options (duration)\nsbatch submit_slurm_comet.sh\nRemember the login node you are using\nFrom laptop, use bash tunnel_notebook_comet.sh N where N is the Comet login number (e.g. 2) from https://gist.github.com/zonca/5f8b5ccb826a774d3f89\nFrom laptop, open browser and connect to http://localhost:YOURPORT"
  },
  {
    "objectID": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#detailed-walkthrough",
    "href": "posts/2015-09-17-ipython-jupyter-notebook-sdsc-comet.html#detailed-walkthrough",
    "title": "IPython/Jupyter notebook setup on SDSC Comet",
    "section": "Detailed walkthrough",
    "text": "Detailed walkthrough\n\nOne time setup on Comet\nLogin into a Comet login node, edit the .bashrc file in your home folder (with nano .bashrc for example) and add module load python scipy at the bottom. This makes sure you always have the Python environment loaded in all your jobs. Logout, log back in, make sure that module list shows python and scipy.\nYou need to be able to SSH from a node to another node on comet with no need of a password. Create a new SSH certificate with ssh-keygen, hit enter to keep all default options, DO NOT ENTER A PASSWORD. Then use ssh-copy-id comet.sdsc.edu, enter your password to make sure the key is copied in the authorized hosts. Now you can check it works by executing:\nssh comet.sdsc.edu\nfrom the login node and make sure you are NOT asked for your password.\n\n\nConfigure the script for SLURM and submit the job\nCopy submit_slurm_comet.sh from https://gist.github.com/zonca/5f8b5ccb826a774d3f89 on your home on Comet.\nChange the port number in the script to a port of your choosing between 8000 and 9999, referenced as YOURPORT in the rest of the tutorial. Two users on the same login node on the same port would not be allowed to forward, so try to avoid common port numbers as 8000, 9000, 8080 or 8888. Tho\nChoose whether you prefer to use a full node to have access to all 24 cores and 128GB of RAM or if you only need 1 core and 5GB of RAM and change the top of the script accordingly.\nChoose a duration of your job, for initial testing better keep 30 minutes so your job starts straight away.\nSubmit the job to the scheduler:\nsbatch submit_slurm_comet.sh\nWait for the job to start running, you should see R in:\nsqueue -u $USER\nThe script launches an IPython notebook on a computing node and tunnels its port to the login node.\nYou can check that everything worked by checking that no errors show up in the notebook.log file, and that you can access the notebook page with wget:\nwget localhost:YOURPORT\nshould download a index.html file in the current folder, and NOT give an error like “Connection refused”.\nCheck what login node you were using on comet, i.e. the hostname on your terminal on comet, for example comet-ln2.\n\n\nTunnel the port to your laptop\n\nLinux / MAC\nDownload the tunnel_notebook_comet.sh script from https://gist.github.com/zonca/5f8b5ccb826a774d3f89.\nCustomize the script with your port number.\nLauch bash tunnel_notebook_comet.sh N where N is the comet login node number. So if you were on comet-ln2, use bash tunnel_notebook_comet.sh 2.\nThe script forwards the port from the login node of comet to your laptop.\n\n\nWindows\nInstall putty.\nFollow tutorial for local port forwarding on https://www.akadia.com/services/ssh_putty.html/\n\nset comet-ln2.sdsc.edu as remote host, 22 as SSH port\nset YOURPORT as tunnel port, replace both 8080 and 80 in the tutorial with your port number.\n\n\n\n\nConnect to the Notebook\nOpen a browser and type http://localhost:YOURPORT in the address bar."
  },
  {
    "objectID": "posts/2015-10-05-use-own-python-in-jupyterhub.html",
    "href": "posts/2015-10-05-use-own-python-in-jupyterhub.html",
    "title": "Use your own Python installation (kernel) in Jupyterhub",
    "section": "",
    "text": "Updated February 2017\nYou have access to a Jupyterhub server but the Python installation provided does not satisfy your needs, how to use your own?"
  },
  {
    "objectID": "posts/2015-10-05-use-own-python-in-jupyterhub.html#install-anaconda",
    "href": "posts/2015-10-05-use-own-python-in-jupyterhub.html#install-anaconda",
    "title": "Use your own Python installation (kernel) in Jupyterhub",
    "section": "Install Anaconda",
    "text": "Install Anaconda\nIf you haven’t already your own Python installation on the Jupyterhub server you have access to, you can install Anaconda in your home folder. I assume here you have a permanent home folder on the server.\nIn order to type commands, you can either get a Jupyterhub Terminal, or run in the IPython notebook with !.\n\n!wget https://repo.continuum.io/archive/Anaconda3-2.3.0-Linux-x86_64.sh\n!bash ./Anacon*"
  },
  {
    "objectID": "posts/2015-10-05-use-own-python-in-jupyterhub.html#create-a-kernel-file-for-jupyterhub",
    "href": "posts/2015-10-05-use-own-python-in-jupyterhub.html#create-a-kernel-file-for-jupyterhub",
    "title": "Use your own Python installation (kernel) in Jupyterhub",
    "section": "Create a kernel file for Jupyterhub",
    "text": "Create a kernel file for Jupyterhub\nYou probably already know you can have Python 2 and Python 3 kernels on the same Jupyter notebook installation. In the same way you can create your own KernelSpec that launches instead another Python installation.\nIPython can automatically create a KernelSpec for you, from the IPython notebook, run:\n!~/anaconda3/bin/ipython kernel install --user --name anaconda\nIn case your path is different, just insert the full path to ipython from the Python installation you would like to use.\nThis will create a file kernel.json in ~/.local/share/jupyter/kernels/anaconda.\nYou can also add KernelSpecs for other conda environments doing:\n!source activate environmentname\n!ipython kernel install --user --name environmentname"
  },
  {
    "objectID": "posts/2015-10-05-use-own-python-in-jupyterhub.html#launch-a-notebook",
    "href": "posts/2015-10-05-use-own-python-in-jupyterhub.html#launch-a-notebook",
    "title": "Use your own Python installation (kernel) in Jupyterhub",
    "section": "Launch a Notebook",
    "text": "Launch a Notebook\nGo back to the Jupyterhub dashboard, reload the page, now you should have another option in the New menu that says My Anaconda.\nIn order to use your new kernel with an existing notebook, click on the notebook file in the dashboard, it will launch with the default kernel, then you can change kernel from the top menu Kernel &gt; Change kernel."
  },
  {
    "objectID": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html",
    "href": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html",
    "title": "Quick Jupyterhub deployment for workshops with pre-built image",
    "section": "",
    "text": "This tutorial explains how to use a OpenStack image I already built to quickly deploy a Jupyterhub Virtual Machine that can provide a good initial setup for a workshop, providing students access to Python 2/3, Julia, R, file editor and terminal with bash.\nFor details about building the instance yourself for more customization, see the full tutorial at http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html."
  },
  {
    "objectID": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#create-a-virtual-machine-in-openstack-with-the-pre-built-image",
    "href": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#create-a-virtual-machine-in-openstack-with-the-pre-built-image",
    "title": "Quick Jupyterhub deployment for workshops with pre-built image",
    "section": "Create a Virtual Machine in OpenStack with the pre-built image",
    "text": "Create a Virtual Machine in OpenStack with the pre-built image\nFollow the 3 steps at the step by step tutorial under “Create a Virtual Machine in OpenStack”:\n\nNetwork setup\nCreate a new Virtual Machine: here instead of choosing the base ubuntu image, choose jupyterhub_docker, also you can choose any size, I recommend to start with a c1.large for experimentation, you can then resize it later to a more powerful instance depending on the needs of your workshop\nGive public IP to the instance"
  },
  {
    "objectID": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#connect-to-jupyterhub",
    "href": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#connect-to-jupyterhub",
    "title": "Quick Jupyterhub deployment for workshops with pre-built image",
    "section": "Connect to Jupyterhub",
    "text": "Connect to Jupyterhub\nThe Jupyterhub instance is ready! Just open your browser and connect to the floating IP of the instance you just created.\nThe browser should show a security error related to the fact that the pre-installed SSL certificate is not trusted, click on “Advanced properties” and choose to connect anyway, we’ll see later how to fix this.\nYou already have 50 training users, named training01 to training50, all with the same password jupyterhubSDSC (see below how to change it). Check that you can login and create a notebook."
  },
  {
    "objectID": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#administer-the-jupyterhub-instance",
    "href": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#administer-the-jupyterhub-instance",
    "title": "Quick Jupyterhub deployment for workshops with pre-built image",
    "section": "Administer the Jupyterhub instance",
    "text": "Administer the Jupyterhub instance\nLogin into the Virtual Machine with ssh -i jupyterhub.pem ubuntu@xxx.xxx.xxx.xxx using the key file and the public IP setup in the previous steps.\nTo get rid of the annoying “unable to resolve host” warning, add the hostname of the machine (check by running hostname) to /etc/hosts, i.e. the first line should become something like 127.0.0.1 localhost jupyterhub if jupyterhub is the hostname\n\nChange password/add more users\nIn the home folder of the ubuntu users, there is a file named create_users.sh, edit it to change the PASSWORD variable and the number of users from 50 to a larger number. Then run it with bash create_users.sh. Training users cannot SSH into the machine.\nUse sudo passwd trainingXX to change the password of a single user.\n\n\nSetup a domain (needed for SSL certificate)\nIf you do not know how to get a domain name, here some options:\n\nyou can generally request a subdomain name from your institution, see for example UCSD\nif you own a domain, go in the DNS settings, add a record of type A to a subdomain, like jupyterhub.yourdomain.com that points to the floating IP of the Jupyterhub instance\nyou can get a free dynamic dns at websites like noip.com\n\nIn each case you need to have a DNS record of type A that points to the floating IP of the Jupyterhub instance.\n\n\nSetup a SSL Certificate\nLetsencrypt provides free SSL certificates by using a command line client.\nSSH into the server, run:\ngit clone https://github.com/letsencrypt/letsencrypt\ncd letsencrypt\nsudo service nginx stop\n./letsencrypt-auto certonly --standalone -d jupyterhubdeploy.ddns.net\nFollow instructions at the terminal to obtain a certificate\nNow open the nginx configuration file: sudo vim /etc/nginx/nginx.conf\nAnd modify the SSL certificate lines:\nssl_certificate /etc/letsencrypt/live/yoursub.domain.edu/cert.pem;\nssl_certificate_key /etc/letsencrypt/live/yoursub.domain.edu/privkey.pem;\nStart NGINX:\nsudo service nginx start\nConnect again to Jupyterhub and check that your browser correctly detects that the HTTPS connection is safe."
  },
  {
    "objectID": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#comments-suggestions",
    "href": "posts/2016-04-27-jupyterhub-image-sdsc-cloud.html#comments-suggestions",
    "title": "Quick Jupyterhub deployment for workshops with pre-built image",
    "section": "Comments? Suggestions?",
    "text": "Comments? Suggestions?\n\nTwitter\nEmail zonca on the domain sdsc.edu"
  },
  {
    "objectID": "posts/2016-10-12-dockerspawner-cuda.html",
    "href": "posts/2016-10-12-dockerspawner-cuda.html",
    "title": "Jupyterhub Docker Spawner with GPU support",
    "section": "",
    "text": "Docker Spawner allows users of Jupyterhub to run Jupyter Notebook inside isolated Docker Containers. Access to the host NVIDIA GPU was not allowed until NVIDIA release the NVIDIA-docker plugin."
  },
  {
    "objectID": "posts/2016-10-12-dockerspawner-cuda.html#build-the-docker-image",
    "href": "posts/2016-10-12-dockerspawner-cuda.html#build-the-docker-image",
    "title": "Jupyterhub Docker Spawner with GPU support",
    "section": "Build the Docker image",
    "text": "Build the Docker image\nIn order to make Jupyerhub work with NVIDIA-docker we need to build a Jupyterhub docker image for dockerspawner that includes both the dockerspawner singleuser or systemuser images and the nvidia-docker image.\nThe Jupyter systemuser images are built in several steps so let’s use them as a starting point, it is good that both image start from Ubuntu 14.04.\n\nDownload the nvidia-docker repository\nIn ubuntu-14.04/cuda/8.0/runtime/Dockerfile, replace FROM ubuntu:14.04 with FROM jupyterhub/systemuser\nBuild this image sudo docker build -t systemuser-cuda-runtime runtime\nIn ubuntu-14.04/cuda/8.0/devel/Dockerfile, replace FROM cuda:8.0-runtime with FROM systemuser-cuda-runtime\nBuild this image sudo docker build -t systemuser-cuda-devel devel\n\nNow we have 2 images, either just CUDA 8.0 runtime or also the compiler nvcc and other development tools.\nMake sure the image itself runs from the command line on the host:\nsudo nvidia-docker run --rm systemuser-cuda-devel nvidia-smi"
  },
  {
    "objectID": "posts/2016-10-12-dockerspawner-cuda.html#configure-jupyterhub",
    "href": "posts/2016-10-12-dockerspawner-cuda.html#configure-jupyterhub",
    "title": "Jupyterhub Docker Spawner with GPU support",
    "section": "Configure Jupyterhub",
    "text": "Configure Jupyterhub\nIn jupyterhub_config.py, first of all set the right image:\nc.DockerSpawner.container_image = \"systemuser-cuda-devel\"\nHowever this is not enough, nvidia-docker images need special flags to work properly and mount the host GPU into the containers, this is usually performed calling nvidia-docker instead of docker from the command line. In dockerspawner however, we are directly using the docker library, so we need to properly configure the environment there.\nFirst of all, we can get the correct flags by calling from the host machine:\ncurl -s localhost:3476/docker/cli\nThe result for my machine is:\n--volume-driver=nvidia-docker --volume=nvidia_driver_361.93.02:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools --device=/dev/nvidia0 --device=/dev/nvidia1\nNow we can configure dockerspawner using those values, in my case:\nc.DockerSpawner.read_only_volumes = {\"nvidia_driver_361.93.02\":\"/usr/local/nvidia\"}\nc.DockerSpawner.extra_create_kwargs = {\"volume_driver\":\"nvidia-docker\"}\nc.DockerSpawner.extra_host_config = { \"devices\":[\"/dev/nvidiactl\",\"/dev/nvidia-uvm\",\"/dev/nvidia-uvm-tools\",\"/dev/nvidia0\",\"/dev/nvidia1\"] }"
  },
  {
    "objectID": "posts/2016-10-12-dockerspawner-cuda.html#test-it",
    "href": "posts/2016-10-12-dockerspawner-cuda.html#test-it",
    "title": "Jupyterhub Docker Spawner with GPU support",
    "section": "Test it",
    "text": "Test it\nLogin with Jupyterhub, try this notebook: http://nbviewer.jupyter.org/gist/zonca/a14af3b92ab472580f7b97b721a2251e"
  },
  {
    "objectID": "posts/2016-10-12-dockerspawner-cuda.html#current-issues",
    "href": "posts/2016-10-12-dockerspawner-cuda.html#current-issues",
    "title": "Jupyterhub Docker Spawner with GPU support",
    "section": "Current issues",
    "text": "Current issues\n\nEnvironment on the Jupyterhub kernel is missing LD_LIBRARY_PATH, running directly on the image instead is fine\nI’d like to test using numba in Jupyterhub, but that requires cudatoolkit 8.0 which is not available yet in Anaconda"
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html",
    "href": "posts/2017-02-01-publish-research-software-github.html",
    "title": "How to publish your research software to Github",
    "section": "",
    "text": "Do you want to make your research software available publicly on Github?\nHas your reviewer asked to publish the code described in your paper?\nWould you like to collaborate on your research software with other people, either local or remote?\nNowadays many journals require that the software used to produce results described in a scientific paper be made available publicly for other peers to be able to reproduce the results or even just explore the analysis more in detail.\nThe most popular platform is Github, it allows to create a homepage for your software, keep track of any future code change and allows people to report issues or contribute patches easily.\nI’ll assume familiarity with working from the command line."
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html#prepare-your-software-for-publication",
    "href": "posts/2017-02-01-publish-research-software-github.html#prepare-your-software-for-publication",
    "title": "How to publish your research software to Github",
    "section": "Prepare your software for publication",
    "text": "Prepare your software for publication\nFirst it is necessary to make sure your code is all inside a single root folder (with any number of subfolders), then cleanup any build artifact, data or executable present in your tree of folders. Ideally you should only have the source code and documentation. If you have small datasets (&lt;10MB total) it is convenient to store them inside the repository, otherwise better host them on dedicated free services like Figshare.\nYou should cleanup the build and installation process for your code, if any, and ideally you should structure your code in a standard format to ease adoption, for example using a project template generated by Cookiecutter.\nYou should create a README.md file in the root folder of your project, this is very important because it will be transformed into HTML and displayed in the homepage of your software project. Here you should use the Markdown formatting language, see a Markdown cheatsheet on Github, to explain:\n\nshort description of your software\nbuild/usage requirements for your process\ninstallation instructions (and point to another file INSTALL.md for more details)\nquickstart section\nlink to usage examples\nlink to your paper about the project\nlist of developers\noptionally: how users can get support (i.e. a mailing list)\n\nFinally you should choose a license, otherwise even if the project is public, nobody is allowed to modify and re-use it legally. Create a LICENSE file in the root of folder tree and paste the content of the license. I recommend MIT license which is very permissive and simple: https://choosealicense.com/licenses/mit/"
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html#create-an-account-on-github",
    "href": "posts/2017-02-01-publish-research-software-github.html#create-an-account-on-github",
    "title": "How to publish your research software to Github",
    "section": "Create an account on Github",
    "text": "Create an account on Github\nSecond step is to create an account on Github: this just requires a username, email and password, choose your username carefully because it will become the root internet address of all your software projects, i.e. https://github.com/username/software-name.\nA Github account is free and allows any number of public software projects, private repositories are generally available only on paid account, however who has a .edu email address can apply for unlimited private repositories by applying for the academic discount."
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html#create-a-repository-on-github",
    "href": "posts/2017-02-01-publish-research-software-github.html#create-a-repository-on-github",
    "title": "How to publish your research software to Github",
    "section": "Create a repository on Github",
    "text": "Create a repository on Github\nGithub hosts software inside a version control system, git, so that it stores the complete history of all the incremental changes over time and allows to easily recover previous versions of the software. Each software project is stored in a repository, which includes both the current version and all previous versions of the software.git is a more modern alternative to subversion.\nFirst you need to create a repository on Github: authenticate on Github.com and click on the “New Repository” button, choose a name for your software project and leave all other options as default."
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html#publish-your-software-on-github",
    "href": "posts/2017-02-01-publish-research-software-github.html#publish-your-software-on-github",
    "title": "How to publish your research software to Github",
    "section": "Publish your software on Github",
    "text": "Publish your software on Github\nMake sure that the git command line tool is available on the machine where your code is stored, install it from your package manager or see installation instructions on the git website.\nFinally you can follow the instructions on the repository homepage https://github.com/username/software-name in the section ..or create a new repository on the command line, make sure you are in the root folder of your repository and follow this steps:\nTurn the current folder into a git repository:\ngit init\nAdd recursively all files and folders, otherwise specify filenames or wildcard to pick only some, be careful not to accidentally upload sensitive content like passwords:\ngit add *\nStore into the repository a first version of the software:\ngit commit -m \"first version of the software\"\nTell git the address of the remote repository on Github (make sure to use your username and the name you chose for your software project):\ngit remote add origin https://github.com/username/software-name\nUpload the software to Github:\ngit push -u origin master\nYou can then check in your browser that all the code you meant to publish is available on Github"
  },
  {
    "objectID": "posts/2017-02-01-publish-research-software-github.html#update-your-software",
    "href": "posts/2017-02-01-publish-research-software-github.html#update-your-software",
    "title": "How to publish your research software to Github",
    "section": "Update your software",
    "text": "Update your software\nWhenever in the future you need to make modifications to the software:\n\nedit the files\ngit add filename1 filename2 to prepare them for commit\ngit commit -m \"bugfix\" create a version in the history with a explanatory commit message\ngit push to publish to Github\n\nFor more details on git, check the Software Carpentry lessons."
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "",
    "text": "Usecase: You have access to a Jupyterhub server and you would like to install some packages but cannot use pip install and modify the systemwide Python installation."
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#check-if-conda-is-available",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#check-if-conda-is-available",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "Check if conda is available",
    "text": "Check if conda is available\nFirst check if the Python installation you have access to is based on Anaconda, open a Notebook and type:\n!which conda\n! executes bash commands instead of Python commands, we want to check if the conda package manager is installed.\nIf not, the setup is a bit tedious, so see my tutorial on installing Anaconda in your home folder"
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#create-a-conda-environment",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#create-a-conda-environment",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "Create a conda environment",
    "text": "Create a conda environment\nConda allows to create independent environments in our home folder, this has the advantage that the environment will be writable so we can install any other package with pip or conda install.\n!conda create -n myownenv --clone root\nYou can declare all the packages you want to install bu good starting point is just to clone the root environment, this will link all the global packages in your home folder, then you can customize it further."
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#create-a-jupyter-notebook-kernel-to-launch-this-new-environment",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#create-a-jupyter-notebook-kernel-to-launch-this-new-environment",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "Create a Jupyter Notebook kernel to launch this new environment",
    "text": "Create a Jupyter Notebook kernel to launch this new environment\nWe need to notify Jupyter of this new Python environment by creating a Kernel, from a Notebook launch:\n!source activate myownenv; ipython kernel install --user --name myownenv"
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#launch-a-notebook",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#launch-a-notebook",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "Launch a Notebook",
    "text": "Launch a Notebook\nGo back to the Jupyterhub dashboard, reload the page, now you should have another option in the New menu that says myownenv.\nIn order to use your new kernel with an existing notebook, click on the notebook file in the dashboard, it will launch with the default kernel, then you can change kernel from the top menu Kernel &gt; Change kernel."
  },
  {
    "objectID": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#install-new-packages",
    "href": "posts/2017-02-24-customize-your-python-environment-in-jupyterhub.html#install-new-packages",
    "title": "Customize your Python environment in Jupyterhub",
    "section": "Install new packages",
    "text": "Install new packages\nInside a Notebook using the myownenv environment you can install other packages running:\n!conda install newpackagename\nor:\n!pip install newpackagename"
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "",
    "text": "All the commands are executed on your local machine, the purpose of this tutorial is to be able to use globus-url-copy to copy efficiently data back and forth between your local machine and a XSEDE Supercomputer on the command line.\nFor a simpler point and click web interface, install Globus Conect Personal instead: https://www.globus.org/globus-connect-personal"
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html#install-globus-toolkit",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html#install-globus-toolkit",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "Install Globus toolkit",
    "text": "Install Globus toolkit\nSee http://toolkit.globus.org/toolkit/docs/latest-stable/admin/install/#install-toolkit\nOn Ubuntu, download the deb of the Globus repo from:\nwget http://www.globus.org/ftppub/gt6/installers/repo/globus-toolkit-repo_latest_all.deb\nsudo dpkg -i globus-toolkit-repo_latest_all.deb\nsudo apt-get install globus-data-management-client"
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html#install-xsede-certificates-on-your-machine",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html#install-xsede-certificates-on-your-machine",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "Install XSEDE certificates on your machine",
    "text": "Install XSEDE certificates on your machine\nwget https://software.xsede.org/security/xsede-certs.tar.gz\ntar xvf xsede-certs.tar.gz\nsudo mv certificates /etc/grid-security\nFull instructions here:\nhttps://software.xsede.org/production/CA/CA-install.html"
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html#authenticate-with-the-myproxy-provided-by-xsede",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html#authenticate-with-the-myproxy-provided-by-xsede",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "Authenticate with the myproxy provided by XSEDE",
    "text": "Authenticate with the myproxy provided by XSEDE\nAuthenticate with your XSEDE user and password:\nmyproxy-logon -s myproxy.xsede.org -l $USER -t 36\nYou can specify the lifetime of the certificate in hours with -t.\nyou should get a certificate:\nA credential has been received for user zonca in /tmp/x509up_u1000.\nYou can check how much time is left on a certificate by running grid-proxy-info."
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html#run-globus-url-copy",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html#run-globus-url-copy",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "Run globus-url-copy",
    "text": "Run globus-url-copy\nFor example copy to my home on Comet:\nglobus-url-copy -vb -p 4 local_file.tar.gz gsiftp://oasis-dm.sdsc.edu///home/zonca/\nSee the quickstart guide on the most used globus-url-copy options:\nhttp://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/user/#gridftp-user-basic"
  },
  {
    "objectID": "posts/2017-04-19-globus-ftp-local-machine.html#synchronize-2-folders",
    "href": "posts/2017-04-19-globus-ftp-local-machine.html#synchronize-2-folders",
    "title": "Configure Globus on your local machine for GridFTP with XSEDE authentication",
    "section": "Synchronize 2 folders",
    "text": "Synchronize 2 folders\nOnly copy new files using the -sync and -sync-level options:\n-sync\n  Only transfer files where the destination does not exist or differs from the source. -sync-level controls how to determine if files differ.\n-sync-level number\n  Criteria for determining if files differ when performing a sync transfer. The default sync level is 2.\\\nThe available levels are:\n\nLevel 0 will only transfer if the destination does not exist.\nLevel 1 will transfer if the size of the destination does not match the size of the source.\nLevel 2 will transfer if the time stamp of the destination is older than the time stamp of the source.\nLevel 3 will perform a checksum of the source and destination and transfer if the checksums do not match."
  },
  {
    "objectID": "posts/2017-06-30-quick-github-pull-requests.html",
    "href": "posts/2017-06-30-quick-github-pull-requests.html",
    "title": "How to create pull requests on Github",
    "section": "",
    "text": "Pull Requests are the web-based version of sending software patches via email to code maintainers. They allow a person that has no access to a code repository to submit a code change to the repository administrator for review and 1-click merging."
  },
  {
    "objectID": "posts/2017-06-30-quick-github-pull-requests.html#preparation",
    "href": "posts/2017-06-30-quick-github-pull-requests.html#preparation",
    "title": "How to create pull requests on Github",
    "section": "Preparation",
    "text": "Preparation\n\nCreate a free Github account at https://github.com\nLogin on Github with your credentials\nGo to the homepage of the repository, for example https://github.com/sdsc/sdsc-summer-institute-2017"
  },
  {
    "objectID": "posts/2017-06-30-quick-github-pull-requests.html#small-changes-via-github.com",
    "href": "posts/2017-06-30-quick-github-pull-requests.html#small-changes-via-github.com",
    "title": "How to create pull requests on Github",
    "section": "Small changes via Github.com",
    "text": "Small changes via Github.com\nFor small changes, like create a folder and upload a few files, or a quick fix on a previous file, you don’t even need to use the git command line client.\n\nIf you need to create a folder\n\nclick on “Create new file”\nin the “Name your file…” box, insert: “yourfolder/README.md”\nin the README.md write a description of the content of the folder, you can use markdown syntax, (see the Markdown Cheatsheet )\ncreate a bullet list with description of the files you will be uploading next\nClick on “Propose new file”\nthis will ask you to create a Pull Request, follow the prompts and make sure to confirm at the end that you want to create a Pull Request, you have to click twice on “Create Pull Request” buttons\n\nIf you want to upload files in the folder you just created, you need an additional step, if you want to upload to a folder already existing in the original repo, skip this:\n\nGo to the fork of the original repository that was created automatically under your account, for example: https://github.com/YOURUSERNAME/sdsc-summer-institute-2017\nClick on the dropdown “Branch” menu and look for the branch named patch-1, or patch-n if you have more.\n\nClick on the “Upload files” button, select and upload all files, a few notes:\n\ndo not upload zip archives\ndo not upload large data files, Github is for code\nif you are uploading binary files like images, downgrade them to a small size\nthis will ask you to create a Pull Request, follow the prompts and make sure to confirm at the end that you want to create a Pull Request, you have to click twice on “Create Pull Request” buttons\n\nCheck that your pull request appeared in the Pull Requests area of the repository, for example https://github.com/sdsc/sdsc-summer-institute-2017/pulls"
  },
  {
    "objectID": "posts/2017-06-30-quick-github-pull-requests.html#update-a-previously-create-pull-request-via-github.com",
    "href": "posts/2017-06-30-quick-github-pull-requests.html#update-a-previously-create-pull-request-via-github.com",
    "title": "How to create pull requests on Github",
    "section": "Update a previously create Pull Request via Github.com",
    "text": "Update a previously create Pull Request via Github.com\nIf the repository maintainer has some feedback on your Pull Request, you can update it to accomodate any requested change.\n\nGo to the fork of the original repository that was created automatically under your account, for example: https://github.com/YOURUSERNAME/sdsc-summer-institute-2017\nClick on the dropdown “Branch” menu and look for the branch named patch-1, or patch-n if you have more.\nNow make changes to files or upload new files, then confirm and write a commit message from the web interface\nCheck that your changes appear as updates inside the Pull Request you created before, for example https://github.com/sdsc/sdsc-summer-institute-2017/pull/N where N is the number assigned to your Pull Request"
  },
  {
    "objectID": "posts/2017-06-30-quick-github-pull-requests.html#use-the-command-line-client",
    "href": "posts/2017-06-30-quick-github-pull-requests.html#use-the-command-line-client",
    "title": "How to create pull requests on Github",
    "section": "Use the command line client",
    "text": "Use the command line client\nFor more control and especially if you expect the repository maintainer to make changes to your Pull Request before merging it, better use git.\n\nClick on the “Fork” button on the top right of the repository\nNow you should be on the copy of the repository under your own account, for example https://github.com/YOURUSERNAME/sdsc-summer-institute-2017\nNow open your terminal, if you never used git before, set it up with:\n  $ git config --global user.name \"Your Name\"\n  $ git config --global user.email \"your@email.edu\"\nNow open your terminal and clone the repository with:\n  git clone https://github.com/YOURUSERNAME/sdsc-summer-institute-2017\nEnter in the repository folder\nCreate a branch to isolate your changes with:\n  git checkout -b \"add_XXXX_material\"\nNow create folders, modify files, you can use any text editor\nOnce you are done doing modifications, you can prepare them to be committed with, this adds everything inside the folder:\n  git add my_folder\nGenerally better instead to add each file to make sure you don’t accidentally commit wrong files\n  git add my_folder/aaa.txt my_folder/README.md\nThen write this changes to history with a commit\n  git commit -m \"Added material about XXXX\"\nPush changes to Github\n  git push -u origin add_XXXX_material\nNow go to the homepage of the original repository, for example https://github.com/sdsc/sdsc-summer-institute-2017\nThere should be a yellow notice saying that it detected a recently pushed branch, click on “Compare and Pull Request”\nAdd a description\nConfirm with the green “Create Pull Request” button\n\nIn case you want to update your Pull Request, repeat the steps of git add, git commit and git push, any changes will be reflected inside the pull request."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "",
    "text": "Follow the instructions at https://github.com/zonca/singularity-comet to build images from the ubuntu_anaconda_jupyterhub.def and centos_anaconda_jupyterhub.def definition files, or use the containers I have already built on Comet:\n/oasis/scratch/comet/zonca/temp_project/centos_anaconda_jupyterhub.img\n/oasis/scratch/comet/zonca/temp_project/ubuntu_anaconda_cmb_jupyterhub.img\nThese containers have Centos 7 and Ubuntu 16.04 base images, MPI support (not needed for this), Anaconda 4.4.0, the Jupyterhub (for the jupyterhub-singleuser script) and Jupyterlab (for the awesomeness) packages."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#build-singularity-containers-to-run-single-user-notebook-applications",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#build-singularity-containers-to-run-single-user-notebook-applications",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "",
    "text": "Follow the instructions at https://github.com/zonca/singularity-comet to build images from the ubuntu_anaconda_jupyterhub.def and centos_anaconda_jupyterhub.def definition files, or use the containers I have already built on Comet:\n/oasis/scratch/comet/zonca/temp_project/centos_anaconda_jupyterhub.img\n/oasis/scratch/comet/zonca/temp_project/ubuntu_anaconda_cmb_jupyterhub.img\nThese containers have Centos 7 and Ubuntu 16.04 base images, MPI support (not needed for this), Anaconda 4.4.0, the Jupyterhub (for the jupyterhub-singleuser script) and Jupyterlab (for the awesomeness) packages."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#initial-setup-of-jupyterhub-with-ansible",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#initial-setup-of-jupyterhub-with-ansible",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "Initial setup of Jupyterhub with Ansible",
    "text": "Initial setup of Jupyterhub with Ansible\nFirst we want to use the Ansible playbook provided by the Jupyter team to setup a Ubuntu Virtual Machine, for example on SDSC Cloud or XSEDE Jetstream. This sets up already a Jupyterhub instance on a single machine with Github authentication, NGINX with letsencrypt SSL and spawning of Notebooks as local processes.\nStart from: Automated deployment of Jupyterhub with Ansible\nIt looks like there is a compatibility error with conda 4.3 and above, I had to fix this (and provided PR upstream), I used the version at https://github.com/zonca/jupyterhub-deploy-teaching/tree/globus_singularity. In particular check the example configuration file in the host_vars/ folder.\nOnce we have executed the scripts, connect to the Virtual Machine, login with Github and check that Notebooks are working."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-authentication-with-globus",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-authentication-with-globus",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "Setup Authentication with Globus",
    "text": "Setup Authentication with Globus\nNext we can SSH into the Jupyterhub Virtual Machine and customize Jupyterhub configuration in /etc/jupyterhub\noauthenticator should alrady be installed,, but it needs the Globus SDK to support authentication with Globus:\nsudo /opt/conda/bin/pip install globus_sdk[jwt]\nThen follow the instructions to setup Globus Auth: https://github.com/jupyterhub/oauthenticator#globus-setup\nyou should now have add these lines in /etc/jupyterhub/jupyterhub_config.py\nfrom oauthenticator.globus import GlobusOAuthenticator\nc.JupyterHub.authenticator_class = GlobusOAuthenticator\nc.GlobusOAuthenticator.oauth_callback_url = 'https://xxx-xxx-xxx-xxx.compute.cloud.sdsc.edu/hub/oauth_callback'\nc.GlobusOAuthenticator.client_id = ''\nc.GlobusOAuthenticator.client_secret = ''\nYou should now be able to login with your Globus ID credentials, see the documentation to support credentials from institutions supported by Globus Auth. After login, don’t worry if you get an error in starting your notebook."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-spawning-with-batchspawner",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-spawning-with-batchspawner",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "Setup Spawning with Batchspawner",
    "text": "Setup Spawning with Batchspawner\nIn my last post about spawning Notebooks on Comet I was using XSEDE authentication so that each user would have to use their own Comet account. In this scenario instead we imagine a Gateway system where the administrator shares their own allocation with the Gateway users. Therefore you should create a SSH keypair for the root user on the Jupyterhub Virtual Machine and make sure you can login with no need for a password to Comet as the Gateway user.\nThen you need to install batchspawner:\ngit clone https://github.com/jupyterhub/batchspawner.git\ncd batchspawner/\nsudo /opt/conda/bin/pip install .\nThen configure the Spawner, see my configuration of Jupyterhub: jupyterhub_config.py.\nYou should modify comet_spawner.py to point to your Gateway user home folder and then fill all the details in jupyterhub_config.py marked by the CONF string.\nIn CometSpawner I also create a form for the user to choose the parameters of the job and also the Singularity image they want to use.\nHere the spawner uses SSH to connect to the Comet login node and submit jobs as the Gateway user.\nAt this point you should be able to login and launch a job on Comet, execute squeue on Comet to check if that works or look in the home folder of the Gateway user for the logfile of the job and in /var/log/jupyterhub on the Virtual machine for errors."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-tunneling",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#setup-tunneling",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "Setup tunneling",
    "text": "Setup tunneling\nFinally we need a way for the gateway Virtual Machine to access the port on the Comet computing node in order to proxy the Notebook application back to the user.\nThe simpler solution is to create a user tunnelbot on the VM with no shell access, then create a SSH keypair and paste the private key into the jupyterhub_config.py file (contact me if you have a btter solution!). The job on Comet sets up then a SSH tunnel between the Comet computing node and the Jupyterhub VM."
  },
  {
    "objectID": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#improvements",
    "href": "posts/2017-08-11-jupyterhub-globus-comet-singularity.html#improvements",
    "title": "Deployment of Jupyterhub with Globus Auth to spawn Notebook on Comet in Singularity containers",
    "section": "Improvements",
    "text": "Improvements\nTo keep the setup simple, all users are running on the home folder of the Gateway user, for a real deployment, it is possible to create a subfolder for each user beforehand and then use Singularity to mount that as the home folder."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html",
    "href": "posts/2017-10-25-compile-software-singularity.html",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "",
    "text": "Singularity allows to run your own OS within most Supercomputers, see my previous post about Running Ubuntu on Comet via Singularity\nSingularity’s adoption by High Performance Computing centers has been driven by its strict security model. It never allows a user in a container to have root privileges unless the user is root on the Host system.\nThis means that you can only modify containers on a machine where you have root. Therefore you generally build a container on your local machine and then copy it to a Supercomputer. The process is tedious if you are still tweaking your container and modifying it often, and each time your have to copy back a 4 or maybe 8 GB container image.\nIn the next section I’ll investigate possible solutions/workarounds."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html#introduction",
    "href": "posts/2017-10-25-compile-software-singularity.html#introduction",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "",
    "text": "Singularity allows to run your own OS within most Supercomputers, see my previous post about Running Ubuntu on Comet via Singularity\nSingularity’s adoption by High Performance Computing centers has been driven by its strict security model. It never allows a user in a container to have root privileges unless the user is root on the Host system.\nThis means that you can only modify containers on a machine where you have root. Therefore you generally build a container on your local machine and then copy it to a Supercomputer. The process is tedious if you are still tweaking your container and modifying it often, and each time your have to copy back a 4 or maybe 8 GB container image.\nIn the next section I’ll investigate possible solutions/workarounds."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html#use-dockerhub",
    "href": "posts/2017-10-25-compile-software-singularity.html#use-dockerhub",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "Use DockerHub",
    "text": "Use DockerHub\nSingularity can pull a container from DockerHub, so it is convenient if you are already using Docker, maybe to provide a simple way to install your software.\nI found out that if you are using the Automatic build of your container by DockerHub itself, this is very slow, sometimes it takes 30 minutes to have your new container build.\nTherefore the best is to manually build your container locally and then push it to DockerHub. A Docker container is organized in layers of the filesystem, so for small tweaks to your image you transfer tens of MB to DockerHub instead of GB.\nThen from the Supercomputer you can run singularity pull docker://ubuntu:latest with no need of root privileges. Singularity keeps a cache of the docker layers, so you would download just the layers modified in the previous step."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html#build-your-application-locally",
    "href": "posts/2017-10-25-compile-software-singularity.html#build-your-application-locally",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "Build your application locally",
    "text": "Build your application locally\nIf you are modifying an application often you could build a Singularity container with all the requirements, copy it to the Supercomputer and then build your application there. This is also useful if the architecture of your CPU is different between your local machine and the Supercomputer and you are worried the compiler would not apply all the possible optimizations.\nIn this case you can use singularity shell to get a terminal inside the container, then build your software with the compiler toolchain available inside the container and then install it to your $HOME folder, then modify your $PATH and $LD_LIBRARY_PATH to execute and load libraries from this local folder.\nThis is also useful in case the container has already an application installed but you want to develop on it. You can follow this process and then mask the installed application with your new version.\nOf course this makes your analysis not portable, the software is not available inside the container.\n\nFreeze your application inside the container\nOnce you have completed tweaking the application on the Supercomputer, you can now switch back to your local machine, get the last version of your application and install it system-wide inside the container so that it will be portable.\nOn the other hand, you might be concerned about performance and prefer to have the application built on the Supercomputer. You can run the build process (e.g. make or python setup.py build) on the Supercomputer in your home, then sync the build artifacts back to your local machine and run the install process there (e.gsudo make installorsudo python setup.py install). Optionally usesshfs` to mount the build folder on both machines and make the process transparent."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html#use-a-local-singularity-registry",
    "href": "posts/2017-10-25-compile-software-singularity.html#use-a-local-singularity-registry",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "Use a local Singularity registry",
    "text": "Use a local Singularity registry\nSingularity released singularity-registry, an application to build a local image registry, like DockerHub, that can take care of building containers.\nThis can be hosted locally at a Supercomputing Center to provide a local building service. For example Texas Advanced Computing Center builds locally Singularity images from BioContainers, software packages for the Life Sciences.\nOtherwise, for example, a user at SDSC could install Singularity Registry on SDSC Cloud and configure it to mount one of Comet’s filesystems and build the container images there. Even installing Singularity Registry on Jetstream could be an option thanks to its fast connection to other XSEDE resources."
  },
  {
    "objectID": "posts/2017-10-25-compile-software-singularity.html#feedback",
    "href": "posts/2017-10-25-compile-software-singularity.html#feedback",
    "title": "How to modify Singularity images on a Supercomputer",
    "section": "Feedback",
    "text": "Feedback\nIf you have any feedback, please reach me at @andreazonca or find my email from there."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "",
    "text": "Jupyterhub genrally requires roughly 500MB per user for light data processing and many GB for heavy data processing, therefore it is often necessary to deploy it across multiple machines to support many users.\nThe recommended scalable deployment for Jupyterhub is on Kubernetes, see Zero to Jupyterhub (and I’ll cover it next). However the learning curve for Kubernetes is quite steep, I believe that for smaller deployments (30/50 users, 10 users per machine) and where high availability is not critical, deploying on Docker with Swarm Mode is a simpler option.\nIn the past I have covered a Jupyterhub deployment on the old version of Docker Swarm using DockerSpawner. The most important difference is that the last version of Docker has a more sophisticated “Swarm mode” that allows you to launch and manage services instead of individual containers, support for this is provided by SwarmSpawner. Thanks to the new architecture, we do not need to have actual Unix accounts on the Host but all users can run with the jovyan user account defined only inside the Docker containers. Then we can also deploy Jupyterhub itself as a Docker container instead of installing it on the Host."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#introduction",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#introduction",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "",
    "text": "Jupyterhub genrally requires roughly 500MB per user for light data processing and many GB for heavy data processing, therefore it is often necessary to deploy it across multiple machines to support many users.\nThe recommended scalable deployment for Jupyterhub is on Kubernetes, see Zero to Jupyterhub (and I’ll cover it next). However the learning curve for Kubernetes is quite steep, I believe that for smaller deployments (30/50 users, 10 users per machine) and where high availability is not critical, deploying on Docker with Swarm Mode is a simpler option.\nIn the past I have covered a Jupyterhub deployment on the old version of Docker Swarm using DockerSpawner. The most important difference is that the last version of Docker has a more sophisticated “Swarm mode” that allows you to launch and manage services instead of individual containers, support for this is provided by SwarmSpawner. Thanks to the new architecture, we do not need to have actual Unix accounts on the Host but all users can run with the jovyan user account defined only inside the Docker containers. Then we can also deploy Jupyterhub itself as a Docker container instead of installing it on the Host."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-a-virtual-machine-for-the-hub",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-a-virtual-machine-for-the-hub",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "Setup a Virtual Machine for the Hub",
    "text": "Setup a Virtual Machine for the Hub\nFirst of all we need to create a Virtual Machine, I tested this on XSEDE Jetstream CentOS 7 image (with Docker pre-installed), but I would recommend Ubuntu 16.04 which is more universally used so it is easier to find support for it. The same setup would work on a bare-metal server.\nMake sure that a recent version of Docker is installed, I used 17.07.0-ce.\nSetup networking so that port 80 and 443 are accessible for HTTP and HTTPS. Associate a Public IP to this instance so that it is accessible from the Internet.\nAdd your user to the docker group so you do not need sudo to run docker commands. Check that docker works running docker info.\n\nClone the config files repository\nI recommend to create the folder /etc/jupyterhub, set ownership to your user and clone my configuration repository there:\ngit clone https://github.com/zonca/deploy-jupyterhub-dockerswarm /etc/jupyterhub\n\n\nSetup Swarm\nThe first node is going to be the Master node of the Swarm, launch:\ndocker swarm init --advertise-addr INTERNAL_IP_ADDRESS\nIt is better to use a internal IP address, for example on Jetstream the 192.xxx.xxx.xxx IP. This is the address that the other instances will use to connect to this node.\nThis command will print out the string that the other nodes will need to run to join this swarm, save it for later (you can recover it with docker swarm join-token)\n\n\nInstall the NGINX web server\nNGINX is going to sit in front of Jupyterhub as a proxy and handle SSL (at the end of this tutorial), we are going to have also NGINX as a Docker service:\ndocker pull nginx:latest\nNow let’s test that Docker and the networking is working correctly, launch nginx with the default configuration:\ndocker service create \\\n  --name nginx \\\n  --publish 80:80 \\\n  nginx\nThis is going to create a service, then the service creates the containers, check with docker service ls and docker ps, if a container dies, the service will automatically relaunch it. Now if you connect to your instance from an external machine you should see the NGINX welcome page. If this is not the case check docker ps -a and docker logs INSTANCE_ID to debug the issue.\nFinally remove the service with:\ndocker service rm nginx\nNow run the service with the configuration for Jupyterhub, edit nginx.conf and replace SERVER_URL then launch:\nbash ngnx_service.sh\nAt this point you should gate a Gateway error if you connect with a browser to your instance.\n\n\nInstall Jupyterhub\nBefore launching Jupyterhub you need to create a Docker network so that the containers in the swarm can communicate easily:\ndocker network create --driver overlay jupyterhub\nYou can launch the official Jupyterhub 0.8.0 container as a service with:\ndocker service create \\\n  --name jupyterhubserver \\\n  --network jupyterhub \\\n  --detach=true \\\n  jupyterhub/jupyterhub:0.8.0\nThis would run Jupyterhub with the default jupyterhub_config.py with local auth and local spawner. If you connect to the instance now you should see the Jupyterhub login page, you cannot login because you don’t have a user account inside the container. We’ll setup authentication next.\n\nConfigure Jupyterhub\nNext we want to customize the hub, first login on http://hub.docker.com and create a new repository, then follow the instructions there to setup docker push on your server so you can push your image to the registy.\nThis is necessary because Swarm might spawn the service on a different machine, so itneeds an external registry to make sure to pull the right image.\nYou can now customize the hub image in /etc/jupyterhub/hub with docker build . -t yourusername/jupyterhub-docker and push it remotely with docker push yourusername/jupyterhub-docker.\nThis image includes oauthenticator for Github, Google, CILogon and Globus authentication and swarmspawner for spawning containers for the users.\nWe can now create jupyterhub_config.py, for now we just want temporary home folders, so replace the mounts variable with [] in c.SwarmSpawner.container_spec. Then customize the server URL server_url.com and IP SERVER_IP (it will be necessary later). At the bottom of jupyterhub_config.py we can also customize CPU and memory contraints. Unfortunately there is no easy way to setup a custom disk space limit.\nFollow the documentation of oauthenticator to setup authentication.\nCreate the folder /var/nfs that we will configure later but it is harcoded in the script to launch the service.\nTemporarily remove from launch_service_jupyterhub.sh the line:\n--mount src=nfsvolume,dst=/var/nfs \\\nLaunch the service from /etc/jupyterhub with bash launch_service_jupyterhub.sh.\nCheck in the script that we are mounting the Docker socket into the container so that Jupyterhub can launch Docker containers for the users. We also mount the /etc/jupyterhub folder so that it has access to jupyterhub_config.py. We also contraint it to run in the manager node of this Swarm, this assures that it always runs on this first node. We could later add another manager node for resiliency and the Hub could potentially spawn there with no issues.\nAt this point we have a first working configuration of Jupyterhub, try to login and check if the notebooks are working. This configuration has no permanent storage, so the users will have a home folder inside their container and will be able to write Notebooks and data there up to the image reaching 10GB, so about 5GB. If they logout and log back in they will find their files still there, but if they do “Close my Server” from the control panel or if for any other reason their container is removed, they will loose their data. So this setup could be used for short workshops or demos."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-other-nodes",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-other-nodes",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "Setup other nodes",
    "text": "Setup other nodes\nWe can create another Virtual Machine with the same version of Docker and make sure that the two machines internally have all the port open to simplify networking. Any additional machine needs no open ports to the outside world, all connections will go through nginx.\nWe can have it join the Swarm by pasting the token got at Swarm initialization on the first node.\nNow when Jupyterhub launches a single user container, it could spawn either on this server or on the first server, Swarm will automatically take care of load balancing. It will also automatically download the Docker image specified in jupyterhub_config.py.\nWe can add as many nodes as necessary."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-permanent-storage",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-permanent-storage",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "Setup Permanent storage",
    "text": "Setup Permanent storage\nSurprisingly enough, Swarm has no easy way to setup permament storage that would automatically move data from one node to another in case a user container is re-spawned on another server. There are some volume plugins but I believe that their configuration is so complex that at this point would be better to directly switch to Kubernetes. In order to achieve a simpler setup that I believe could easily handle few tens of users we can use NFS. Moreover Docker volumes can handle NFS natively, so we don’t even need to have home folders owned by each user but we can just point Docker volumes to our NFS folder and Docker will manage that for us and we can just use one single user. Users cannot access other people’s files because only their own folder is mounted into their container.\n\nSetup a NFS server\nFirst we need to decide which server acts as NFS server, for small deployments we can have just the first server which runs the hub also handle this, for more performance we might want to have a dedicated server that only runs NFS and which is part of the internal network but does not participate in the Swarm so that it won’t have user containers running on it.\nIn a Cloud environment like Jetstream or Amazon, it is useful to create a Volume and attach it to that instance so that we can enlarge it later or back it up independently from the Instance and that would survive the Hub instance. Make sure to choose the XFS filesystem if you need to setup disk space contraints. Mount it in /var/nfs/ and make sure it is writable by any user.\nOn that server we can install NFS following the OS instructions and setup /etc/exports with:\n/var/nfs        *(rw,sync,no_subtree_check)\nThe NFS port is accessible only on the internal network anyway so we can just accept any connection.\nSSH into any of the Swarm nodes and check this works fine with:\nsudo mount 192.NFS.SRV.IP:/var/nfs /mnt\ntouch /mnt/writing_works\n\n\nSetup Jupyterhub to use Docker Volumes over NFS\nIn /etc/jupyterhub/jupyterhub_config.py we should configure the mounts to swarmspawner:\nmounts = [{'type': 'volume',\n           'source': 'jupyterhub-user-{username}',\n           'target': notebook_dir,\n        'no_copy' : True,\n        'driver_config' : {\n          'name' : 'local',\n          'options' : {\n             'type' : 'nfs4',\n             'o' : 'addr=SERVER_IP,rw',\n             'device' : ':/var/nfs/{username}/'\n           }\n        },\n}]\nReplace SERVER_IP with your server, this tells the Docker local Volume driver to mount folders /var/nfs/{username} as home folders of the single user notebook container.\nThe only problem is that these folders need to be pre-existing, so I modified the swarmspawner plugin to create those folders the first time a user authenticates, please let me know if there is a better way and I’ll improve this tutorial. See the branch createfolder on my fork of swarmspawner. In order to install this you need to modify your custom jupyterhub-docker to install from there (see the commented out section in hub/Dockerfile). Often the Authenticator transform the username into a hash, so I added a feature on this spawner to also create a text file HASH_email.txt and save the email of the user there so that it is easier to check directly from the filesystem who owns a specific folder.\nFor this to work the Hub needs access to /var/nfs/, the best way to achieve this is to create another Volume, add the NFS_SERVER_IP and launch on the first server:\nbash create_volume_nfs.sh\nThen uncomment the --mount src=nfsvolume,dst=/var/nfs \\ line from launch_service_jupyterhub.sh and relaunch the service so that it is available locally.\nAt this point you should test that if you login, then stop/kill the container, your data should still be there when you launch it again.\n\n\nSetup user quota\nThe Docker local Volume driver does not support setting a user quota so we have to resort to our filesystem. You can modify /etc/fstab to mount the XFS volume with the pquota option that supports setting a limit to a folders and all of its subfolders. We cannot use user quotas because all of the users are running under the same UNIX account.\nCreate a folder /var/nfs/testquota and then test that setting quota is working with:\nsudo set_quota.sh /var/nfs testquota\nThere should be a space between /var/nfs and testquota, then check with:\nbash get_quota.sh\nYou should see a quota of 1GB for that folder. Modify set_quota.sh to choose another size.\n\nAutomatically set quotas\nWe want quota to be automatically set each time the spawner creates another folder, incrond can monitor a folder for any new created file and launch the set_quota.sh script for us.\nInstall the incrond package and make sure it is active and restarted on boot. Then customize it with sudo incrontab -e and paste the content of incrontab in /etc/jupyterhub.\nNow delete your user folder in /var/nfs and launch Jupyterhub again to check that the folder is created with the correct quota. The spawner also creates a /var/nfs/{username}_QUOTA_NOT_SET that is deleted then by the set_quota.sh script."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-https",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#setup-https",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "Setup HTTPS",
    "text": "Setup HTTPS\nWe would like to setup NGINX to provide SSL encryption for Jupyterhub using the free Letsencrypt service. The main issue is that those certificates need to be renewed every few months, so we need a service running regularly to take care of that.\nThe simplest option would be to add --publish 8000 to the Jupyterhub so that Jupyterhub exposes its port to the host and then remove the NGINX Docker container and install NGINX and certbot directly on the first host following a standard setup.\nHowever, to keep the setup more modular, we’ll proceed and use another NGINX container that comes equipped with automatic Let’s Encrypt certificates request and renewal available at: https://github.com/linuxserver/docker-letsencrypt.\n\nModify networking setup\nOne complication is that this container requires additional privileges to handle networking that are not availble in Swarm mode, so we will run this container outside of the Swarm on the first node.\nWe need to make the jupyterhub network that we created before attachable by containers outside the Swarm.\ndocker service rm nginx\nbash remove_service_jupyterhub.sh\ndocker network rm jupyterhub\ndocker network create --driver overlay --attachable jupyterhub\nThen add --publish 8000 to launch_service_juputerhub.sh and start Jupyterhub again. Make sure that if you SSH to the first node you can wget localhost:8000 successfully but if you try to access yourdomain:8000 from the internet you should not be able to connect (the port should be closed by the networking configuration on OpenStack for example).\n\n\nTest the NGINX/Letsencrypt container\nCreate a volume to save the configuration and the logs (optionally on the NFS volume):\ndocker volume create --driver local nginx_volume\nTest the container running:\ndocker run \\\n  --cap-add=NET_ADMIN \\\n  --name nginx \\\n  -p 443:443 \\\n  -e EMAIL=your_email@domain.edu \\\n  -e URL=your.domain.org \\\n  -v nginx_volume:/config \\\n  linuxserver/letsencrypt\nIf this works correctly, connect to https://your.domain.org, you should have a valid SSL certificate and a welcome message. If not check docker logs nginx.\n\n\nConfigure NGINX to proxy Jupyterhub\nWe can use letsencrypt_container_nginx.conf to handle NGINX configuration with HTTPS support, this loads the certificates from a path automatically created by the letsencrypt container.\nCustomize launch_letsencrypt_container.sh and then run it, it will create the NGINX container again and it will also bind-mount the NGINX configuration into the container.\nNow you should be able to connect to your server over HTTPS and access Jupyterhub."
  },
  {
    "objectID": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#feedback",
    "href": "posts/2017-10-26-scalable-jupyterhub-docker-swarm-mode.html#feedback",
    "title": "Deploy scalable Jupyterhub on Docker Swarm mode",
    "section": "Feedback",
    "text": "Feedback\nFeedback appreciated, @andreazonca\nI am also available to support US scientists to deploy scientific gateways through the XSEDE ECSS consultation program."
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "",
    "text": "Tested in June 2018 with Ubuntu 18.04 and Kubernetes 1.10\nUpdated in February 2018 with newer version of kubeadm-bootstrap, Kubernetes 1.9.2"
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#introduction",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#introduction",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Introduction",
    "text": "Introduction\nThe best infrastructure available to deploy Jupyterhub at scale is Kubernetes. Kubernetes provides a fault-tolerant system to deploy, manage and scale containers. The Jupyter team released a recipe to deploy Jupyterhub on top of Kubernetes, Zero to Jupyterhub. In this deployment both the hub, the proxy and all Jupyter Notebooks servers for the users are running inside Docker containers managed by Kubernetes.\nKubernetes is a highly sophisticated system, for smaller deployments (30/50 users, less then 10 servers), another option is to use the Docker Swarm mode, I covered this in a tutorial on how to deploy it on Jetstream.\nIf you are not already familiar with Kubernetes, better first read the section about tools in Zero to Jupyterhub.\nIn this tutorial we will be installing Kubernetes on 2 Ubuntu instances on the XSEDE Jetstream OpenStack-based cloud, configure permanent storage with the Ceph distributed filesystem and run the “Zero to Jupyterhub” recipe to install Jupyterhub on it."
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-two-virtual-machines",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-two-virtual-machines",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Setup two virtual machines",
    "text": "Setup two virtual machines\nFirst of all we need to create two Virtual Machines from the Jetstream Atmosphere admin panelI tested this on XSEDE Jetstream Ubuntu 16.04 image (with Docker pre-installed), for testing purposes “small” instances work, then they can be scaled up for production. You can name them master_node and node_1 for example. Make sure that port 80 and 443 are open to outside connections.\nThen you can SSH into the first machine with your XSEDE username with sudo privileges."
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#install-kubernetes",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#install-kubernetes",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Install Kubernetes",
    "text": "Install Kubernetes\nThe “Zero to Jupyterhub” recipe targets an already existing Kubernetes cluster, for example on Google Cloud. However the Berkeley Data Science Education Program team, which administers one of the largest Jupyterhub deployments to date, released a set of scripts based on the kubeadm tool to setup Kubernetes from scratch.\nThis will install all the Kubernetes services and configure the kubectl command line tool for administering and monitoring the cluster and the helm package manager to install pre-packaged services.\nSSH into the first server and follow the instructions at https://github.com/data-8/kubeadm-bootstrap to “Setup a Master Node” this will install a more recent version of Docker.\nOnce the initialization of the master node is completed, you should be able to check that several containers (pods in Kubernetes) are running:\nzonca@js-xxx-xxx:~/kubeadm-bootstrap$ sudo kubectl get pods --all-namespaces\nNAMESPACE     NAME                                                    READY     STATUS    RESTARTS   AGE\nkube-system   etcd-js-169-xx.jetstream-cloud.org                      1/1       Running   0          1m\nkube-system   kube-apiserver-js-169-xx.jetstream-cloud.org            1/1       Running   0          1m\nkube-system   kube-controller-manager-js-169-xx.jetstream-cloud.org   1/1       Running   0          1m\nkube-system   kube-dns-6f4fd4bdf-nxxkh                                3/3       Running   0          2m\nkube-system   kube-flannel-ds-rlsgb                                   1/1       Running   1          2m\nkube-system   kube-proxy-ntmwx                                        1/1       Running   0          2m\nkube-system   kube-scheduler-js-169-xx.jetstream-cloud.org            1/1       Running   0          2m\nkube-system   tiller-deploy-69cb6984f-77nx2                           1/1       Running   0          2m\nsupport       support-nginx-ingress-controller-k4swb                  1/1       Running   0          36s\nsupport       support-nginx-ingress-default-backend-cb84895fb-qs9pp   1/1       Running   0          36s\nMake also sure routing is working by accessing with your web browser the address of the Virtual Machine js-169-xx.jetstream-cloud.org and verify you are getting the error message default backend - 404.\nThen SSH to the other server and set it up as a worker following the instructions in “Setup a Worker Node” at https://github.com/data-8/kubeadm-bootstrap,\nOnce the setup is complete on the worker, log back in to the master and check that the worker joined Kubernetes:\nzonca@js-169-xx:~/kubeadm-bootstrap$ sudo kubectl get nodes\nNAME                             STATUS    ROLES     AGE       VERSION\njs-168-yyy.jetstream-cloud.org   Ready     &lt;none&gt;    1m        v1.9.2\njs-169-xx.jetstream-cloud.org    Ready     master    2h        v1.9.2"
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-permanent-storage-for-kubernetes",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-permanent-storage-for-kubernetes",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Setup permanent storage for Kubernetes",
    "text": "Setup permanent storage for Kubernetes\nThe cluster we just setup has no permament storage, so user data would disappear every time a container is killed. We woud like to provide users with a permament home that would be available across all of the Kubernetes cluster, so that even if a user container spawns again on a different server, the data are available.\nFirst we want to login again to Jetstream web interface and create 2 Volumes (for example 10 GB) and attach them one each to the master and to the first node, these will be automatically mounted on /vol_b, with no need of rebooting the servers.\nKubernetes has capability to provide Permanent Volumes but it needs a backend distributed file system. In this tutorial we will be using Rook which sets up the Ceph distributed filesystem across the nodes.\nWe can first use Helm to install the Rook services (I ran my tests with v0.6.1):\nsudo helm repo add rook-alpha https://charts.rook.io/alpha\nsudo helm install rook-alpha/rook\nThen check that the pods have started:\nzonca@js-xxx-xxx:~/kubeadm-bootstrap$ sudo kubectl get pods\nNAME                            READY     STATUS    RESTARTS   AGE\nrook-agent-2v86r                1/1       Running   0          1h\nrook-agent-7dfl9                1/1       Running   0          1h\nrook-operator-88fb8f6f5-tss5t   1/1       Running   0          1h\nOnce the pods have started we can actually configure the storage, copy this rook-cluster.yaml file to the master node. Better clone all of the repository as we will be using other files later.\nThe most important bits are:\n\ndataDirHostPath: this is a folder to save the Rook configuration, we can set it to /var/lib/rook\nstorage: directories: this is were data is stored, we can set this to /vol_b which is the default mount point of Volumes on Jetstream. This way we can more easily back those up or increase their size.\nversionTag: make sure this is the same as your rook version (you can find it with sudo helm ls)\n\nThen run it with:\nsudo kubectl create -f rook-cluster.yaml\nAnd wait for the services to launch:\nzonca@js-xxx-xxx:~/kubeadm-bootstrap$ sudo kubectl -n rook get pods\nNAME                              READY     STATUS    RESTARTS   AGE\nrook-api-68b87d48d5-xmkpv         1/1       Running   0          6m\nrook-ceph-mgr0-5ddd685b65-kw9bz   1/1       Running   0          6m\nrook-ceph-mgr1-5fcf599447-j7bpn   1/1       Running   0          6m\nrook-ceph-mon0-g7xsk              1/1       Running   0          7m\nrook-ceph-mon1-zbfqt              1/1       Running   0          7m\nrook-ceph-mon2-c6rzf              1/1       Running   0          6m\nrook-ceph-osd-82lj5               1/1       Running   0          6m\nrook-ceph-osd-cpln8               1/1       Running   0          6m\nThis step launches the distributed file system Ceph on all nodes.\nFinally we can create a new StorageClass which provides block storage for the pods to store data persistently, get rook-storageclass.yaml from the same repository we used before and execute with:\nsudo kubectl create -f rook-storageclass.yaml\nYou should now have the rook storageclass available:\nsudo kubectl get storageclass\nNAME         PROVISIONER\nrook-block   rook.io/block\n\n(Optional) Test Rook Persistent Storage\nOptionally, we can deploy a simple pod to verify that the storage system is working properly.\nYou can copy alpine-rook.yaml from Github and launch it with:\nsudo kubectl create -f alpine-rook.yaml\nIt is a very small pod with Alpine Linux that creates a 2 GB volume from Rook and mounts it on /data.\nThis creates a Pod with Alpine Linux that requests a Persistent Volume Claim to be mounted under /data. The Persistent Volume Claim specified the type of storage and its size. Once the Pod is created, it asks the Persistent Volume Claim to actually request Rook to prepare a Persistent Volume that is then mounted into the Pod.\nWe can verify the Persistent Volumes are created and associated with the pod, check:\nsudo kubectl get pv\nsudo kubectl get pvc\nsudo kubectl get logs alpine\nWe can get a shell in the pod with:\nsudo kubectl exec -it alpine  -- /bin/sh\naccess /data/ and make sure we can write some files.\nOnce you have completed testing, you can delete the pod and the Persistent Volume Claim with:\nsudo kubectl delete -f alpine-rook.yaml\nThe Persistent Volume will be automatically deleted by Kubernetes after a few minutes."
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-https-with-letsencrypt",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#setup-https-with-letsencrypt",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Setup HTTPS with letsencrypt",
    "text": "Setup HTTPS with letsencrypt\nWe need kube-lego to automatically get a HTTPS certificate from Letsencrypt, For more information see the Ingress section on the Zero to Jupyterhub Advanced topics.\nFirst we need to customize the Kube Lego configuration, edit the config_kube-lego_helm.yaml file from the repository and set your email address, then:\nsudo helm install stable/kube-lego --namespace=support --name=lego -f config_kube-lego_helm.yaml\nThen after you deploy Jupyterhub if you have some HTTPS trouble, you should check the logs of the kube-lego pod. First find the name of the pod with:\nsudo kubectl get pods -n support\nThen check its logs:\nsudo kubectl logs -n support lego-kube-lego-xxxxx-xxx"
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#install-jupyterhub",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#install-jupyterhub",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nRead all of the documentation of “Zero to Jupyterhub”, then download config_jupyterhub_helm.yaml from the repository and customize it with the URL of the master node (for Jetstream js-xxx-xxx.jetstream-cloud.org) and generate the random strings for security, finally run the Helm chart:\nsudo helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/\nsudo helm repo update\nsudo helm install jupyterhub/jupyterhub --version=v0.6 --name=jup \\\n    --namespace=jup -f config_jupyterhub_helm.yaml\nOnce you modify the configuration you can update the deployment with:\nsudo helm upgrade jup jupyterhub/jupyterhub -f config_jupyterhub_helm.yaml\n\nTest Jupyterhub\nConnect to the public URL of your master node instance at: https://js-xxx-xxx.jetstream-cloud.org\nTry to login with your XSEDE username and password and check if Jupyterhub works properly.\nIf something is wrong, check:\nsudo kubectl --namespace=jup get pods\nGet the name of the hub pod and check the logs:\nsudo kubectl --namespace=jup logs hub-xxxx-xxxxxxx\nCheck that Rook is working properly:\nsudo kubectl --namespace=jup get pv\nsudo kubectl --namespace=jup get pvc\nsudo kubectl --namespace=jup describe pvc claim-YOURXSEDEUSERNAME"
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#administration-tips",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#administration-tips",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Administration tips",
    "text": "Administration tips\n\nAdd more servers to Kubernetes\nWe can create more Ubuntu instances (with a volume attached) and add them to Kubernetes by repeating the same setup we performed on the first worker node. Once the node joins Kubernetes, it will be automatically used as a node for the distributed filesystem by Rook and be available to host user containers.\n\n\nRemove a server from Kubernetes\nLaunch first the kubectl drain command to move the currently active pods to other nodes:\nsudo kubectl get nodes\nsudo kubectl drain &lt;node name&gt;\nThen suspend or delete the instance on the Jetstream admin panel.\n\n\nConfigure a different authentication system\n“Zero to Jupyterhub” supports out of the box authentication with:\n\nXSEDE credentials with CILogon\nMany Campuses credentials with CILogon\nGlobus\nGoogle\n\nSee the documentation and modify config_jupyterhub_helm_v0.5.0.yaml accordingly."
  },
  {
    "objectID": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#acknowledgements",
    "href": "posts/2017-12-05-scalable-jupyterhub-kubernetes-jetstream.html#acknowledgements",
    "title": "Deploy scalable Jupyterhub with Kubernetes on Jetstream",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThe Jupyter team, in particular Yuvi Panda, for providing a great software platform and a easy-to-user resrouce for deploying it and for direct support in debugging my issues\nXSEDE Extended Collaborative Support Services for supporting part of my time to work on deploying Jupyterhub on Jetstream and providing computational time on Jetstream\nPacific Research Platform, in particular John Graham, Thomas DeFanti and Dmitry Mishin (SDSC) for access to their Kubernetes platform for testing\nXSEDE Jetstream’s Jeremy Fischer for prompt answers to my questions on Jetstream"
  },
  {
    "objectID": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html",
    "href": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html",
    "title": "Install custom Python environment on Jupyter Notebooks at NERSC",
    "section": "",
    "text": "NERSC has provided a JupyterHub instance for quite some time to all NERSC users. It is currently running on a dedicated large-memory node on Cori, so now it can access also data on Cori $SCRATCH, not only /project and $HOME. See their documentation"
  },
  {
    "objectID": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html#jupyter-notebooks-at-nersc",
    "href": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html#jupyter-notebooks-at-nersc",
    "title": "Install custom Python environment on Jupyter Notebooks at NERSC",
    "section": "",
    "text": "NERSC has provided a JupyterHub instance for quite some time to all NERSC users. It is currently running on a dedicated large-memory node on Cori, so now it can access also data on Cori $SCRATCH, not only /project and $HOME. See their documentation"
  },
  {
    "objectID": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html#customize-your-python-environment",
    "href": "posts/2017-12-21-custom-conda-python-jupyter-nersc.html#customize-your-python-environment",
    "title": "Install custom Python environment on Jupyter Notebooks at NERSC",
    "section": "Customize your Python environment",
    "text": "Customize your Python environment\nNERSC provides Anaconda in a Ubuntu container, of course the user doesn’t have permission to write to the Anaconda folder to install new packages.\nThe easiest way is to install a custom Python environment is to create another conda environment and then register the Kernel with Jupyter.\nCreate a new conda environment, best choice is /project if you have one, otherwise $HOME would work. Access http://jupyter.nersc.gov, open a terminal with “New”-&gt;“Terminal”.\nconda create --prefix $HOME/myconda python=3.6 ipykernel\nThis is the minimal requirement, you could just add anaconda to get all the latest packages, you can also specify conda-forge to install other packages, e.g.:\nsource activate myconda\nconda install -c conda-forge healpy\nRegister the kernel with the Jupyter Notebook:\nipython kernel install --name myconda --user\nThe name of the kernel specified here doesn’t need to be the same as the conda environment name, but it is simpler.\nOnce the conda environment is active, you can also install packages with pip.\nconda install pip\npip install somepackage"
  },
  {
    "objectID": "posts/2018-03-03-zarr-on-jetstream.html",
    "href": "posts/2018-03-03-zarr-on-jetstream.html",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "",
    "text": "Updated again in January 2019"
  },
  {
    "objectID": "posts/2018-03-03-zarr-on-jetstream.html#zarr",
    "href": "posts/2018-03-03-zarr-on-jetstream.html#zarr",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Zarr",
    "text": "Zarr\nZarr is a pretty new file format designed for cloud computing, see documentation and a webinar for more details.\nZarr is also supported by dask, the parallel computing framework for Python, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2018-03-03-zarr-on-jetstream.html#use-openstack-swift-on-jetstream-for-object-storage",
    "href": "posts/2018-03-03-zarr-on-jetstream.html#use-openstack-swift-on-jetstream-for-object-storage",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Use OpenStack swift on Jetstream for object storage",
    "text": "Use OpenStack swift on Jetstream for object storage\nJetstream also offers (currently in beta) access to object storage via OpenStack Swift. This is a separate service from the Jetstream Virtual Machines, so you do not need to spin any Virtual Machine dedicated to storing the data but just use the object storage already provided by Jetstream."
  },
  {
    "objectID": "posts/2018-03-03-zarr-on-jetstream.html#read-zarr-files-from-object-store",
    "href": "posts/2018-03-03-zarr-on-jetstream.html#read-zarr-files-from-object-store",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Read Zarr files from object store",
    "text": "Read Zarr files from object store\nIf somebody else has already made available some files on object store and set their visibility to “public”, anybody can read them.\nSee the example Notebook to read Zarr files.\nOpenStack Swift already provides an endpoint which has an interface compatible with Amazon S3, therefore we can directly use the S3FileSystem provided by s3fs.\nThen we can build a S3Map object which zarr and xarray can access. I removed the endpoint url from the Notebook to avoid test traffic. You can request it to the XSEDE helpdesk.\nIn this example I am using the distributed scheduler on a single node, you can scale up your computation having workers distributed on multiple nodes, just make sure that all the workers have access to the zarr, xarray, s3fs packages."
  },
  {
    "objectID": "posts/2018-03-03-zarr-on-jetstream.html#write-zarr-files-or-read-private-files",
    "href": "posts/2018-03-03-zarr-on-jetstream.html#write-zarr-files-or-read-private-files",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage",
    "section": "Write Zarr files or read private files",
    "text": "Write Zarr files or read private files\nIn this case we need authentication.\nFirst you need to ask to the XSEDE helpdesk API access to Jetstream, this also gives access to the Horizon interface, which has many advanced features that are not available in Atmosphere.\nConsider that credentials are different whether you are using the object store at IU or TACC, therefore make sure that credentials and JETSTREAM_SWIFT_ENDPOINT are consistent.\n\nCreate a bucket\nObject store systems are organized on buckets, which are like root folders of our filesystem. From the Horizon interface, we can choose Object Store -&gt; Containers (quite confusing way of referring to buckets in OpenStack). Here we can check content of existing buckets or create a new one.\n\n\nGet credentials\nFrom Horizon, choose the project you want to charge usage from the dropdown menu at the top.\nThen download the openstack RC file version 3 from: https://iu.jetstream-cloud.org/project/api_access/\nAt this point we need to transform it into Amazon-style credentials, you can do this on any host, not necessarily on Jetstream, install OpenStack client:\npip install python-openstackclient\nsource the openstackRC file, put the password, this is the TACC password (the same used to access Horizon), NOT the XSEDE Password.\nNow we can check the content of the bucket we created above:\nopenstack object list my_bucket\nNow create ec2 credentials with:\nopenstack ec2 credentials create\nThis is going to display AWS access key and AWS secret, we can save credentials in ~/.aws/config in the machine we want then use to write to object store.\n[default]\nregion=RegionOne\naws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\naws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nTest access\nWe can check if we can successfully login using s3fs, notice we do not use anon=True as we did before:\nimport s3fs\nfs = s3fs.S3FileSystem(client_kwargs=dict(endpoint_url=\"JETSTREAM_SWIFT_ENDPOINT\"))\nfs.ls(\"my_bucket\")\nMake sure that JETSTREAM_SWIFT_ENDPOINT does not include /swift/v1!\n\n\nRead a file from local filesystem and write to Object store\nSee this notebook as an example of writing to object store, first we make sure to have the necessary Python packages, then we use xarray to read data from NetCDF and then write back to Zarr first locally and then via s3fs to Openstack Swift.\nSee the Zarr documentation about how to tweak, compression, data transformations and chunking.\n\n\nTroubleshooting\nIn case anything doesn’t work, you can get the debug logging executing:\nimport boto3\nboto3.set_stream_logger(name='')\nbefore executing s3fs.\nAlso, in case you believe anything is not working in s3fs, here is a handy test script using only boto3."
  },
  {
    "objectID": "posts/2018-03-29-boinc-server-jetstream.html",
    "href": "posts/2018-03-29-boinc-server-jetstream.html",
    "title": "Install a BOINC server on Jetstream",
    "section": "",
    "text": "BOINC is the leading platform for volunteer computing.\nScientists can create a project on the platform and submit computational jobs that will be executed on computers of volunteers all over the world.\nIn this post we’ll deploy a BOINC server on Jetstream. All US scientists can get a free allocation on Jetstream via XSEDE.\nThe deployment will be based on the Docker setup developed by the Cosmology@Home project."
  },
  {
    "objectID": "posts/2018-03-29-boinc-server-jetstream.html#prepare-a-jetstream-virtual-machine",
    "href": "posts/2018-03-29-boinc-server-jetstream.html#prepare-a-jetstream-virtual-machine",
    "title": "Install a BOINC server on Jetstream",
    "section": "Prepare a Jetstream Virtual Machine",
    "text": "Prepare a Jetstream Virtual Machine\nFirst we login on the Atmosphere Jetstream control panel and create a new instance of Ubuntu 16.04 with Docker preinstalled, a “small” size is enough for testing.\n\n(Optional) Mount a Jetstream Volume for docker images\nIt is ideal to have a dedicated Jetstream Volume and mount it in the location where Docker stores its data. So we have more space, less usage of the root filesystem and no issues on the OS if we get out of disk space.\nWe can create a volume of 10/20 GB in the Jetstream control panel and attach it to the running Virtual Machine. This will be automatically mounted to /vol_b, we want to mount instead to /var/lib/docker:\nsudo systemctl stop docker\nsudo mv /var/lib/docker/* /vol_b/\nsudo umount /vol_b\nReplace /vol_b with /var/lib/docker in /etc/fstab, e.g.:\nzonca@js-xxx-xxx:~$ cat /etc/fstab\nLABEL=cloudimg-rootfs   /        ext4   defaults        0 0\n/dev/sdb /var/lib/docker ext4 defaults,nofail 0 2\nFinally:\nsudo mount /var/lib/docker\nsudo systemctl start docker\n\n\nUpdate Docker\nDocker in 16.04 is a bit old, we want to update it to a more recent version.\nWe also want to make sure to remove the old docker and docker-compose:\nsudo apt remove docker-compose docker\nThen install a recent version, we can follow the instructions from the docker website or use this script:\nhttps://gist.github.com/zonca/f5faba190f5285c68dad48e897622e90\nI adapted it from kubeadm-bootstrap.\nFinally install the latest docker-compose, see the documentation\nLast step, add your user to the docker group:\nsudo adduser $USER docker\nlogout and back in and make sure you can run docker commands without sudo:\ndocker ps\n\n\nInstall BOINC server via Docker\nFollow the instructions from boinc-server-docker to launch a test deployment, in the last step, specify a URL_BASE so that the deployment will be accessible from outside connections:\nURL_BASE=http://$(hostname) docker-compose up -d\nYou can check that the 3 containers are running with:\ndocker ps\nand inspect their logs with:\ndocker logs &lt;container_id&gt;\nAfter a few minutes you should be able to check that the server is running at the public address of your instance:\nhttp://js-xxx-xxx.jetstream-cloud.org/boincserver/"
  },
  {
    "objectID": "posts/2018-03-29-boinc-server-jetstream.html#optional-mount-jetstream-volumes-on-the-containers",
    "href": "posts/2018-03-29-boinc-server-jetstream.html#optional-mount-jetstream-volumes-on-the-containers",
    "title": "Install a BOINC server on Jetstream",
    "section": "(Optional) Mount Jetstream volumes on the containers",
    "text": "(Optional) Mount Jetstream volumes on the containers\nThe Docker compose recipe defines 3 Docker volumes:\n\nmysql: Data of the MySQL database\nproject: Files about the project\nresults: Result of the BOINC jobs\n\nthose volumes are managed internally by Docker and stored somewhere inside /var/lib/docker on the host node.\nDocker also allows to mount specific folders from the host into a container, if we back these folders by a Jetstream volume, we can have dedicated detachable Jetstream volumes that live independently from any virtual machine.\nLet’s start by mysql, the same process can then be replicated for the other resources.\nWe create another Jetstream volume from the Atmosphere, name it mysql and attach it to the virtual machine, this will be automatically mounted to /vol_c, we can rename it by:\nsudo umount /vol_c\nReplace vol_c with mysql in /etc/fstab, finally:\nsudo mount /mysql\nFinally you can modify the docker-compose.yml to use this folder instead of a Docker Volume:\nIn the volumes: section, remove mysql:, in the definition of the MySQL service, replace:\nvolumes:\n - \"mysql:/var/lib/mysql\"\nwith:\nvolumes:\n - \"/mysql:/var/lib/mysql\"\nSo that instead of using a Docker Volume named mysql is creating a bind-mount to /mysql on the host."
  },
  {
    "objectID": "posts/2018-03-29-boinc-server-jetstream.html#test-jobs",
    "href": "posts/2018-03-29-boinc-server-jetstream.html#test-jobs",
    "title": "Install a BOINC server on Jetstream",
    "section": "Test jobs",
    "text": "Test jobs\nOpen a terminal in the BOINC server container:\ndocker exec -it &lt;boincserver&gt; /bin/bash\n\n\nbin/boinc2docker_create_work.py \\\n    python:alpine python -c \"open('/root/shared/results/hello.txt','w').write('Hello BOINC')\"\nThen we can test a client connection and execution either with a standard BOINC desktop client or on another Jetstream instance.\n\nTest with a BOINC Desktop client\nFollow the instructions on the BOINC website to install a client for your OS, install also VirtualBox, then set as the URL of the BOINC server the URL of the server we just created.\n\n\nTest with a BOINC client in another Jetstream instance\nCreate another Ubuntu with Docker tiny instane on Jetstream, login,\nsudo adduser $USER docker\nWe need Virtualbox: sudo apt install virtualbox-dkms\nand reboot to make sure VirtualBox is active.\nURL=http://js-xxx-xxx.jetstream-cloud.org/boincserver/\ndocker exec boinc boinccmd --create_account $URL email password name\n\nstatus: Success\npoll status: operation in progress\npoll status: operation in progress\npoll status: operation in progress\naccount key: de9c4cc66b8c923d04f834a0609ae742\nWe can save the account key in a environment variable:\nURL=http://js-xxx-xxx.jetstream-cloud.org/boincserver/\nURL=http://js-xxx-xxx.jetstream-cloud.org/boincserver/\naccount_key=de9c4cc66b8c923d04f834a0609ae742\ndocker exec boinc boinccmd --project_attach $URL $account_key\nThen we can check the logs for the job being received and executed:\ndocker logs boinc\n30-Mar-2018 13:02:04 [boincserver] Started download of layer_e9e858f6a2ba5a3e5a04b5799ef2de1c21a58602ffd400838ed10599f1b4a42c.tar.manual.gz\n30-Mar-2018 13:02:06 [boincserver] Finished download of layer_10ffed26db733866a346caf7c79558e4addb23ae085a991b5e7237edaa69f8e2.tar.manual.gz\n30-Mar-2018 13:02:06 [boincserver] Finished download of layer_e9e858f6a2ba5a3e5a04b5799ef2de1c21a58602ffd400838ed10599f1b4a42c.tar.manual.gz\n30-Mar-2018 13:02:06 [boincserver] Started download of layer_0e650ab7661f993eff514b84c6e7b775f5be8c6dde8b63eb584f0f22ea24005f.tar.manual.gz\n30-Mar-2018 13:02:06 [boincserver] Started download of image_4fcaf5fb5f2b8230c53b5fd4c4325df00021d45272dc4bfbb2148e5ca91ac166.tar.manual.gz\n30-Mar-2018 13:02:07 [boincserver] Finished download of layer_0e650ab7661f993eff514b84c6e7b775f5be8c6dde8b63eb584f0f22ea24005f.tar.manual.gz\n30-Mar-2018 13:02:07 [boincserver] Finished download of image_4fcaf5fb5f2b8230c53b5fd4c4325df00021d45272dc4bfbb2148e5ca91ac166.tar.manual.gz\n30-Mar-2018 13:02:07 [boincserver] Starting task boinc2docker_3766_1522410497.503524_0\n30-Mar-2018 13:02:07 [boincserver] Sending scheduler request: To fetch work.\n30-Mar-2018 13:02:07 [boincserver] Requesting new tasks for CPU\n30-Mar-2018 13:02:08 [boincserver] Scheduler request completed: got 1 new tasks\n30-Mar-2018 13:02:12 [---] Vbox app stderr indicates CPU VM extensions disabled\n30-Mar-2018 13:02:13 [boincserver] Computation for task boinc2docker_3766_1522410497.503524_0 finished\n30-Mar-2018 13:02:13 [boincserver] Output file boinc2docker_3766_1522410497.503524_0_r207563194_0.tgz for task boinc2docker_3766_1522410497.503524_0 absent\n30-Mar-2018 13:02:13 [boincserver] Starting task boinc2docker_3766_1522410497.503524_1\n30-Mar-2018 13:02:18 [---] Vbox app stderr indicates CPU VM extensions disabled\n30-Mar-2018 13:02:18 [boincserver] Computation for task boinc2docker_3766_1522410497.503524_1 finished\n30-Mar-2018 13:02:18 [boincserver] Output file boinc2docker_3766_1522410497.503524_1_r1095010587_0.tgz for task boinc2docker_3766_1522410497.503524_1 absent"
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "",
    "text": "Make sure you have the DOI from ACM\nIf you have Latex: create a zip with sources, figures and .bbl (not .bib), no output PDF\nIf you have Word: export to PDF\nGo to https://arxiv.org/submit\nChoose the first option for license and “Computer Science” and “Distributed, Parallel, and Cluster Computing” for category\nIn Metadata set Comments as: “7 pages, 3 figures, PEARC ’18: Practice and Experience in Advanced Research Computing, July 22–26, 2018, Pittsburgh, PA, USA”\nMake sure you set the DOI or you violate ACM rules\nFollow instructions until you publish\n\nFollows the step-by-step version:"
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#quick-version",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#quick-version",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "",
    "text": "Make sure you have the DOI from ACM\nIf you have Latex: create a zip with sources, figures and .bbl (not .bib), no output PDF\nIf you have Word: export to PDF\nGo to https://arxiv.org/submit\nChoose the first option for license and “Computer Science” and “Distributed, Parallel, and Cluster Computing” for category\nIn Metadata set Comments as: “7 pages, 3 figures, PEARC ’18: Practice and Experience in Advanced Research Computing, July 22–26, 2018, Pittsburgh, PA, USA”\nMake sure you set the DOI or you violate ACM rules\nFollow instructions until you publish\n\nFollows the step-by-step version:"
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#why-upload-a-pre-print-to-arxiv",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#why-upload-a-pre-print-to-arxiv",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Why upload a pre-print to arXiv",
    "text": "Why upload a pre-print to arXiv\nJournals provide a Open Access option, but it is very expensive, however, they generally allow authors to upload manuscripts before copy-editing to non-profit pre-print servers like the arXiv. This makes your paper accessible to anybody without the need of any Journal subscription, you can also upload your work months before the conference proceedings are available.\nSee for example the page of my PEARC18 paper on the arXiv: https://arxiv.org/abs/1805.04781"
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#license",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#license",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "License",
    "text": "License\nBefore publishing any pre-print, you need to check on the Journal or Conference website if it is allowed and at what conditions.\nPEARC18 in particular publishes with ACM, therefore we can look at the author rights page on the ACM website.\nCurrently the requirements for posting a pre-print are:\n\nthe paper needs to be accepted and peer-reviewed\nthis is the version by the author, before copy-editing, if any, by the journal\nit needs a DOI pointing to the ACM version of the paper"
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#get-a-doi",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#get-a-doi",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Get a DOI",
    "text": "Get a DOI\nA DOI is generated once the author chooses a license. PEARC18 first authors should have received an email around May 10th with a link to the ACM website to choose a license. There are 3 choices, Open Access is quite expensive, but we do not need that, we are still allowed to post the pre-print even with any of the other 2 licenses, I personally recommend the “license” option, that does not transfer copyright to ACM. After completing this you should receive a DOI, which is a set of numbers of the form 10.1145/xxxxx.xxxxxx. Also remember to add the license text you will receive via email to the paper before going on with the upload."
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#prepare-your-latex-submission",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#prepare-your-latex-submission",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Prepare your Latex submission",
    "text": "Prepare your Latex submission\nThe arXiv requires the source for any Latex paper. If you are using the online platform Overleaf, click on “Project” and then “Download as zip” at the bottom. If you are using anything else, create a zip file with all the paper sources and figures, not the output PDF, also make sure that you include the .bbl file, not the .bib, so you need to compile your paper locally and add just the .bbl to the archive. Also, the arXiv dislikes large figures, so if you already know you have them, better resize or lower their quality before submission. Anyway you can just submit it as it is and check if they are accepted."
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#prepare-your-word-submission",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#prepare-your-word-submission",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Prepare your Word submission",
    "text": "Prepare your Word submission\nExport the paper as PDF."
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#upload-to-arxiv",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#upload-to-arxiv",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Upload to arXiv",
    "text": "Upload to arXiv\n\nGo to https://arxiv.org/submit, either login or create a new account.\nAt the submission page, fill the form, for license, the safest is to use the first option: “arXiv.org perpetual, non-exclusive license to distribute this article (Minimal rights required by arXiv.org)”\nFor “Archive and Subject Class”, choose “Computer Science” and “Distributed, Parallel, and Cluster Computing” unless in the list there is a more suitable field\nThen upload the Latex sources zip file or the conversion of the Word file to PDF.\nOnce you have uploaded the zip file, it shows you a list of the archive content, you can delete extra files are not needed to build the paper, if you used the Overleaf ACM template, remove sample-sigconf-authordraft.tex\nIf the paper doesn’t build, the arXiv displays the log, check for missing files or unsupported packages in particular, you can click “Add files” to upload different files\nIf the paper successfully builds, click on the “View” button to check that the PDF is fine\nIn the Metadata, complete the form, in the Comments, add also the conference information, for example “7 pages, 3 figures, PEARC ’18: Practice and Experience in Advanced Research Computing, July 22–26, 2018, Pittsburgh, PA, USA”\nStill in Metadata, make sure you add the DOI otherwise it is a violation of the conditions by ACM, the DOI is in the form 10.1145/xxxxxx.xxxx\nFinally check the preview and finalize your submission\nThe submission is not available immediately, it will first be in “Processing” stage and it will be published in the next few days, you’ll get an email with the publishing date and time."
  },
  {
    "objectID": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#update-your-submission",
    "href": "posts/2018-05-12-pearc18-paper-preprint-arxiv.html#update-your-submission",
    "title": "How to post a PEARC18 paper pre-print to Arxiv",
    "section": "Update your submission",
    "text": "Update your submission\n\nAnytime before publication you can update (overwrite) your submission\nAfter your pre-print is published you can update it at will but all previous versions will always be available on the arXiv servers.\n\nIn order to update the publication, login to the Arxiv and click on the “Replace” icon to update your paper with a new version."
  },
  {
    "objectID": "posts/2018-06-20-organize-code-simulations-nersc.html",
    "href": "posts/2018-06-20-organize-code-simulations-nersc.html",
    "title": "How to organize code and data for simulations at NERSC",
    "section": "",
    "text": "I recently improved my strategy for organizing code and data for simulations run at NERSC, I’ll write it here for reference."
  },
  {
    "objectID": "posts/2018-06-20-organize-code-simulations-nersc.html#libraries",
    "href": "posts/2018-06-20-organize-code-simulations-nersc.html#libraries",
    "title": "How to organize code and data for simulations at NERSC",
    "section": "Libraries",
    "text": "Libraries\nI mostly use Python (often with C/C++ extensions), so I first rely on the Anaconda module maintained by NERSC, currently python/3.6-anaconda-4.4.\nIf I need to add many more packages I can create a conda environment, but for just installing 1 or 2 packages I prefer to just add them to my PYTHONPATH.\nI have core libraries that I rely on and often modify to run my simulations, those should be installed on Global Common Software: /global/common/software/projectname which is specifically designed to access small files like Python packages. I generally create a subfolder and reference it with an environment variable:\n export PREFIX=/global/common/software/projectname/zonca/python_prefix\nThen I create a env.sh script in the source folder of the package (in Global Home) that loads the environment:\nmodule load python/3.6-anaconda-4.4\nexport PREFIX=/global/common/software/projectname/zonca/python_prefix\nexport PATH=$PREFIX/bin:$PATH\nexport LD_LIBRARY_PATH=$PREFIX/lib:$LD_LIBRARY_PATH\nexport PYTHONPATH=$PREFIX/lib/python3.6/site-packages:$PYTHONPATH\nThis environment is automatically propagated to the computing nodes when I submit a SLURM script, therefore I do not add any of these environment details to my SLURM scripts.\nThen I can install a package there with:\npython setup.py install --prefix=$PREFIX\nor from pip:\npip install apackage --prefix=$PREFIX\nIt is also common to install a newer version of a package which is already provided by the base environment:\npip install apackage --ignore-installed --upgrade --no-deps --prefix=$PREFIX"
  },
  {
    "objectID": "posts/2018-06-20-organize-code-simulations-nersc.html#simulations-slurm-scripts-and-configuration-files",
    "href": "posts/2018-06-20-organize-code-simulations-nersc.html#simulations-slurm-scripts-and-configuration-files",
    "title": "How to organize code and data for simulations at NERSC",
    "section": "Simulations SLURM scripts and configuration files",
    "text": "Simulations SLURM scripts and configuration files\nI first create a repository on Github for my simulations and clone it to my home folder at NERSC. I generally create a repository for each experiment, then I create a subfolder for each type of simulation I am working on.\nInside a folder I create parameters files to configure my run and slurm scripts to launch the simulations and put everything under version control immediately, I often create a Pull Request on Github and ask my collaborators to cross-check the configuration before a submit a run.\nSmaller input data files, even binaries, can be added for convenience to the Github repository.\nOnce a run has been validated, inside the simulation type folder I createa a subfolder runs/201806_details_about_run and add a README.md, this will include all the details about the simulation. I also tag both the core library I depend on and the simulation repository with the same name e.g.:\ngit tag -a 201806_details_about_run -m \"software version used for 201806_details_about_run\"\nI’ll also add the path at NERSC of the input data and output results.\nThen for future simulations I’ll keep modifying the SLURM scripts and parameter files but always have a reference to each previous version."
  },
  {
    "objectID": "posts/2018-06-20-organize-code-simulations-nersc.html#larger-input-data-and-output-data",
    "href": "posts/2018-06-20-organize-code-simulations-nersc.html#larger-input-data-and-output-data",
    "title": "How to organize code and data for simulations at NERSC",
    "section": "Larger input data and output data",
    "text": "Larger input data and output data\nLarger input data and outputs are not suitable for version control and should live in a SCRATCH filesystem. I always use the Global Scratch $CSCRATCH which is available both on Edison on Cori and also from the Jupyter Notebook environment at: https://jupyter.nersc.gov.\nI create a root folder for the project at:\n$CSCRATCH/projectname\nThen a subfolder for each simulation type:\n$CSCRATCH/projectname/simulation_type_1\n$CSCRATCH/projectname/simulation_type_2\nThen I symlink those inside the simulation repository as the folder out/:\ncd $HOME/projectname/simulation_type_1\nln -s $CSCRATCH/projectname/simulation_type_1 out\nTherefore I can setup my simulation software to save all results inside out/201806_details_about_run and this is going to be written to CSCRATCH.\nThis setup makes it very convenient to regularly backup everything to tape using cput which just backs up files that are not already on tape, e.g.:\ncd $CSCRATCH\nhsi\ncput -R projectname\nThis is going to synchronize the backup on tape with the latest results on CSCRATCH.\nI do the same for input files:\nmkdir $CSCRATCH/projectname/input_simulation_type_1\ncd $HOME/projectname/simulation_type_1\nln -s $CSCRATCH/projectname/input_simulation_type_1 input"
  },
  {
    "objectID": "posts/2018-07-22-updated-singularity-2.5-comet.html",
    "href": "posts/2018-07-22-updated-singularity-2.5-comet.html",
    "title": "Updated Singularity images for Comet",
    "section": "",
    "text": "Back in January 2017 I wrote a blog post about running Singularity on Comet.\nI recently needed to update all my container images to the latest scientific python packages, so I also took the opportunity to create both a Docker auto-build repository on DockerHub and a SingularityHub image.\nThose images have a working MPI installation which has the same MPI version of Comet so they can be used as a base for MPI programs.\nThe Docker image is based on the Jupyter Datascience notebook, therefore has Python, R and Julia. the Singularity image on SingularityHub has instead only Python. Anyway singularity pull also works with Docker containers, so also the Docker container can easily be turned into a singularity container.\nSee https://github.com/zonca/singularity-comet"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "",
    "text": "This tutorial is obsolete, check the updated version of the tutorial\nThe purpose of this tutorial series is to deploy Jupyterhub on top of Kubernetes on Jetstream. This material was presented as a tutorial at the Gateways 2018 conference, see also the slides on Figshare.\nCompared to my initial tutorial, I focused on improving automation. Instead of creating Jetstream instances via the Atmosphere web interface and then SSHing into the instances and run kubeadm based commands to setup Docker and Kubernetes we will:"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "Create Jetstream Virtual machines with Terraform",
    "text": "Create Jetstream Virtual machines with Terraform\nkubespray is able to deploy production-ready Kubernetes deployments and initially targeted only commercial cloud platforms.\nThey recently added support for Openstack via a Terraform recipe which is available in their Github repository.\nTerraform allows to execute recipes that describe a set of OpenStack resources and their relationship. In the context of this tutorial, we do not need to learn much about Terraform, we will configure and execute the recipe provided by kubespray.\n\nRequirements\nOn a Ubuntu 18.04 install python3-openstackclient with APT. Any other platform works as well, also install terraform by copying the correct binary to /usr/local/bin/, see https://www.terraform.io/intro/getting-started/install.html. The current version of the recipe requires Terraform 0.11.x, not the newest 0.12.\n\n\nRequest API access\nIn order to make sure your XSEDE account can access the Jetstream API, you need to contact the Helpdesk, see the instructions on the Jetstream Wiki. You will also receive your TACC password, which could be different than your XSEDE one (username is generally the same).\nLogin to the TACC Horizon panel at https://tacc.jetstream-cloud.org/dashboard, this is basically the low level web interface to OpenStack, a lot more complex and powerful than Atmosphere available at https://use.jetstream-cloud.org/application. Use tacc as domain, your TACC username (generally the same as your XSEDE username) and your TACC password.\nFirst choose the right project you would like to charge to in the top dropdown menu (see the XSEDE website if you don’t recognize the grant code).\nClick on Compute / API Access and download the OpenRC V3 authentication file to your machine. Source it typing:\nsource XX-XXXXXXXX-openrc.sh\nit should ask for your TACC password. This configures all the environment variables needed by the openstack command line tool to interface with the Openstack API.\nTest with:\nopenstack flavor list\nThis should return the list of available “sizes” of the Virtual Machines.\n\n\nClone kubespray\nI had to make a few modifications to kubespray to adapt it to Jetstream or backport bug fixes not merged yet, so currently better use my fork of kubespray:\ngit clone https://github.com/zonca/jetstream_kubespray\nSee an overview of my changes compared to the standard kubespray release 2.6.0.\n\n\nRun Terraform\nInside jetstream_kubespray, copy from my template:\nexport CLUSTER=$USER\ncp -LRp inventory/zonca_kubespray inventory/$CLUSTER\ncd inventory/$CLUSTER\nOpen and modify cluster.tf, choose your image and number of nodes. Make sure to change the network name to something unique, like the expanded form of $CLUSTER_network.\nYou can find suitable images (they need to be JS-API-Featured, you cannot use the same instances used in Atmosphere):\nopenstack image list | grep \"JS-API\"\nI already preconfigured the network UUID both for IU and TACC, but you can crosscheck looking for the public network in:\nopenstack network list\nInitialize Terraform:\nbash terraform_init.sh\nCreate the resources:\nbash terraform_apply.sh\nThe last output log of Terraform should contain the IP of the master node k8s_master_fips, wait for it to boot then SSH in with:\nssh ubuntu@$IP\nor centos@$IP for CentOS images.\nInspect with Openstack the resources created:\nopenstack server list\nopenstack network list\nYou can cleanup the virtual machines and all other Openstack resources (all data is lost) with bash terraform_destroy.sh."
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "Install Kubernetes with kubespray",
    "text": "Install Kubernetes with kubespray\nChange folder back to the root of the jetstream_kubespray repository,\nFirst make sure you have a recent version of ansible installed, you also need additional modules, so first run:\npip install -r requirements.txt\nIt is useful to create a virtualenv and install packages inside that. This will also install ansible, it is important to install ansible with pip so that the path to access the modules is correct. So remove any pre-installed ansible.\nThen following the kubespray documentation, we setup ssh-agent so that ansible can SSH from the machine with public IP to the others:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nTest the connection through ansible:\nansible -i inventory/$CLUSTER/hosts -m ping all\nIf a server is not answering to ping, first try to reboot it:\nopenstack server reboot $CLUSTER-k8s-node-nf-1\nOr delete it and run terraform_apply.sh to create it again.\ncheck inventory/$CLUSTER/group_vars/all.yml, in particular bootstrap_os, I setup ubuntu, change it to centos if you used the Centos 7 base image.\nDue to a bug in the recipe, run ( see details in the Troubleshooting notes below):\nexport OS_TENANT_ID=$OS_PROJECT_ID\nFinally run the full playbook, it is going to take a good 10 minutes:\nansible-playbook --become -i inventory/$CLUSTER/hosts cluster.yml\nIf the playbook fails with “cannot lock the administrative directory”, it is due to the fact that the Virtual Machine is automatically updating so it has locked the APT directory. Just wait a minute and launch it again. It is always safe to run ansible multiple times.\nIf the playbook gives any error, try to retry the above command, sometimes there are temporary failed tasks, Ansible is designed to be executed multiple times with consistent results.\nYou should have now a Kubernetes cluster running, test it:\n$ ssh ubuntu@$IP\n$ kubectl get pods --all-namespaces\nNAMESPACE       NAME                                                   READY     STATUS    RESTARTS   AGE\ncert-manager    cert-manager-78fb746bc7-w9r94                          1/1       Running   0          2h\ningress-nginx   default-backend-v1.4-7795cd847d-g25d8                  1/1       Running   0          2h\ningress-nginx   ingress-nginx-controller-bdjq7                         1/1       Running   0          2h\nkube-system     kube-apiserver-zonca-kubespray-k8s-master-1            1/1       Running   0          2h\nkube-system     kube-controller-manager-zonca-kubespray-k8s-master-1   1/1       Running   0          2h\nkube-system     kube-dns-69f4c8fc58-6vhhs                              3/3       Running   0          2h\nkube-system     kube-dns-69f4c8fc58-9jn25                              3/3       Running   0          2h\nkube-system     kube-flannel-7hd24                                     2/2       Running   0          2h\nkube-system     kube-flannel-lhsvx                                     2/2       Running   0          2h\nkube-system     kube-proxy-zonca-kubespray-k8s-master-1                1/1       Running   0          2h\nkube-system     kube-proxy-zonca-kubespray-k8s-node-nf-1               1/1       Running   0          2h\nkube-system     kube-scheduler-zonca-kubespray-k8s-master-1            1/1       Running   0          2h\nkube-system     kubedns-autoscaler-565b49bbc6-7wttm                    1/1       Running   0          2h\nkube-system     kubernetes-dashboard-6d4dfd56cb-24f98                  1/1       Running   0          2h\nkube-system     nginx-proxy-zonca-kubespray-k8s-node-nf-1              1/1       Running   0          2h\nkube-system     tiller-deploy-5c688d5f9b-fpfpg                         1/1       Running   0          2h\nCompare that you have all those services running also in your cluster. We have also configured NGINX to proxy any service that we will later deploy on Kubernetes, test it with:\n$ wget localhost\n--2018-09-24 03:01:14--  http://localhost/\nResolving localhost (localhost)... 127.0.0.1\nConnecting to localhost (localhost)|127.0.0.1|:80... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2018-09-24 03:01:14 ERROR 404: Not Found.\nError 404 is a good sign, the service is up and serving requests, currently there is nothing to deliver. Finally test that the routing through the Jetstream instance is working correctly by opening your browser and test that if you access js-XX-XXX.jetstream-cloud.org you also get a default backend - 404 message. If any of the tests hangs or cannot connect, there is probably a networking issue."
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#next",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#next",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "Next",
    "text": "Next\nNext you can explore the kubernetes deployment to learn more about how you deploy resources in the second part of my tutorial or skip it and proceed directly to the third and final part of the tutorial and deploy Jupyterhub and configure it with HTTPS.\n\nTroubleshooting notes\nFor future reference, disregard this.\nFailing ansible task: openstack_tenant_id is missing\nfixed with: export OS_TENANT_ID=$OS_PROJECT_ID, this should be fixed once https://github.com/kubernetes-incubator/kubespray/pull/2783 is merged, anyway this is not blocking.\nFailing task Write cacert file:\nNOTE: had to cherry-pick a commit from https://github.com/kubernetes-incubator/kubespray/pull/3280, this will be unnecessary once this is fixed upstream"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "(Optional) Setup kubectl locally",
    "text": "(Optional) Setup kubectl locally\nWe also set kubectl_localhost: true and kubeconfig_localhost: true. so that kubectl is installed on your local machine\nit also copies admin.conf to:\ninventory/$CLUSTER/artifacts\nnow copy that to ~/.kube/config\nthis has an issue, it has the internal IP of the Jetstream master. We cannot replace it with the public floating ip because the certificate is not valid for that. Best workaround is to replace it with 127.0.0.1 inside ~/.kube/config at the server: key. Then make a SSH tunnel:\nssh ubuntu@$IP -f -L 6443:localhost:6443 sleep 3h\n\n-f sends the process in the background\nexecuting sleep for 3 hours makes the tunnel automatically close after 3 hours, otherwise -N would keep the tunnel permanently open"
  },
  {
    "objectID": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#optional-setup-helm-locally",
    "href": "posts/2018-09-23-jetstream_kubernetes_kubespray.html#optional-setup-helm-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 1/3",
    "section": "(Optional) Setup helm locally",
    "text": "(Optional) Setup helm locally\nssh into the master node, check helm version with:\nhelm version\nDownload the same binary version from the release page on Github and copy the binary to /url/local/bin.\nhelm ls"
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "",
    "text": "All of the following assumes you are logged in to the master node of the Kubernetes cluster deployed with kubespray and checked out the repository:\nhttps://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream"
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#install-jupyterhub",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#install-jupyterhub",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nFirst run\nbash create_secrets.sh\nto create the secret strings needed by JupyterHub then edit its output secrets.yaml to make sure it is consistent, edit the hosts lines if needed. For example, supply the Jetstream DNS name of the master node js-XXX-YYY.jetstream-cloud.org (XXX and YYY are the last 2 groups of the floating IP of the instance AAA.BBB.XXX.YYY). See part 2, “Publish service externally with ingress”.\nbash configure_helm_jupyterhub.sh\nbash install_jhub.sh\nCheck some preliminary pods running with:\nkubectl get pods -n jhub\nOnce the proxy is running, even if hub is still in preparation, you can check in browser, you should get “Service Unavailable” which is a good sign that the proxy is working."
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#customize-jupyterhub",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#customize-jupyterhub",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Customize JupyterHub",
    "text": "Customize JupyterHub\nAfter JupyterHub is deployed and integrated with Cinder for persistent volumes, for any other customizations, first authentication, you are in good hands as the Zero-to-Jupyterhub documentation is great.\nThe only setup that could be peculiar to the deployment on top of kubespray is setup with HTTPS, see the next section."
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-https-with-letsencrypt",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-https-with-letsencrypt",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Setup HTTPS with letsencrypt",
    "text": "Setup HTTPS with letsencrypt\nthis is outdated as of March 2020 Follow this tutorial to deploy letsencrypt\nLetsencrypt won’t issue certificates anymore with the old version of cert-manager installed by our version of Kubespray. I disabled the installation of cert-manager, please follow this newer tutorial on how to setup a recent cert-manager\nKubespray instead of installing kube-lego, installs certmanager to handle HTTPS certificates.\nFirst we need to create a Issuer, set your email inside setup_https_kubespray/https_issuer.yml and create it with the usual:\nkubectl create -f setup_https_kubespray/https_issuer.yml\nThen we can manually create a HTTPS certificate, certmanager can be configured to handle this automatically, but as we only need a domain this is pretty quick, edit setup_https_kubespray/https_certificate.yml and set the domain name of your master node, then create the certificate resource with:\nkubectl create -f setup_https_kubespray/https_certificate.yml\nFinally we can configure JupyterHub to use this certificate, first edit your secrets.yaml following as an example the file setup_https_kubespray/example_letsencrypt_secrets.yaml, then update your JupyterHub configuration running again:\nbash install_jhub.sh"
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-https-with-custom-certificates",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-https-with-custom-certificates",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Setup HTTPS with custom certificates",
    "text": "Setup HTTPS with custom certificates\nIn case you have custom certificates for your domain, first create a secret in the jupyterhub namespace with:\nkubectl create secret tls cert-secret --key ssl.key --cert ssl.crt -n jhub\nThen setup ingress to use this in secrets.yaml:\ningress:\n  enabled: true\n  hosts:\n    - js-XX-YYY.jetstream-cloud.org\n  tls:\n  - hosts:\n    - js-XX-YYY.jetstream-cloud.org\n    secretName: cert-secret\nEventually, you may need to update the certificate. This can be achieved with:\nkubectl create secret tls cert-secret --key ssl.key --cert ssl.crt -n jhub \\\n    --dry-run -o yaml | kubectl apply -f -"
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-custom-http-headers",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#setup-custom-http-headers",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Setup custom HTTP headers",
    "text": "Setup custom HTTP headers\nAfter you have deployed JupyterHub, edit ingress:\nkubectl edit ingress -n jhub\nAdd a configuration-snippet line inside annotations:\nmetadata:\n  annotations:\n    kubernetes.io/tls-acme: \"true\"\n    nginx.ingress.kubernetes.io/configuration-snippet: |\n      more_set_headers \"X-Frame-Options: DENY\";\n      more_set_headers \"X-Xss-Protection: 1\";\nThis doesn’t require to restart or modify any other resource."
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#modify-the-kubernetes-cluster-size",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#modify-the-kubernetes-cluster-size",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Modify the Kubernetes cluster size",
    "text": "Modify the Kubernetes cluster size\nSee a followup short tutorial on scaling Kubernetes manually."
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#persistence-of-user-data",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#persistence-of-user-data",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Persistence of user data",
    "text": "Persistence of user data\nWhen a JupyterHub user logs in for the first time, a Kubernetes PersistentVolumeClaim of the size defined in the configuration file is created. This is a Kubernetes resource that defines a request for storage.\nkubectl get pvc -n jhub\nNAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\nclaim-zonca   Bound    pvc-c469967a-3968-11e9-aaad-fa163e9c7d08   1Gi        RWO            standard       2m34s\nhub-db-dir    Bound    pvc-353114a7-3968-11e9-aaad-fa163e9c7d08   1Gi        RWO            standard       6m34s\nInspecting the claims we find out that we have a claim for the user and a claim to store the database of JupyterHub. Currently they are already Bound because they are already satistied.\nThose claims are then satisfied by our Openstack Cinder provisioner to create a Openstack volume and wrap it into a Kubernetes PersistentVolume resource:\nkubectl get pv -n jhub\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE\npvc-353114a7-3968-11e9-aaad-fa163e9c7d08   1Gi        RWO            Delete           Bound    jhub/hub-db-dir    standard                8m52s\npvc-c469967a-3968-11e9-aaad-fa163e9c7d08   1Gi        RWO            Delete           Bound    jhub/claim-zonca   standard                5m4s\nThis corresponds to Openstack volumes automatically mounted onto the node that is executing the user pod:\n+--------------------------------------+-------------------------------------------------------------+-----------+------+----------------------------------------------+\n| ID                                   | Name                                                        | Status    | Size | Attached to                                  |\n+--------------------------------------+-------------------------------------------------------------+-----------+------+----------------------------------------------+\n| e6eddaaa-d40d-4832-addd-a05343ec3a80 | kubernetes-dynamic-pvc-c469967a-3968-11e9-aaad-fa163e9c7d08 | in-use    |    1 | Attached to zonca-k8s-node-nf-1 on /dev/sdc  |\n| 00f1e822-8098-4633-804e-46ba44d7de7e | kubernetes-dynamic-pvc-353114a7-3968-11e9-aaad-fa163e9c7d08 | in-use    |    1 | Attached to zonca-k8s-node-nf-1 on /dev/sdb  |\nIf the user disconnects, the Openstack volume is un-attached from the instance but it is not delete and it is mounted back, optionally on another instance, if the user logs back in.\n\nDelete and reinstall JupyterHub\nHelm release deleted:\nhelm delete --purge jhub\nAs long as you do not delete the whole namespace, the volumes are not deleted, therefore you can re-deploy the same version or a newer version using helm and the same volume is mounted back for the user\n\n\nDelete and recreate Openstack instances\nWhen we run terraform to delete all Openstack resources:\nbash terraform_destroy.sh\nthis does not include the Openstack volumes that are created by the Kubernetes persistent volume provisioner.\nIn case we are interested in keeping the same ip address, run instead:\nbash terraform_destroy_keep_floatingip.sh\nThe problem is that if we recreate Kubernetes again, it doesn’t know how to link the Openstack volume to the Persistent Volume of a user. Therefore we need to backup the Persistent Volumes and the Persistent Volume Claims resources before tearing Kubernetes down:\nkubectl get pvc -n jhub -o yaml &gt; pvc.yaml\nkubectl get pv -n jhub -o yaml &gt; pv.yaml\nI recommend always to run kubectl on the local machine instead of the master node, because if you delete the master instance you loose any temporary modification to your scripts. In this case, even more importantly, if you are running on the master node please backup pvc.yaml and pv.yaml locally before running terraform_destroy.sh or they will be wiped out.\nThen open the files with a text editor and delete the Persistent Volume and the Persistent Volume Claim related to hub-db-dir.\nEdit pv.yaml and set:\n  persistentVolumeReclaimPolicy:Retain\nOtherwise if you create the PV first, it is deleted because there is no PVC.\nAlso remove the claimRef section of all the volumes in pv.yaml, otherwise you get the error “two claims are bound to the same volume, this one is bound incorrectly” on the PVC.\nNow we can proceed to create the cluster again and then restore the volumes with:\nkubectl apply -f pv.yaml\nkubectl apply -f pvc.yaml"
  },
  {
    "objectID": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#feedback",
    "href": "posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub.html#feedback",
    "title": "Deploy JupyterHub on Kubernetes deployment on Jetstream created with Kubespray 3/3",
    "section": "Feedback",
    "text": "Feedback\nFeedback on this is very welcome, please open an issue on the Github repository or email me at zonca on the domain of the San Diego Supercomputer Center (sdsc.edu)."
  },
  {
    "objectID": "posts/2018-10-26-pandas-astro-example.html",
    "href": "posts/2018-10-26-pandas-astro-example.html",
    "title": "Advanced pandas with Astrophysics example Notebook",
    "section": "",
    "text": "Taught a lesson today on advanced python and pandas based on an example application in Astrophysics with simulations of data from the Planck Satellite, features also a Binder button to run it yourself. Jupyter Notebook available at: https://github.com/zonca/pandas-astro-example under CC-BY"
  },
  {
    "objectID": "posts/2018-12-12-twofactor_ucsd.html",
    "href": "posts/2018-12-12-twofactor_ucsd.html",
    "title": "Setup two factor authentication for UCSD, and Lastpass",
    "section": "",
    "text": "Starting at the end of January 2019 UCSD requires every employee to have activated two factor authentication.\nGo over to https://duo-registration.ucsd.edu to register your devices and https://twostep.ucsd.edu to read more details.\nHere some suggestions after I have used this for a few months.\nThe most convenient option is definitely to have the Duo application installed on your phone, so that once you try to login it sends a notification to your phone, you click accept and you’re done.\nSecond best is to use the Duo or the Google Authenticator app to generate codes, then you can copy those codes into the login form, and this is anyway useful for VPN access, you choose the “2 Steps secured - allthroughucsd” option, type your password followed by a comma and the code, otherwise just the password and get a push notification on your primary device.\nThen you can just add a mobile number and receive a text or add a landline and receive a call.\nI also recommend to buy a security key and add it as a authentication option at https://duo-registration.ucsd.edu, either Google Titan or a Yubico key (I have a Titan), you can keep it always with you so that if you don’t have your phone or the phone battery is dead, you can plug the security key in your USB port on the laptop and click on its button to authenticate.\nAnther option is to request a fob token, a device that generates and displays timed codes and that is independent of a phone, see instructions on the UCSD website. They say there are only a limited number available and you need to be prepared to justify why you are requesting one."
  },
  {
    "objectID": "posts/2018-12-12-twofactor_ucsd.html#other-services",
    "href": "posts/2018-12-12-twofactor_ucsd.html#other-services",
    "title": "Setup two factor authentication for UCSD, and Lastpass",
    "section": "Other services",
    "text": "Other services\nNow that you already have Duo installed on your phone, I recommend to also activate two factor auth on all other services:\n\nXSEDE\nNERSC\nGoogle\nGithub\nAmazon\nMicrosoft\nDropbox\n\nConsider that most of them just request the second step verification if you are on a new device, so you need to do the verification just once in a while and it provides a lot of security. Many of those also support the security key."
  },
  {
    "objectID": "posts/2018-12-12-twofactor_ucsd.html#password-handling-with-lastpass",
    "href": "posts/2018-12-12-twofactor_ucsd.html#password-handling-with-lastpass",
    "title": "Setup two factor authentication for UCSD, and Lastpass",
    "section": "Password handling with Lastpass",
    "text": "Password handling with Lastpass\nUpdate October 2019: Fed up of using Lastpass, their interface is clunky and slow, both in Chrome and Android, I switched to Bitwarden. Way better, it also allows sharing with another user, only downside is that the do not offer Duo push 2FA for free, you need premium, but still supports using Duo as a token generator.\nAs you are into security, just go all the way and also install a password manager. UCSD provides free enterprise accounts for all employees, see the details.\nWith Lastpass, you just remember 1 strong password to descrypt all of your other passwords. If you ever used the Google Chrome builtin password manager, this is way way better.\nYou install the Lastpass extension on your browsers and the Lastpass app on your phone.\nThe only issue with Lastpass is that by default the Lastpass app on the smartphone automatically logsout every 30 minutes or so, so you have to re-authenticate very often. This is due to UCSD having configured it too strictly. I recommend to have a personal account and save all of the passwords in the personal account and then link it from the Enterprise account. Now from the desktop/laptop browsers you can use your Enterprise account, from the smartphone app instead use the personal account.\nYou can also automatically import your Google Chrome passwords into Lastpass.\nNow you have no excuse to re-use the same password, automatically generate a 20 char random password and save it in Lastpass.\n\nSave one-time codes\nWhen you activate two factor auth on Google/Github and many other services, they also give you some one-time codes that you can use to login to the service if you do not have access to your phone, you can save them as “Notes” into the related account inside Lastpass.\n\n\nActivate 2 factor auth for Lastpass\nYou should also activate 2 factor auth in Lastpass, it also supports Duo so the configuration is similar to the configuration for UCSD. Only issue is that they do not support a security key here, so you can only add your smartphone."
  },
  {
    "objectID": "posts/2019-01-24-zarr_jetstream_2019.html",
    "href": "posts/2019-01-24-zarr_jetstream_2019.html",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage, 2019",
    "section": "",
    "text": "This is an updated version of the 2018 edition"
  },
  {
    "objectID": "posts/2019-01-24-zarr_jetstream_2019.html#zarr",
    "href": "posts/2019-01-24-zarr_jetstream_2019.html#zarr",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage, 2019",
    "section": "Zarr",
    "text": "Zarr\nZarr is a pretty new file format designed for cloud computing, see documentation and a webinar for more details.\nZarr is also supported by dask, the parallel computing framework for Python, and the Dask team implemented storage backends for Google Cloud Storage and Amazon S3."
  },
  {
    "objectID": "posts/2019-01-24-zarr_jetstream_2019.html#use-openstack-swift-on-jetstream-for-object-storage",
    "href": "posts/2019-01-24-zarr_jetstream_2019.html#use-openstack-swift-on-jetstream-for-object-storage",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage, 2019",
    "section": "Use OpenStack swift on Jetstream for object storage",
    "text": "Use OpenStack swift on Jetstream for object storage\nJetstream also offers (currently in beta) access to object storage via OpenStack Swift. This is a separate service from the Jetstream Virtual Machines, so you do not need to spin any Virtual Machine dedicated to storing the data but just use the object storage already provided by Jetstream."
  },
  {
    "objectID": "posts/2019-01-24-zarr_jetstream_2019.html#read-zarr-files-from-object-store",
    "href": "posts/2019-01-24-zarr_jetstream_2019.html#read-zarr-files-from-object-store",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage, 2019",
    "section": "Read Zarr files from object store",
    "text": "Read Zarr files from object store\nIf somebody else has already made available some files on object store and set their visibility to “public”, anybody can read them.\nSee the example Notebook to read Zarr files.\nOpenStack Swift already provides an endpoint which has an interface compatible with Amazon S3, therefore we can directly use the S3FileSystem provided by s3fs.\nThen we can build a S3Map object which zarr and xarray can access. I removed the endpoint url from the Notebook to avoid test traffic. You can request it to the XSEDE helpdesk.\nIn this example I am using the distributed scheduler on a single node, you can scale up your computation having workers distributed on multiple nodes, just make sure that all the workers have access to the zarr, xarray, s3fs packages."
  },
  {
    "objectID": "posts/2019-01-24-zarr_jetstream_2019.html#write-zarr-files-or-read-private-files",
    "href": "posts/2019-01-24-zarr_jetstream_2019.html#write-zarr-files-or-read-private-files",
    "title": "Use the distributed file format Zarr on Jetstream Swift object storage, 2019",
    "section": "Write Zarr files or read private files",
    "text": "Write Zarr files or read private files\nIn this case we need authentication.\nFirst you need to ask to the XSEDE helpdesk API access to Jetstream, this also gives access to the Horizon interface, which has many advanced features that are not available in Atmosphere.\n\nCreate a bucket\nObject store systems are organized on buckets, which are like root folders of our filesystem. From the Horizon interface, we can choose Object Store -&gt; Containers (quite confusing way of referring to buckets in OpenStack). Here we can check content of existing buckets or create a new one.\n\n\nGet credentials\nFrom Horizon, choose the project you want to charge usage from the dropdown menu at the top.\nThen download the openstack RC file version 3 from: https://iu.jetstream-cloud.org/project/api_access/\nAt this point we need to transform it into Amazon-style credentials, you can do this on any host, not necessarily on Jetstream, install OpenStack client:\npip install python-openstackclient\nsource the openstackRC file, put the password, this is the TACC password (the same used to access Horizon), NOT the XSEDE Password.\nNow we can check the content of the bucket we created above:\nopenstack object list my_bucket\nNow create ec2 credentials with:\nopenstack ec2 credentials create\nThis is going to display AWS access key and AWS secret, we can save credentials in ~/.aws/config in the machine we want then use to write to object store.\n[default]\nregion=RegionOne\naws_access_key_id=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\naws_secret_access_key=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n\nTest access\nWe can check if we can successfully login using s3fs, notice we do not use anon=True as we did before:\nimport s3fs\nfs = s3fs.S3FileSystem(client_kwargs=dict(endpoint_url=\"JETSTREAM_SWIFT_ENDPOINT\"))\nfs.ls(\"my_bucket\")\nMake sure that JETSTREAM_SWIFT_ENDPOINT does not include /swift/v1!\n\n\nRead a file from local filesystem and write to Object store\nSee this notebook as an example of writing to object store, first we make sure to have the necessary Python packages, then we use xarray to read data from NetCDF and then write back to Zarr first locally and then via s3fs to Openstack Swift.\nSee the Zarr documentation about how to tweak, compression, data transformations and chunking."
  },
  {
    "objectID": "posts/2019-02-22-kubernetes-monitoring.html",
    "href": "posts/2019-02-22-kubernetes-monitoring.html",
    "title": "Kubernetes monitoring with Dashboard, Prometheus, and Grafana",
    "section": "",
    "text": "Install with helm:\nhttps://github.com/helm/charts/tree/master/stable/prometheus-operator\nincludes grafana already"
  },
  {
    "objectID": "posts/2019-02-22-kubernetes-monitoring.html#monitoring-prometheus-and-grafana",
    "href": "posts/2019-02-22-kubernetes-monitoring.html#monitoring-prometheus-and-grafana",
    "title": "Kubernetes monitoring with Dashboard, Prometheus, and Grafana",
    "section": "",
    "text": "Install with helm:\nhttps://github.com/helm/charts/tree/master/stable/prometheus-operator\nincludes grafana already"
  },
  {
    "objectID": "posts/2019-04-20-jetstream_kubernetes_monitoring.html",
    "href": "posts/2019-04-20-jetstream_kubernetes_monitoring.html",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "",
    "text": "See the updated version of this tutorial\nUpdated September 2020\nIn a production Kubernetes deployment it is necessary to make it easier to monitor the status of the cluster effectively. Kubernetes provides Prometheus to gather data from the different components of Kubernetes and Grafana to access those data and provide real-time plotting and inspection capability. Moreover, they both provide systems to send alerts in case some conditions on the state of the cluster are met, i.e. using more than 90% of RAM or CPU.\nThe only downside is that the pods that handle monitoring consume some resource themselves, so this could be significant for small clusters below 5 nodes or so, but shouldn’t be a problem for typical larger production deployments.\nBoth Prometheus and Grafana can be installed separately with Helm recipes or using the Prometheus operator Helm recipe, however those deployments do not have any preconfigured dashboards, it is easier to get started thanks to the kube-prometheus project, which not only installs Prometheus and Grafana, but also preconfigures about 10 different Grafana dashboards to explore in depth the status of a Kubernetes cluster.\nThe main issue is that customizing it is really complicated, it requires modifying jsonnet templates and recompiling them with a jsonnet builder which requires go, however I don’t foresee the need to do that for most users.\nUnfortunately it is not based on Helm, so you need to first checkout the repository:\nand then follow the instructions in the documentation, copied here for convenience:\nWait a few minutes, then:\nThis creates several pods in the monitoring namespace:\nThen you can setup forwarding on your laptop to export grafana locally:\nAccess localhost:3000 with your browser and you should be able to navigate through all the statistics of your cluster, see for example this screenshot. The credentials are user admin and password admin.\nFrom the “Home” page, you can access all the preconfigured dashboards by clicking on the top “Home” button, it will show a searchable list of all available dashboards."
  },
  {
    "objectID": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#access-the-ui-from-a-different-machine",
    "href": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#access-the-ui-from-a-different-machine",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Access the UI from a different machine",
    "text": "Access the UI from a different machine\nIn case you are running the configuration on a remote server and you would like to access the Grafana UI (or any other service) from your laptop, you can install kubectl also your my laptop, then copy the .kube/config to the laptop with:\n scp -r KUBECTLMACHINE:~/.kube/config ~/.kube\nand run:\n ssh ubuntu@$IP -f -L 6443:localhost:6443 sleep 3h &\nfrom the laptop and then run the port-forward command locally on the laptop."
  },
  {
    "objectID": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#monitor-jupyterhub",
    "href": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#monitor-jupyterhub",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Monitor JupyterHub",
    "text": "Monitor JupyterHub\nOnce we have deployed JupyterHub with Helm, we can pull up the “namespace” monitor and select the jhub namespace to visualize resource usage but also usage requests and limits of all pods created by JupyterHub and its users. See a screenshot below.\n\n\n\nScreenshot of the Grafana namespace UI"
  },
  {
    "objectID": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#setup-alerts",
    "href": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#setup-alerts",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Setup alerts",
    "text": "Setup alerts\nGrafana supports email alerts, but it needs a SMTP server, and it is not easy to setup and to avoid being filtered as spam. The easiest way is to setup an alert to Slack, and optionally be notified via email of Slack messages.\nFollow the instructions for slack on the Grafana documentation\n\nCreate a Slack app, name it e.g. Grafana\nAdd feature “Incoming webhook”\nCreate a incoming webhook in the workspace and channel your prefer on Slack\nIn the Grafana Alerting menu, set the webhook incoming url, the channel name\n\n\n\n\nScreenshot of the Grafana slack notification"
  },
  {
    "objectID": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#configure-ingress",
    "href": "posts/2019-04-20-jetstream_kubernetes_monitoring.html#configure-ingress",
    "title": "Kubernetes monitoring with Prometheus and Grafana",
    "section": "Configure ingress",
    "text": "Configure ingress\nIt is also possible to expose Grafana to the web via an Ingress, the easiest is to have a dedicated URL just for grafana (different from the URL of JupyterHub), in this case, see an example ingress. It is important that it is in the monitoring namespace.\nThe configuration also supports HTTPS, for that to work you also need to create an Issuer in the namespace monitoring (also rename the secret key), for more details see the tutorial on deploying letsencrypt."
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "",
    "text": "The Kubernetes Cluster Autoscaler is a service that runs within a Kubernetes cluster and when there are not enough resources to accomodate the pods that are queued to run, it contacts the API of the cloud provider to create more Virtual Machines to join the Kubernetes Cluster.\nInitially the Cluster Autoscaler only supported commercial cloud provides, but back in March 2019 a user contributed Openstack support based on Magnum.\nFirst step you should have a Magnum-based deployment running on Jetstream, see my recent tutorial about that.\nTherefore you should also have already a copy of the repository of all configuration files checked out on your local machine that you are using to interact with the openstack API, if not:\nand enter the folder dedicated to the autoscaler:"
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#setup-credentials",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#setup-credentials",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "Setup credentials",
    "text": "Setup credentials\nWe first create the service account needed by the autoscaler to interact with the Kubernetes API:\nkubectl create -f cluster-autoscaler-svcaccount.yaml \nThen we need to provide all connection details for the autoscaler to interact with the Openstack API, those are contained in the cloud-config of our cluster available in the master node and setup by Magnum. Get the IP of your master node from:\nopenstack server list\nIP=xxx.xxx.xxx.xxx\nNow ssh into the master node and access the cloud-config file:\nssh fedora@$IP\ncat /etc/kubernetes/cloud-config \nnow copy the [Global] section at the end of cluster-autoscaler-secret.yaml on the local machine. Also remove the line of ca-file\nkubectl create -f cluster-autoscaler-secret.yaml"
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#launch-the-autoscaler-deployment",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#launch-the-autoscaler-deployment",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "Launch the Autoscaler deployment",
    "text": "Launch the Autoscaler deployment\nCreate the Autoscaler deployment:\nkubectl create -f cluster-autoscaler-deployment-master.yaml\nAlternatively, I also added a version for a cluster where we are not deploying pods on master cluster-autoscaler-deployment.yaml.\nCheck that the deployment is active:\nkubectl -n kube-system get pods\nNAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ncluster-autoscaler     1         1         1            0           10s\nAnd check its logs:\nkubectl -n kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2\n\nI0905 05:29:21.589062       1 leaderelection.go:217] attempting to acquire leader lease  kube-system/cluster-autoscaler...\nI0905 05:29:39.412449       1 leaderelection.go:227] successfully acquired lease kube-system/cluster-autoscaler\nI0905 05:29:43.896557       1 magnum_manager_heat.go:293] For stack ID 17ab3ae7-1a81-43e6-98ec-b6ffd04f91d3, stack name is k8s-lu3bksbwsln3\nI0905 05:29:44.146319       1 magnum_manager_heat.go:310] Found nested kube_minions stack: name k8s-lu3bksbwsln3-kube_minions-r4lhlv5xuwu3, ID d0590824-cc70-4da5-b9ff-8581d99c666b\nIf you redeploy the cluster and keep a older authentication, you’ll see “Authentication failed” in the logs of the autoscaler pod, you need to update the secret every time you redeploy the cluster."
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#test-the-autoscaler",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#test-the-autoscaler",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "Test the autoscaler",
    "text": "Test the autoscaler\nNow we need to produce a significant load on the cluster so that the autoscaler is triggered to request Openstack Magnum to create more Virtual Machines.\nWe can create a deployment of the NGINX container (any other would work for this test):\nkubectl create deployment autoscaler-demo --image=nginx\nAnd then create a large number of replicas:\nkubectl scale deployment autoscaler-demo --replicas=300\nWe are using 2 nodes with a large amount of memory and CPU, so they can accommodate more then 200 of those pods. The rest remains in the queue:\nkubectl get deployment autoscaler-demo\nNAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nautoscaler-demo   300       300       300          213         18m\nAnd this triggers the autoscaler:\nkubectl -n kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2\n\nI0905 05:34:47.401149       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2\nI0905 05:34:49.267280       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&gt;2\nI0905 05:35:22.222387       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_IN_PROGRESS status\nCheck also in the Openstack API:\nopenstack coe cluster list\n+------+------+---------+------------+--------------+--------------------+\n| uuid | name | keypair | node_count | master_count | status             |\n+------+------+---------+------------+--------------+--------------------+\n| 09fcf| k8s  | comet   |          2 |            1 | UPDATE_IN_PROGRESS |\n+------+------+---------+------------+--------------+--------------------+\nIt takes about 4 minutes for a new VM to boot, be configured by Magnum and join the Kubernetes cluster.\nChecking the logs again should show another line:\nI0912 17:18:28.290987       1 magnum_nodegroup.go:67] Waited for cluster UPDATE_COMPLETE status\nThen you should have all 3 nodes available:\nkubectl get nodes\nNAME                        STATUS   ROLES    AGE   VERSION\nk8s-6bawhy45wr5t-master-0   Ready    master   38m   v1.11.1\nk8s-6bawhy45wr5t-minion-0   Ready    &lt;none&gt;   38m   v1.11.1\nk8s-6bawhy45wr5t-minion-1   Ready    &lt;none&gt;   30m   v1.11.1\nand all 300 NGINX containers deployed:\nkubectl get deployments\nNAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nautoscaler-demo   300       300       300          300         35m\nYou can also test scaling down by scaling back the number of NGINX containers to only a few and check in the logs of the autoscaler that this process triggers the scale-down process.\nIn cluster-autoscaler-deployment-master.yaml I have configured the scale down process to trigger just after 1 minute, to simplify testing. For production, better increase this to 10 minutes or more. Check the documentation of Cluster Autoscaler 1.14 for all other available options."
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#note-about-the-cluster-autoscaler-container",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#note-about-the-cluster-autoscaler-container",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "Note about the Cluster Autoscaler container",
    "text": "Note about the Cluster Autoscaler container\nThe Magnum provider was added in Cluster Autoscaler 1.15, however this version is not compatible with Kubernetes 1.11 which is currently available on Jetstream. Therefore I have taken the development version of Cluster Autoscaler 1.14 and compiled it myself. I also noticed that the scale down process was not working due to incompatible IDs when the Cloud Provider tried to lookup the ID of a Minion in the Stack. I am now directly using the MachineID instead of going through these indices. This version is available in my fork of autoscaler and it is built into docker containers on the zonca/k8s-cluster-autoscaler-jetstream repository on Docker Hub. The image tags are the short version of the repository git commit hash.\nI build the container using the run_gobuilder.sh and run_build_autoscaler_container.sh scripts included in the repository."
  },
  {
    "objectID": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#note-about-images-used-by-magnum",
    "href": "posts/2019-09-12-jetstream_kubernetes_magnum_autoscaler.html#note-about-images-used-by-magnum",
    "title": "Deploy Cluster Autoscaler for Kubernetes on Jetstream",
    "section": "Note about images used by Magnum",
    "text": "Note about images used by Magnum\nI have tested this deployment using the Fedora-Atomic-27-20180419 image on Jetstream at Indiana University. The Fedora Atomic 28 image had a long hang-up during boot and took more than 10 minutes to start and that caused timeout in the autoscaler and anyway it would have been too long for a user waiting to start a notebook.\nI also tried updating the Fedora Atomic 28 image with sudo atomic host upgrade and while this fixed the slow startup issue, it generated a broken Kubernetes installation, i.e. the Kubernetes services didn’t detect the master node as part of the cluster, kubectl get nodes only showed the minion."
  },
  {
    "objectID": "posts/2019-03-24-folder-inherit-group-permission.html",
    "href": "posts/2019-03-24-folder-inherit-group-permission.html",
    "title": "Inherit group permission in folder",
    "section": "",
    "text": "I have googled this so many times…\nOn shared systems, like Supercomputers, you often belong to many different Unix groups, and that membership allows you to access data from specific projects you are working on and you can share data with your collaborators.\nIf you set SGID on a folder, any folder of file created in that folder will automatically belong to the Unix group of that folder, and not your default group. You first set the right group on the folder, recursively so that older files will get the right permissions:\nchown -R somegroup sharedfolder\nThen you set the SGID so future files will automatically belong to somegroup:\nchmod g+s sharedfolder\nThis is very useful for example in the /project filesystem at NERSC, you can set the SGID so that every file that is copied to the shared /project filesystem is accessible by other collaborators.\nRelated to this is also the default umask, most systems by default give “read” permission for the group, so setting SGID is enough, otherwise it is also necessary to configure umask properly."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "",
    "text": "This tutorial is obsolete, please checkout the updated version at https://zonca.dev/2020/05/kubernetes-jupyterhub-jetstream-magnum.html.\nThis tutorial deploys Kubernetes on Jetstream with Magnum and then JupyterHub on top of that using zero-to-jupyterhub.\nIn my previous tutorials I deployed Kubernetes using Kubespray. The main driver to using Magnum is that there is support for autoscaling, i.e. create and destroy Openstack instances based on the load on JupyterHub. I haven’t tested that yet, though, that will come in a following tutorial.\nMagnum is a technology built into Openstack to deploy Container Orchestration engines based on templates. The main difference with kubespray is that is way less configurable, the user does not have access to modify those templates but has just a number of parameters to set. Instead Kubespray is based on ansible and the user has full control of how the system is setup, it also supports having more High Availability features like multiple master nodes. On the other hand, the ansible recipe takes a very long time to run, ~30 min, while Magnum creates a cluster in about 10 minutes."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#setup-access-to-the-jetstream-api",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#setup-access-to-the-jetstream-api",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Setup access to the Jetstream API",
    "text": "Setup access to the Jetstream API\nFirst install the OpenStack client, please use these exact versions, also please run at Indiana, which currently has the Rocky release of Openstack, the TACC deployment has an older release of Openstack.\npip install python-openstackclient==3.16 python-magnumclient==2.10\nLoad your API credentials from openrc.sh, check documentation of the Jetstream wiki for details.\nYou need to have a keypair uploaded to Openstack, this just needs to be done once per account. See the Jetstream documentation under the section “Upload SSH key - do this once”."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Create the cluster with Magnum",
    "text": "Create the cluster with Magnum\nAs usual, checkout the repository with all the configuration files on the machine you will use the Jetstream API from, typically your laptop.\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\ncd jupyterhub-deploy-kubernetes-jetstream\ncd kubernetes_magnum\nNow we are ready to use Magnum to first create a cluster template and then the actual cluster, edit first create_cluster.sh and set the parameters of the cluster on the top. Also make sure to set the keypair name. Finally run:\nbash create_network.sh\nbash create_template.sh\nbash create_cluster.sh\nI have setup a test cluster with only 1 master node and 1 normal node but you can modify that later.\nCheck the status of your cluster, after about 10 minutes, it should be in state CREATE_COMPLETE:\nopenstack coe cluster show k8s\n\nConfigure kubectl locally\nInstall the kubectl client locally, first check the version of the master node:\nopenstack server list # find the floating public IP of the master node (starts with 149_\nIP=149.xxx.xxx.xxx\nssh fedora@$IP\nkubectl version\nNow install the same version following the Kubernetes documentation\nNow configure kubectl on your laptop to connect to the Kubernetes cluster created with Magnum:\nmkdir kubectl_secret\ncd kubectl_secret\nopenstack coe cluster config k8s\nThis downloads a configuration file and the required certificates.\nand returns export KUBECONFIG=/absolute/path/to/config\nSee also the update_kubectl_secret.sh script to automate this step, but it requires to already have setup the environment variable.\nexecute that and then:\nkubectl get nodes"
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#configure-storage",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#configure-storage",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Configure storage",
    "text": "Configure storage\nMagnum configures a provider that knows how to create Kubernetes volumes using Openstack Cinder, but does not configure a storageclass, we can do that with:\nkubectl create -f storageclass.yaml\nWe can test this by creating a Persistent Volume Claim:\nkubectl create -f persistent_volume_claim.yaml\n\nkubectl describe pv\n\nkubectl describe pvc\nName:            pvc-e8b93455-898b-11e9-a37c-fa163efb4609\nLabels:          failure-domain.beta.kubernetes.io/zone=nova\nAnnotations:     kubernetes.io/createdby: cinder-dynamic-provisioner\n                 pv.kubernetes.io/bound-by-controller: yes\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/cinder\nFinalizers:      [kubernetes.io/pv-protection]\nStorageClass:    standard\nStatus:          Bound\nClaim:           default/pvc-test\nReclaim Policy:  Delete\nAccess Modes:    RWO\nCapacity:        5Gi\nNode Affinity:   &lt;none&gt;\nMessage:         \nSource:\n    Type:       Cinder (a Persistent Disk resource in OpenStack)\n    VolumeID:   2795724b-ef11-4053-9922-d854107c731f\n    FSType:     \n    ReadOnly:   false\n    SecretRef:  nil\nEvents:         &lt;none&gt;\nWe can also test creating an actual pod with a persistent volume and check that the volume is successfully mounted and the pod started:\nkubectl create -f ../alpine-persistent-volume.yaml\nkubectl describe pod alpine\n\nNote about availability zones\nBy default Openstack servers and Openstack volumes are created in different availability zones. This created an issue with the default Magnum templates because we need to modify the Kubernetes scheduler policy to allow this. Kubespray does this by default, so I created a fix to be applied to the Jetstream Magnum templates, this needs to be re-applied after every Openstack upgrade."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#install-helm",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#install-helm",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Install Helm",
    "text": "Install Helm\nThe Kubernetes deployment from Magnum is not as complete as the one out of Kubespray, we need to setup helm and the NGINX ingress ourselves. We would also need to setup a system to automatically deploy HTTPS certificates, I’ll add this later on.\nFirst install the Helm client on your laptop, make sure you have configured kubectl correctly.\nThen we need to create a service account to give enough privilege to Helm to reconfigure the cluster:\nkubectl create -f tiller_service_account.yaml\nThen we can create the tiller pod inside Kubernetes:\nhelm init --service-account tiller --wait --history-max 200\nkubectl get pods --all-namespaces\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   coredns-78df4bf8ff-f2xvs                   1/1     Running   0          2d\nkube-system   coredns-78df4bf8ff-pnj7g                   1/1     Running   0          2d\nkube-system   heapster-74f98f6489-xsw52                  1/1     Running   0          2d\nkube-system   kube-dns-autoscaler-986c49747-2m64g        1/1     Running   0          2d\nkube-system   kubernetes-dashboard-54cb7b5997-c2vwx      1/1     Running   0          2d\nkube-system   openstack-cloud-controller-manager-tf5mc   1/1     Running   3          2d\nkube-system   tiller-deploy-6b5cd64488-4fkff             1/1     Running   0          20s\nAnd check that all the versions agree:\nhelm version\nClient: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"}\nServer: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"}"
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#setup-nginx-ingress",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#setup-nginx-ingress",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Setup NGINX ingress",
    "text": "Setup NGINX ingress\nWe need to have the NGINX web server to act as front-end to the services running inside the Kubernetes cluster.\n\nOpen HTTP and HTTPS ports\nFirst we need to open the HTTP and HTTPS ports on the master node, you can either connect to the Horizon interface, create new rule named http_https, then add 2 rules, in the Rule drop down choose HTTP and HTTPS; or from the command line:\nopenstack security group create http_https\nopenstack security group rule create --ingress --protocol tcp --dst-port 80 http_https \nopenstack security group rule create --ingress --protocol tcp --dst-port 443 http_https \nThen you can find the name of the master node in openstack server list then add this security group to that instance:\nopenstack server add security group  k8s-xxxxxxxxxxxx-master-0 http_https\n\n\nInstall NGINX ingress with Helm\nbash install_nginx_ingress.sh\nNote, the documentation says we should add this annotation to ingress with kubectl edit ingress -n jhub, but I found out it is not necessary:\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\nIf this is correctly working, you should be able to run curl localhost from the master node and get a Default backend: 404 message."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Install JupyterHub",
    "text": "Install JupyterHub\nFinally, we can go back to the root of the repository and install JupyterHub, first create the secrets file:\nbash create_secrets.sh\nThen edit secrets.yaml and modify the hostname under hosts to display the hostname of your master Jetstream instance, i.e. if your instance public floating IP is aaa.bbb.xxx.yyy, the hostname should be js-xxx-yyy.jetstream-cloud.org (without http://).\nYou should also check that connecting with your browser to js-xxx-yyy.jetstream-cloud.org shows default backend - 404, this means NGINX is also reachable from the internet, i.e. the web port is open on the master node.\nFinally:\nbash configure_helm_jupyterhub.sh\nbash install_jhub.sh\nConnect with your browser to js-xxx-yyy.jetstream-cloud.org to check if it works."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Issues and feedback",
    "text": "Issues and feedback\nPlease open an issue on the repository to report any issue or give feedback. Also you find out there there what I am working on next."
  },
  {
    "objectID": "posts/2019-06-14-jetstream_kubernetes_magnum.html#acknowledgments",
    "href": "posts/2019-06-14-jetstream_kubernetes_magnum.html#acknowledgments",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany thanks to Jeremy Fischer and Mike Lowe for solving all my tickets, this required a lot of work on their end to make it working."
  },
  {
    "objectID": "posts/2019-09-23-batch_notebook_execution.html",
    "href": "posts/2019-09-23-batch_notebook_execution.html",
    "title": "Execute Jupyter Notebooks not interactively",
    "section": "",
    "text": "Over the years, I have explored how to scale up easily computation through Jupyter Notebooks by executing them not-interactively, possibily parametrized and remotely. This is mostly for reference.\n\nnbsubmit is a Python package which has Python API to send a local notebook for execution on a remote SLURM cluster, for example Comet, see an example. This project is not maintained right now.\nBack in 2017 I tested submitting notebooks to Open Science Grid, see the batch-notebooks-condor repository\nBack in 2016 I created scripts to template a Jupyter Notebook and launch SLURM jobs, see slurm.shared.template and runipyloop.sh"
  },
  {
    "objectID": "posts/2019-12-02-organize_calendars_large_collaboration.html",
    "href": "posts/2019-12-02-organize_calendars_large_collaboration.html",
    "title": "Organize calendars for a large scientific collaboration",
    "section": "",
    "text": "Many scientific collaborations have a central calendar, often hosted on Google Calendar, to coordinate Teleconferences, meetings and events across timezones.\n\nThe issue\nMost users are only interested in a small subset of the events, however Google Calendar does not allow them to subscribe to single events. The central calendar admin could invite each person to events, but that requires lots of work.\nSo, users either subscribe to the whole calendar, but then have a huge clutter of un-interesting events, or copy just a subset of the events to their calendars, but loose track of any rescheduling of the original event.\n\n\nProposed solution\nI recommend to split the events across multiple calendars, for example one for each working group, or any other categorization where most users would be interested in all events in a calendar. And possibly a “General” category with events that should interest the whole collaboration.\nStill, we can embed all of the calendars in a single webpage, see an example below where 2 calendars (Monday and Tuesday telecon calendars) are visualized together, see the Google Calendar documentation.\n\n\nUsers can click on the bottom “Add to Google Calendar” button and subscribe to a subset or all the calendars. See the screenshot below, .\nAs an additional benefit, we can compartimentalize permissions more easily, e.g. leads of a working group get writing access only to their relevant calendar/calendars."
  },
  {
    "objectID": "posts/2020-03-09-pelican-to-fastpages.html",
    "href": "posts/2020-03-09-pelican-to-fastpages.html",
    "title": "Migrate from Pelican to Fastpages",
    "section": "",
    "text": "I have been using the Pelican static website generator for a few years, hosting the content on Github, automatically build on push via Travis-CI and deploy on Github pages to zonca.github.io.\nI am a heavy Jupyter Notebook user so once I saw the announcement of Fastpages I decided it was time to switch. I loved the idea of having Jupyter Notebooks built-in and not added via plugins, also great idea to use Github actions.\nOnly issue I found was that you cannot setup Fastpages on username.github.io, so went for using a custom domain name instead."
  },
  {
    "objectID": "posts/2020-03-09-pelican-to-fastpages.html#import-content",
    "href": "posts/2020-03-09-pelican-to-fastpages.html#import-content",
    "title": "Migrate from Pelican to Fastpages",
    "section": "Import content",
    "text": "Import content\nI created a script, in Python of course, to modify the front matter of the markdown posts from the Pelican formatting to Jekyll, see pelican_to_jekyll.py. It also renames the files, because Jekyll expects a date at the beginning of filenames."
  },
  {
    "objectID": "posts/2020-03-09-pelican-to-fastpages.html#setup-paginate",
    "href": "posts/2020-03-09-pelican-to-fastpages.html#setup-paginate",
    "title": "Migrate from Pelican to Fastpages",
    "section": "Setup paginate",
    "text": "Setup paginate\nCurrently Fastpages doesn’t support pagination for the homepage, but implemented a workaround.\nUpdate 12 March 2020: Now fastpages supports pagination natively! see the documentation"
  },
  {
    "objectID": "posts/2020-03-09-pelican-to-fastpages.html#redirect-from-the-old-github-pages-blog",
    "href": "posts/2020-03-09-pelican-to-fastpages.html#redirect-from-the-old-github-pages-blog",
    "title": "Migrate from Pelican to Fastpages",
    "section": "Redirect from the old Github Pages blog",
    "text": "Redirect from the old Github Pages blog\nI modified the permalinks of Fastpages so that I have the same URLs in the old and new websites, just the domain changes. Github pages does not support custom rewriting rules, so I modified the Pelican template to put a custom redirection tag in each HTML header.\nIn the Pelican template article.html, in the &lt;header&gt; section I added:\n{% raw %}\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://zonca.dev/{{ article.url }}\"&gt;\n&lt;link rel=\"canonical\" href=\"https://zonca.dev/{{ article.url }}\"&gt;\n{% endraw %}\nSo that Pelican regenerated all the articles with their original address and automatically redirects upon access. The canonical link hopefully helps with SEO.\nDid the same with the index.html template to redirect the homepage, this depends on your template:\n&lt;meta http-equiv=\"refresh\" content=\"0; URL=https://zonca.dev\"&gt;\n&lt;link rel=\"canonical\" href=\"https://zonca.dev/\"&gt;"
  },
  {
    "objectID": "posts/2020-03-09-pelican-to-fastpages.html#screenshots-of-the-old-blog",
    "href": "posts/2020-03-09-pelican-to-fastpages.html#screenshots-of-the-old-blog",
    "title": "Migrate from Pelican to Fastpages",
    "section": "Screenshots of the old blog",
    "text": "Screenshots of the old blog\nYeah, for posterity, growing older I get more nostalgic.\nThe homepage:\n\n\n\nOld blog homepage\n\n\nA section of an article page:\n\n\n\nOld blog article page"
  },
  {
    "objectID": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html",
    "href": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html",
    "title": "Setup HTTPS on Kubernetes with Letsencrypt",
    "section": "",
    "text": "This tutorial is obsolete since September 2023, see the updated tutorial.\nUpdated in August 2022: Add patching script to run on master node\nUpdated in March 2022: changes for Kubernetes 1.22, I am now creating a Cluster Issuer, which works on all namespaces, notice the related change in the configuration of JupyterHub.\nIn this tutorial we will deploy cert-manager in Kubernetes to automatically provide SSL certificates to JupyterHub (and other services).\nFirst make sure your payload, for example JupyterHub, is working without HTTPS, so that you check that the ports are open, Ingress is working, and JupyterHub itself can accept connections.\nLet’s follow the cert-manager documentation, for convenience I pasted the commands below:\nOnce we have cert-manager setup we can create a Issuer in the jhub workspace, (first edit the yml and add your email address):\nAfter this, we can display all the resources in the cert-manager namespace to check that the services and pods are running:\nThe result should be something like:"
  },
  {
    "objectID": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html#bind-the-pods-to-the-master-node",
    "href": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html#bind-the-pods-to-the-master-node",
    "title": "Setup HTTPS on Kubernetes with Letsencrypt",
    "section": "Bind the pods to the master node",
    "text": "Bind the pods to the master node\nIn Jetstream 2 there are routing restrictions which allow Cert Manager to run only from the master node, see the details on Github. At least when the nodes do not have floating IPs, if all your Virtual Machines have a floating IP, you can safely skip this step.\nUnidata has contributed the script they created to patch the 3 Cert Manager pods to have them run on the master node, we can apply it with:\ncd setup_https\nbash deploymentPatch.sh\nThen verify that the pods are redeployed on master:\nkubectl -n cert-manager get pods -o wide"
  },
  {
    "objectID": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html#setup-jupyterhub",
    "href": "posts/2020-03-13-setup-https-kubernetes-letsencrypt.html#setup-jupyterhub",
    "title": "Setup HTTPS on Kubernetes with Letsencrypt",
    "section": "Setup JupyterHub",
    "text": "Setup JupyterHub\nThen we modify the JupyterHub ingress configuration to use this Issuer, modify secrets.yaml to:\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    cert-manager.io/cluster-issuer: \"letsencrypt\"\n  hosts:\n      - js-XXX-YYY.jetstream-cloud.org\n  tls:\n      - hosts:\n         - js-XXX-YYY.jetstream-cloud.org\n        secretName: certmanager-tls-jupyterhub\nFinally update the JupyterHub deployment rerunning the deployment script (no need to delete it):\nbash install_jhub.sh\nAfter a few minutes we should have a certificate resource available:\n&gt; kubectl get certificate --all-namespaces\n\nNAMESPACE   NAME                         READY     SECRET                       AGE\njhub        certmanager-tls-jupyterhub   True      certmanager-tls-jupyterhub   11m\nfor newer versions, check the certificaterequest resource instead:\nkubectl get certificaterequest --all-namespaces\nNAMESPACE   NAME                                   READY   AGE\njhub        certmanager-tls-jupyterhub-781206586   True    9m5s"
  },
  {
    "objectID": "posts/2020-04-20-ecss-symposium-kubernetes-jetstream.html",
    "href": "posts/2020-04-20-ecss-symposium-kubernetes-jetstream.html",
    "title": "ECSS Symposium introduction to cloud computing with Jetstream and deployment of Kubernetes",
    "section": "",
    "text": "ECSS Symposium, April 2020, Web presentation to the XSEDE Extended Collaborative Support Services.\nIt is a 1-hour session introducing cloud computing on Jetstream and then explaining how to deploy Kubernetes and JupyterHub on Jetstream.\n\nGoogle doc slides\nRecording of the talk on Youtube"
  },
  {
    "objectID": "posts/2020-05-27-readthedocs-conda.html",
    "href": "posts/2020-05-27-readthedocs-conda.html",
    "title": "Minimal example of readthedocs configuration for conda",
    "section": "",
    "text": "Prompted by the announced better support of conda in readthedocs, more memory! I setup a Python package with conda to automatically build the documentation on readthedocs.\nI had some trouble because I couldn’t find a minimal example that gives all the necessary configuration options, for example if python / install is not provided, the project is not even built.\nSee these files on Gist"
  },
  {
    "objectID": "posts/2020-06-01-twitter-blocklist.html",
    "href": "posts/2020-06-01-twitter-blocklist.html",
    "title": "Import and export list of blocked users on Twitter",
    "section": "",
    "text": "Back in 2015, Twitter provided the feature to export and import lists of blocked users, unfortunately they discontinued this service.\nIn order to fill this void, https://blocktogether.org/ provides a great service to allow users to subscribe to public block-lists that are always kept updated by some maintainers, unfortunately they have capacity issues and needed to impose limits on number of subscribers and list length.\nI have written a small Python script that can provide a self-service import and export functionality using the Twitter API.\nSee https://github.com/zonca/twitter_blocklist"
  },
  {
    "objectID": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html",
    "href": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "",
    "text": "Important: This tutorial is obsolete, please refer to the new version from February 2023\nIn this tutorial I’ll show how to create a data volume on Jetstream and share it using a NFS server to all JupyterHub users. All JupyterHub users run as the jovyan user, therefore each folder in the shared filesystem can be either read-only, or writable by every user. The main concern is that a user could delete by mistake data of another user, however the users still have access to their own home folder."
  },
  {
    "objectID": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-the-nfs-server",
    "href": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-the-nfs-server",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test the NFS server",
    "text": "Test the NFS server\nEdit test_nfs_mount.yaml to set the right IP for the NFS server, then:\nkubectl create -f test_nfs_mount.yaml\nand access the terminal to test:\nbash ../terminal_pod.sh test-nfs-mount\ndf -h\n\n...\n10.254.204.67:/       9.8G   36M  9.8G   1% /share\n...\nWe have the root user, we can use the terminal to copy or rsync data into the shared volume. We can also create writable folders owned by the user 1000 which maps to jovyan in JupyterHub:\nsh-4.2# mkdir readonly_folder\nsh-4.2# touch readonly_folder/aaa\nsh-4.2# mkdir writable_folder\nsh-4.2# chown 1000:100 writable_folder\nsh-4.2# ls -l /share\ntotal 24\ndrwx------. 2 root root  16384 Jul 10 06:32 lost+found\ndrwxr-xr-x. 2 root root   4096 Jul 10 06:43 readonly_folder\ndrwxr-xr-x. 2 1000 users  4096 Jul 10 06:43 writable_folder"
  },
  {
    "objectID": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#preserve-the-data-volume-across-redeployments",
    "href": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#preserve-the-data-volume-across-redeployments",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Preserve the data volume across redeployments",
    "text": "Preserve the data volume across redeployments\nThe NFS data volume could contain a lot of data that you would want to preserve in case you need to completely tear down the Kubernetes cluster.\nFirst we find out what is the ID of the PersistentVolume associated with the NFS volume:\nkubectl get pv | grep nfs\npvc-ee1f02aa-11f8-433f-806f-186f6d622a30   10Gi       RWO            Delete           Bound    default/nfs-share-folder-claim   standard                5m55s\nThen you can save the PersistentVolume and the PersistentVolumeClaim to YAML:\nkubectl get pvc nfs-share-folder-claim -o yaml &gt; existing_nfs_volume_claim.yaml\nkubectl get pv pvc-ee1f02aa-11f8-433f-806f-186f6d622a30 -o yaml &gt; existing_nfs_volume.yaml\nNext we can delete the servers directly from Openstack, be careful not to delete the PersistentVolume or the PersistentVolumeClaim in Kubernetes or the underlying volume in Openstack will be deleted, also do not delete the namespace associated with those resources.\nFinally redeploy everything, and instead of launching create_nfs_volume.yaml, we create first the PersistentVolume then the PersistentVolumeClaim:\nkubectl create -f existing_nfs_volume.yaml\nkubectl create -f existing_nfs_volume_claim.yaml"
  },
  {
    "objectID": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-in-jupyter",
    "href": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-in-jupyter",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test in Jupyter",
    "text": "Test in Jupyter\nNow connect to JupyterHub and check in a terminal:\njovyan@jupyter-zonca2:/share$ pwd\n/share\njovyan@jupyter-zonca2:/share$ whoami\njovyan\njovyan@jupyter-zonca2:/share$ touch readonly_folder/ccc\ntouch: cannot touch 'readonly_folder/ccc': Permission denied\njovyan@jupyter-zonca2:/share$\njovyan@jupyter-zonca2:/share$ touch writable_folder/ccc\njovyan@jupyter-zonca2:/share$ ls -l writable_folder/\ntotal 0\n-rw-r--r--. 1 jovyan root 0 Jul 10 06:50 ccc"
  },
  {
    "objectID": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-the-ssh-server",
    "href": "posts/2020-07-10-nfs-server-kubernetes-jetstream.html#test-the-ssh-server",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test the SSH server",
    "text": "Test the SSH server\nFrom a machine external to Jetstream:\nssh -i path/to/private/key -p 30022 datacopier@js-xxx-xxx.jetstream-cloud.org\nWelcome to OpenSSH Server\n\nssh-server:~$ whoami\ndatacopier\nssh-server:~$ sudo su\nssh-server:/config# whoami\nroot\nssh-server:/config# cd /share\nssh-server:/share# ls\nlost+found  readonly_folder  writable_folder\nssh-server:/share# touch readonly_folder/moredata\nssh-server:/share#"
  },
  {
    "objectID": "posts/2020-08-24-renew-letsencrypt-nginx.html",
    "href": "posts/2020-08-24-renew-letsencrypt-nginx.html",
    "title": "Renew letsencrypt certificate with NGINX",
    "section": "",
    "text": "If you have a default domain already configured with NGINX, the easiest way to renew manually a domain is to temporarily disable your custom domain and then use certbot-auto renew the certificate."
  },
  {
    "objectID": "posts/2020-09-01-redirect-readthedocs.html",
    "href": "posts/2020-09-01-redirect-readthedocs.html",
    "title": "Redirect readthedocs documentation to another website",
    "section": "",
    "text": "This is useful if you switch to hosting your own documentation, for example using Sphinx Multiversion on Github pages, tutorial coming soon.\nWe want to be able to redirect from readthedocs keeping the relative url.\nFirst we can setup user-defined redirects from the admin page on readthedocs, see the full documentation, you can choose “Exact redirect”, I only care about redirecting the latest version, so:\n/en/latest/$rest -&gt; https://myorganization.github.io/myrepo/master/\n$rest is a special variable which redirects also all the other pages correctly.\nThe only issue now is that this redirect only works when the documentation is not found, therefore I made a temporary commit to master which deletes all of the Sphinx pages of the documentation and replaces index.rst with:\n.. raw:: html\n\n    &lt;script type=\"text/javascript\"&gt;\n        window.location.replace('https://myorganization.github.io/myrepo/master/');\n    &lt;/script&gt;\nAfter readthedocs builds this version, go to https://github.com/myorganization/myrepo/settings/hooks and disable the readthedocs web hook.\nFinally restore the documentation on your master branch and push."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html",
    "title": "How to share Jupyter Notebooks",
    "section": "",
    "text": "I tend to preserve and post online all the work I do for later reference and sharing. Jupyter Notebooks are a bit more complex than standard source code because they are natively in a format (JSON) that needs to be rendered to be readable.\nMost of the times you want to share a static copy of your “executed” notebook which contains all the generated plots and outputs."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-on-gist-through-the-browser",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-on-gist-through-the-browser",
    "title": "How to share Jupyter Notebooks",
    "section": "Share on gist through the browser",
    "text": "Share on gist through the browser\nThe easiest way to share a Jupyter Notebook is to use a gist, which is quick sharing service provided by Github based on copy-paste.\nNavigate to https://gist.github.com/, it shows a text-area for copy-pasting content, you can drag your ipynb file from your File Manager into the text-area and it will paste its content into the area, then you can click on “Create a public gist” button and it will be uploaded and rendered automatically. If you don’t want the gist to appear under gist.github.com/yourusername and not be findable on Google, select “Create a private gist” instead. You can also upload multiple notebooks to the same gist.\nThe good thing is that behind this interface there is an actual git repository, so you can later clone and update it using git. Or you can even use the web interface to copy-paste a newer version of your Notebook."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-on-gist-from-the-command-line",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-on-gist-from-the-command-line",
    "title": "How to share Jupyter Notebooks",
    "section": "Share on gist from the command line",
    "text": "Share on gist from the command line\nIf you are working on a remote machine (e.g. HPC), it is way better to setup the gh client for Github, then you can directly create a gist from the command line:\ngh gist create --public -d \"My description\" *.ipynb\nand it will print to stdout the gist URL."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#render-larger-notebooks-via-nbviewer",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#render-larger-notebooks-via-nbviewer",
    "title": "How to share Jupyter Notebooks",
    "section": "Render larger Notebooks via nbviewer",
    "text": "Render larger Notebooks via nbviewer\nThe Github rendering engine is not the most reliable: it fails for larger notebooks, it doesn’t work on mobile and it doesn’t support some JavaScript based libraries, like altair.\nThen after having uploaded your Notebook to gist, you can go to https://nbviewer.jupyter.org/, paste the full link to your gist and share the nbviewer rendering instead."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#save-clean-notebooks-in-github-repositories",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#save-clean-notebooks-in-github-repositories",
    "title": "How to share Jupyter Notebooks",
    "section": "Save clean Notebooks in Github repositories",
    "text": "Save clean Notebooks in Github repositories\nI also recommend to save a copy of notebooks without any outputs (either doing “Clear all outputs” from the interface or using nbstripout) into a related Github repository, this makes it easier to reference it later on. However don’t save executed notebooks inside Github repositories, they are too large and cause ugly diff between versions (also use nbdime to help manage notebooks inside repos)."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-related-group-of-notebooks",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#share-related-group-of-notebooks",
    "title": "How to share Jupyter Notebooks",
    "section": "Share related group of notebooks",
    "text": "Share related group of notebooks\nFor teaching it is often useful to share a number of related notebooks, in this case you can create a repository full of notebooks and then point nbviewer to the address of that repository, it will automatically create an index where people can navigate through notebooks. See for example https://nbviewer.jupyter.org/github/zonca/mapsims_tutorials/tree/master/.\nFor teaching it is often useful to have the main repository which contains clean versions of the notebooks and then have a Github fork with a version of the notebooks all executed. This is useful for later reference and for people that have trouble executing the notebooks. Better use a fork because if we used a branch, we would make the main repository un-necessarily heavier to download."
  },
  {
    "objectID": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#make-notebooks-executable-in-the-browser",
    "href": "posts/2020-09-15-how-to-share-jupyter-notebooks.html#make-notebooks-executable-in-the-browser",
    "title": "How to share Jupyter Notebooks",
    "section": "Make notebooks executable in the browser",
    "text": "Make notebooks executable in the browser\nOnce you have notebooks in a repository, you can also plug them into https://mybinder.org/ to allow people to spin a Virtual Machine on Google Cloud automatically and for free and execute them in their browser. You can also provide a environment.yml file to setup the requirements.\nSee for example the healpy tutorial gist, which can be executed on Binder clicking on the following Icon:"
  },
  {
    "objectID": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html",
    "href": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html",
    "title": "Install HEALPix and PolSpice in a conda environment",
    "section": "",
    "text": "Some notes on how to install HEALPix and PolSpice inside a conda environment, with some details about doing it at NERSC, but most of the tutorial is independent of that."
  },
  {
    "objectID": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#setup-the-conda-environment-and-the-compilers",
    "href": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#setup-the-conda-environment-and-the-compilers",
    "title": "Install HEALPix and PolSpice in a conda environment",
    "section": "Setup the conda environment and the compilers",
    "text": "Setup the conda environment and the compilers\nI assume here we are installing it into a custom conda environement the possibly contains all other cosmology packages, like healpy.\nFor example created with:\nmodule load python\nconda create -n pycmb python==3.7 healpy matplotlib ipykernel\nWhen you activate the conda environment, the variable $CONDA_PREFIX is automatically set to the base folder of the environment, something like:\n~/anaconda/envs/pycmb\nTo make it simpler, I am using gcc and gfortran, if at NERSC run:\nmodule load PrgEnv-gnu\nif that fails, probably you need first to unload the Intel environment:\nmodule unload PrgEnv-intel\nmodule load PrgEnv-gnu"
  },
  {
    "objectID": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-cfitsio",
    "href": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-cfitsio",
    "title": "Install HEALPix and PolSpice in a conda environment",
    "section": "Install cfitsio",
    "text": "Install cfitsio\ncfitsio is quite easy, better download the version included in healpy because it has a couple of fixes:\ngit clone https://github.com/healpy/cfitsio\ncd cfitsio\nWe want to install it into a dedicated folder, not the same lib folder of the conda environment, so that we don’t risk to have conflicts with the compiler libraries during the build process:\n./configure --prefix=$CONDA_PREFIX/cfitsio\nmake -j8 shared install"
  },
  {
    "objectID": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-healpix",
    "href": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-healpix",
    "title": "Install HEALPix and PolSpice in a conda environment",
    "section": "Install HEALPix",
    "text": "Install HEALPix\nHEALPix installs itself in the same folder where it is unpacked, and then modifies the bash profile to make things work. As we want to keep things isolated, let’s unpack the Healpix package into the conda environment folder, so it will be something like:\n$CONDA_PREFIX/Healpix_3.70\n\n./configure\nconfigure the C, the Fortran packages, the Fortran package requires libsharp, set everything to default except location of cfitsio where you need (notice lib at the end):\n$CONDA_PREFIX/cfitsio/lib\nWhen the installer asks whether to modify .profile respond no. Now the installer will create some scripts in the ~/.healpix folder and modify .profile, we want to only activate HEALPix in our conda environment so we should modify .profile and remove the lines added by HEALPix.\nFinally we can have HEALPix automatically activated when the conda environment is initialized (notice we need the script to end in .sh):\nmkdir -p ${CONDA_PREFIX}/etc/conda/activate.d\nln -s ~/.healpix/3_70_Linux/config ${CONDA_PREFIX}/etc/conda/activate.d/config.sh\nRestart the conda environment, and try to run anafast to check that it works. If you are at NERSC, make sure that you are always loading the GNU programming environment by having:\nmodule swap PrgEnv-intel PrgEnv-gnu\nin .bashrc.ext."
  },
  {
    "objectID": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-polspice",
    "href": "posts/2020-10-14-install-healpix-polspice-conda-nersc.html#install-polspice",
    "title": "Install HEALPix and PolSpice in a conda environment",
    "section": "Install PolSpice",
    "text": "Install PolSpice\nCreate a build folder inside the source folder and create a run.sh file with this content:\ncmake .. -DCFITSIO=${CONDA_PREFIX}/cfitsio/lib -DCMAKE_Fortran_COMPILER=gfortran -DCMAKE_C_COMPILER=gcc\nThen:\nbash run.sh\nmake -j8\nThis will put the spice executable into the ../bin folder, just copy it to the conda environment bin folder:\ncd ..\ncp bin/spice ${CONDA_PREFIX}/bin/\nWe can also copy the 2 python modules into the environment:\ncp bin/bin_llcl.py bin/ispice.py ${CONDA_PREFIX}/lib/python3.*/site-packages/\nCheck it works:\nspice -usage\n\nUninstall PolSpice\nrm ${CONDA_PREFIX}/bin/spice ${CONDA_PREFIX}/lib/python3.*/site-packages/bin_llcl.py ${CONDA_PREFIX}/lib/python3.*/site-packages/ispice.py"
  },
  {
    "objectID": "posts/2020-11-30-cfitsio-checkpoint-write.html",
    "href": "posts/2020-11-30-cfitsio-checkpoint-write.html",
    "title": "CFITSIO file writing with checkpointing",
    "section": "",
    "text": "I wrote a prototype implementation of writing FITS files in C checkpointing using the FITS flush function so that if the code segfaults, the file is not corrupted, see:\n\nhttps://github.com/zonca/cfitsio_checkpoint_write"
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "",
    "text": "This is an update to previous tutorials, focused on deploying Kubernetes 1.19.7 (released in Jan 2021, based on 1.19.0 released in August 2020), compared to 1.17.6 of the previous version of the tutorial. Last executed in September 2021.\nFor an overview of my work on deploying Kubernetes and JupyterHub on Jetstream, see my Gateways 2020 paper.\nWe will use Kubespray 2.15.0, which first runs terraform to create the Openstack resources, then ansible to configure the servers to run all the Kubernetes services."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#create-jetstream-virtual-machines-with-terraform",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#create-jetstream-virtual-machines-with-terraform",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Create Jetstream Virtual machines with Terraform",
    "text": "Create Jetstream Virtual machines with Terraform\nTerraform allows to execute recipes that describe a set of OpenStack resources and their relationship. In the context of this tutorial, we do not need to learn much about Terraform, we will configure and execute the recipe provided by kubespray.\n\nRequirements\nOn a Ubuntu 18.04 install python3-openstackclient with APT, I tested with 3.18. Any other platform works as well, also install terraform by copying the correct binary to /usr/local/bin/, see https://www.terraform.io/intro/getting-started/install.html. The requirement is a terraform version &gt; 0.12, I tested with 0.14.4.\n\n\nRequest API access\nIn order to make sure your XSEDE account can access the Jetstream API, you need to contact the Helpdesk, see the instructions on the Jetstream Wiki. You will also receive your TACC password, which could be different than your XSEDE one (username is generally the same).\nLogin to the TACC Horizon panel at https://tacc.jetstream-cloud.org/dashboard, this is basically the low level web interface to OpenStack, a lot more complex and powerful than Atmosphere available at https://use.jetstream-cloud.org/application. Use tacc as domain, your TACC username (generally the same as your XSEDE username) and your TACC password.\nFirst choose the right project you would like to charge to in the top dropdown menu (see the XSEDE website if you don’t recognize the grant code).\nClick on Compute / API Access and download the OpenRC V3 authentication file to your machine. Source it typing:\nsource XX-XXXXXXXX-openrc.sh\nit should ask for your TACC password. This configures all the environment variables needed by the openstack command line tool to interface with the Openstack API.\nTest with:\nopenstack flavor list\nThis should return the list of available “sizes” of the Virtual Machines.\n\n\nClone kubespray\nI needed to make a few modifications to kubespray to adapt it to Jetstream:\ngit clone https://github.com/zonca/jetstream_kubespray\ngit checkout -b branch_v2.15.0 origin/branch_v2.15.0\nSee an overview of my changes compared to the standard kubespray release 2.15.0.\n\n\nReserve a floating IP\nWe prefer not to have a floating IP handled by Terraform, otherwise it would be released every time we need to redeploy the cluster, better create it beforehand:\nopenstack floating ip create public\nThis will return a public floating IP address, it can also be accessed with:\nopenstack floating ip list\n\n\nRun Terraform\nInside jetstream_kubespray, choose a name for the cluster and copy from my template:\nexport CLUSTER=yourclustername\ncp -r inventory/kubejetstream inventory/$CLUSTER\ncd inventory/$CLUSTER\nOpen and modify cluster.tfvars, choose your image and number of nodes.\nYou can find suitable images (they need to be JS-API-Featured, you cannot use the same instances used in Atmosphere):\nopenstack image list | grep \"JS-API\"\nThe default is JS-API-Featured-Ubuntu20-Latest.\nPaste the floating ip created previously into k8s_master_fips.\nI already preconfigured the network UUID both for IU and TACC, but you can crosscheck looking for the public network in:\nopenstack network list\nInitialize Terraform:\nbash terraform_init.sh\nCreate the resources:\nbash terraform_apply.sh\nThe last output log of Terraform should contain the IP of the master node k8s_master_fips, wait for it to boot then SSH in with:\nexport IP=XXX.XXX.XXX.XXX\nssh ubuntu@$IP\nor centos@$IP for CentOS images.\nInspect with Openstack the resources created:\nopenstack server list\nopenstack network list\nYou can cleanup the virtual machines and all other Openstack resources (all data is lost) with bash terraform_destroy.sh. The floating IP won’t be released so we can create a cluster again from scratch with the same IP address."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-and-test-ansible",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-and-test-ansible",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Install and test Ansible",
    "text": "Install and test Ansible\nChange folder back to the root of the jetstream_kubespray repository,\nFirst make sure you have a recent version of ansible installed, you also need additional modules, so first run:\npip install -r requirements.txt\nThis pip script installs a predefined version of ansible, currently 2.9.16, so it is useful to create a virtualenv or a conda environment and install packages inside that.\nThen following the kubespray documentation, we setup ssh-agent so that ansible can SSH from the machine with public IP to the others:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nTest the connection through ansible:\nansible -i inventory/$CLUSTER/hosts -m ping all\nIf a server is not answering to ping, first try to reboot it:\nopenstack server reboot $CLUSTER-k8s-node-nf-1\nOr delete it and run terraform_apply.sh to create it again."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-kubernetes-with-kubespray",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-kubernetes-with-kubespray",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Install Kubernetes with kubespray",
    "text": "Install Kubernetes with kubespray\ncheck inventory/$CLUSTER/group_vars/all/all.yml, in particular bootstrap_os, I setup ubuntu, change it to centos if you used the Centos 7 base image.\nIn inventory/$CLUSTER/group_vars/k8s-cluster/k8s-cluster.yml, set the public floating IP of the master instance in supplementary_addresses_in_ssl_keys.\nFinally run the full playbook, it is going to take a good 10 minutes, go make coffee:\nbash k8s_install.sh\nIf the playbook fails with “cannot lock the administrative directory”, it is due to the fact that the Virtual Machine is automatically updating so it has locked the APT directory. Just wait a minute and launch it again. It is always safe to run ansible multiple times.\nIf the playbook gives any error, try to retry the above command, sometimes there are temporary failed tasks, Ansible is designed to be executed multiple times with consistent results.\nYou should have now a Kubernetes cluster running, test it:\n$ ssh ubuntu@$IP\n$ sudo su\n$ kubectl get pods --all-namespaces\ningress-nginx   ingress-nginx-controller-7tqsr                       1/1     Running   0          3h21m\nkube-system     coredns-85967d65-qczgr                               1/1     Running   0          117m\nkube-system     coredns-85967d65-wp9vm                               1/1     Running   0          117m\nkube-system     dns-autoscaler-5b7b5c9b6f-vh4vv                      1/1     Running   0          3h21m\nkube-system     kube-apiserver-kubejetstream-k8s-master-1            1/1     Running   1          3h24m\nkube-system     kube-controller-manager-kubejetstream-k8s-master-1   1/1     Running   0          3h24m\nkube-system     kube-flannel-5qzxn                                   1/1     Running   0          3h22m\nkube-system     kube-flannel-mrlkz                                   1/1     Running   0          3h22m\nkube-system     kube-proxy-9qpmz                                     1/1     Running   0          118m\nkube-system     kube-proxy-rzqv6                                     1/1     Running   0          118m\nkube-system     kube-scheduler-kubejetstream-k8s-master-1            1/1     Running   0          3h24m\nkube-system     nginx-proxy-kubejetstream-k8s-node-1                 1/1     Running   0          3h22m\nkube-system     nodelocaldns-d7r2c                                   1/1     Running   0          3h21m\nkube-system     nodelocaldns-jx2st                                   1/1     Running   0          3h21m\nCompare that you have all those services running also in your cluster. We have also configured NGINX to proxy any service that we will later deploy on Kubernetes, test it with:\n$ wget localhost\n--2018-09-24 03:01:14--  http://localhost/\nResolving localhost (localhost)... 127.0.0.1\nConnecting to localhost (localhost)|127.0.0.1|:80... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2018-09-24 03:01:14 ERROR 404: Not Found.\nError 404 is a good sign, the service is up and serving requests, currently there is nothing to deliver. Finally test that the routing through the Jetstream instance is working correctly by opening your browser and test that if you access js-XX-XXX.jetstream-cloud.org you also get a default backend - 404 message. If any of the tests hangs or cannot connect, there is probably a networking issue."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#optional-setup-kubectl-locally",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#optional-setup-kubectl-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "(Optional) Setup kubectl locally",
    "text": "(Optional) Setup kubectl locally\nInstall kubectl locally, I am currently using 1.20.\nWe also set kubeconfig_localhost: true, which copies the kubectl configuration admin.conf to:\ninventory/$CLUSTER/artifacts\nI have a script to copy that to .config/kube and to replace the IP with the floating IP of the master node, for this script to work make sure you have exported the variable IP:\nbash k8s_configure_kubectl_locally.sh"
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#optional-setup-helm-locally",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#optional-setup-helm-locally",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "(Optional) Setup helm locally",
    "text": "(Optional) Setup helm locally\nInstall helm 3 from the release page on Github\nI tested with v3.5.0."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-jupyterhub",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#install-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nNow checkout the JupyterHub configuration files repository on the local machine (if you have setup kubectl and helm locally, otherwise on the master node).\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\nInside that, first run\nbash create_secrets.sh\nto create the secret strings needed by JupyterHub then edit its output secrets.yaml to make sure it is consistent, edit the hosts lines if needed. For example, supply the Jetstream DNS name of the master node js-XXX-YYY.jetstream-cloud.org (XXX and YYY are the last 2 groups of the floating IP of the instance AAA.BBB.XXX.YYY).\nbash configure_helm_jupyterhub.sh\nkubectl create namespace jhub\nIt is preferable to run the Hub and the Proxy on the master node, just in case we want to downsize the cluster to only one node to save resources. This is already configured in config_standard_storage.yaml with:\nnodeSelector:\n    node-role.kubernetes.io/master: \"\"\nDelete those lines if instead you’d rather have Hub and Proxy run also on other nodes.\nFinally run helm to install JupyterHub:\nbash install_jhub.sh\nThis is installing zero-to-jupyterhub 0.11.1, you can check on the zero-to-jupyterhub release page if a newer version is available, generally transitioning to new releases is painless, they document any breaking changes very well.\nCheck pods running with:\nkubectl get pods -n jhub\nOnce the proxy is running, even if hub is still in preparation, you can check in browser, you should get “Service Unavailable” which is a good sign that the proxy is working.\nYou can finally connect with your browser to js-XXX-YYY.jetstream-cloud.org and check if the Hub is working fine, after that, the pods running using:\nkubectl get pods -n jhub\nshoud be:\ncontinuous-image-puller-77bb9     1/1     Running   0          7m23s\nhub-75d787584d-bhhgc              1/1     Running   0          7m23s\njupyter-zonca                     1/1     Running   0          4m34s\nproxy-78b8c47d7b-92fjf            1/1     Running   0          7m23s\nuser-scheduler-6c4d6f7f57-mqwmh   1/1     Running   0          7m23s\nuser-scheduler-6c4d6f7f57-sp5sh   1/1     Running   0          7m23s"
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#customize-jupyterhub",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#customize-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Customize JupyterHub",
    "text": "Customize JupyterHub\nAfter JupyterHub is deployed and integrated with Cinder for persistent volumes, for any other customizations, first authentication, you are in good hands as the Zero-to-Jupyterhub documentation is great."
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#setup-https-with-letsencrypt",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#setup-https-with-letsencrypt",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Setup HTTPS with letsencrypt",
    "text": "Setup HTTPS with letsencrypt\nKubespray has the option of deploying also cert-manager, but I had trouble deploying an issuer, it was easier to just deploy it afterwards following my previous tutorial"
  },
  {
    "objectID": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#feedback",
    "href": "posts/2021-01-20-jetstream_kubernetes_kubespray_2.15.0.html#feedback",
    "title": "Deploy Kubernetes on Jetstream with Kubespray 2.15.0",
    "section": "Feedback",
    "text": "Feedback\nFeedback on this is very welcome, please open an issue on the Github repository or email me at zonca on the domain of the San Diego Supercomputer Center (sdsc.edu)."
  },
  {
    "objectID": "posts/2021-03-25-jetstream-video-stream-object-store.html",
    "href": "posts/2021-03-25-jetstream-video-stream-object-store.html",
    "title": "Stream video from object store on Jetstream",
    "section": "",
    "text": "In this tutorial we will serve video stored in Jetstream’s object store (like Amazon S3) as HTTP live streaming to a user’ browser. This makes it difficult for the user to save the whole video as it is served in chunks."
  },
  {
    "objectID": "posts/2021-03-25-jetstream-video-stream-object-store.html#load-a-test-video-on-object-store",
    "href": "posts/2021-03-25-jetstream-video-stream-object-store.html#load-a-test-video-on-object-store",
    "title": "Stream video from object store on Jetstream",
    "section": "Load a test video on object store",
    "text": "Load a test video on object store\nLogin to Horizon\nGo to Project &gt; Object store\nCreate a container, for example videostream, make it public\nUpload a test video, for example:\nhttps://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_30MB.mp4\nWait 5 minutes then test you can access the video from your machine and note the base url."
  },
  {
    "objectID": "posts/2021-03-25-jetstream-video-stream-object-store.html#deploy-the-vm",
    "href": "posts/2021-03-25-jetstream-video-stream-object-store.html#deploy-the-vm",
    "title": "Stream video from object store on Jetstream",
    "section": "Deploy the VM",
    "text": "Deploy the VM\nLogin to Atmosphere and launch the newest Ubuntu 20.04 image\n\nConfigure SSL\nThe streaming server will ask for the SSL certificates on setup.\nSo, install certbot with:\nsudo apt install certbot\nand get certificates:\ncertbot certonly --standalone -d js-xxx-yyy.jetstream-cloud.org"
  },
  {
    "objectID": "posts/2021-03-25-jetstream-video-stream-object-store.html#install-the-streaming-server",
    "href": "posts/2021-03-25-jetstream-video-stream-object-store.html#install-the-streaming-server",
    "title": "Stream video from object store on Jetstream",
    "section": "Install the streaming server",
    "text": "Install the streaming server\nKaltura maintains packages for Ubuntu, so it is easy to install it following https://github.com/kaltura/nginx-vod-module#debianubuntu-deb-package\nFor the configuration interactive prompts (I might have set them out of order here):\n\nUse port 80 instead of 88 (unless you plan to have another NGINX instance on port 80)\nUse port 443 instead of 8443\nFor the mode choose “Remote”, we will load from object store\nFor the remote url set the base url of the object store (before /swift) and the port\nSay “No” when it asks if you use the Kaltura media service\nFor SSL certificate set /etc/letsencrypt/live/js-xxx-yyy.jetstream-cloud.org/fullchain.pem\nFor SSL key set /etc/letsencrypt/live/js-xxx-yyy.jetstream-cloud.org/privkey.pem\n\nBy default it sets the streaming from HTTP instead of HTTPS, so edit /opt/kaltura/nginx/conf/vod-remote.conf, modify:\nproxy_pass http://media/$1;\ninto:\nproxy_pass https://media/$1;\nIf anything goes wrong in the configuration, check the error log at:\n/opt/kaltura/log/nginx/\nand reset it:\nsudo apt purge kaltura-nginx\nrm -fr /opt/kaltura\nsudo apt install kaltura-nginx"
  },
  {
    "objectID": "posts/2021-03-25-jetstream-video-stream-object-store.html#test-the-streaming",
    "href": "posts/2021-03-25-jetstream-video-stream-object-store.html#test-the-streaming",
    "title": "Stream video from object store on Jetstream",
    "section": "Test the streaming",
    "text": "Test the streaming\nFirst try to access the stream directly from your browser, browsers don’t know how to display a HLS stream, but if you get to download a file named “index.m3u8” that is a good sign:\nhttps://js-xxx-yyy.jetstream-cloud.org/hls/swift/v1/videostream/Big_Buck_Bunny_720_10s_30MB/index.m3u8\nIn the URL, videostream is the name of the bucket (container), then the filename, index.m3u8 tells the server that we want to stream that into HLS format.\nIf you get an error message instead, check the Kaltura NGINX logs.\nFinally we can test it into a player, for example VideoJS, go to https://videojs-http-streaming.netlify.app/ and paste the URL of your stream."
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "",
    "text": "In my specific scenario, I have users running JupyterHub on top of Kubernetes on the Jetstream XSEDE Cloud resouce. Each user has a persistent volume as their home folder of a few GB. Instead of snapshotting the entire volume, I would like to only backup the data offsite to OpenStorageNetwork and being able to restore them.\nIn this tutorial I’ll show how to configure Stash for this task. Stash is has a lot of other functionality, so it is really easy to get lost in their documentation. This tutorial is for an advanced topic, it assumes good knowledge of Kubernetes.\nStash under the hood uses restic to backup the data, so that we can also manage the backups outside of Kubernetes, see further down the tutorial. It also automatically decuplicates the data, so if the same file is unchanged in multiple backups, as it is often the case, it is just stored once and referenced by multiple backups.\nAll the configuration files are available in the backup_volumes folder of zonca/jupyterhub-deploy-kubernetes-jetstream"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#install-stash",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#install-stash",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Install Stash",
    "text": "Install Stash\nFirst we need to request a free license for the community edition of the software, I tested with 2021.03.17, replace as needed with a newer version:\n\nhttps://stash.run/docs/v2021.03.17/setup/install/community/\n\nRename it to license.txt, then install Stash via Helm:\nhelm repo add appscode https://charts.appscode.com/stable/\nhelm repo update\nbash install_stash.sh"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#test-object-store",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#test-object-store",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Test object store",
    "text": "Test object store\nI have used object store from OpenStorageNetwork, which is nice as it is offsite, but also using the Jetstream object store is an option. Both support the AWS S3 protocol.\nIt would be useful at this point to test the S3 credentials:\nInstall the AWS cli pip install awscli awscli-plugin-endpoint\nThen create a configuration profile at ~/.aws/config:\n[plugins]\nendpoint = awscli_plugin_endpoint\n\n[profile osn]\naws_access_key_id=\naws_secret_access_key=\ns3 =\n    endpoint_url = https://xxxx.osn.xsede.org\ns3api =\n    endpoint_url = https://xxxx.osn.xsede.org\nThen you can list the content of your bucket with:\naws s3 --profile osn ls s3://your-bucket-name --no-sign-request"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#configure-the-s3-backend-for-stash",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#configure-the-s3-backend-for-stash",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Configure the S3 backend for Stash",
    "text": "Configure the S3 backend for Stash\nSee the Stash documentation about the S3 backend. In summary, we should create 3 text files:\n\nRESTIC_PASSWORD with a random password to encrypt the backups\nAWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY with the S3 style credentials\n\nThen we can create a Secret in Kubernetes that holds the credentials:\nbash create_aws_secret.sh\nThen, customize stash_repository.yaml and create the Stash repository with:\nkubectl create -f stash_repository.yaml\nCheck it was created:\n&gt; kubectl -n jhub get repository\nNAME       INTEGRITY   SIZE   SNAPSHOT-COUNT   LAST-SUCCESSFUL-BACKUP   AGE\nosn-repo                                                                2d15h"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#configuring-backup-for-a-standalone-volume",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#configuring-backup-for-a-standalone-volume",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Configuring backup for a standalone volume",
    "text": "Configuring backup for a standalone volume\nAutomatic and batch backup require a commercial Stash license. With the community version, we can only use the “standalone volume” functionality, which is enough for our purposes.\nSee the relevant documentation\nNext we need to create a BackupConfiguration\nEdit stash_backupconfiguration.yaml, in particular you need to specify which PersistentVolumeClaim you want to backup, for JupyterHub user volumes, these will be claim-username. For testing better leave “each minute” for the schedule, if a backup job is running, the following are skipped. You can also customize excluded folders.\nIn order to pause backups, set paused to true:\nkubectl -n jhub edit backupconfiguration test-backup\nBackupConfiguration should create a CronJob resource:\n&gt; kubectl -n jhub get cronjob\nNAME                       SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nstash-backup-test-backup   * * * * *   True      0        2d15h           2d15h\nCronJob then launches a BackupSession for each trigger of the backup:\n&gt; kubectl -n jhub get backupsession\nNAME                     INVOKER-TYPE          INVOKER-NAME   PHASE     AGE\ntest-backup-1618875244   BackupConfiguration   test-backup    Succeeded   3m13s\ntest-backup-1618875304   BackupConfiguration   test-backup    Succeeded   2m13s\ntest-backup-1618875364   BackupConfiguration   test-backup    Succeeded   73s\ntest-backup-1618875425   BackupConfiguration   test-backup    Running     12s"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#monitor-and-debug-backups",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#monitor-and-debug-backups",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Monitor and debug backups",
    "text": "Monitor and debug backups\nYou can check the logs of a backup with:\n&gt; kubectl -n jhub describe backupsession test-backup-1618869996\n&gt; kubectl -n jhub describe pod stash-backup-test-backup-1618869996-0-rdcdq\n&gt; kubectl -n jhub logs stash-backup-test-backup-1618861992-0-kj2r6\nOnce backups succeed, they should appear on object store:\n&gt; aws s3 --profile osn ls s3://your-bucket-name/jetstream-backup/snapshots/\n2021-04-19 16:34:11        340 1753f4c15da9713daeb35a5425e7fbe663e550421ac3be82f79dc508c8cf5849\n2021-04-19 16:35:12        340 22bccac489a69b4cda1828f9777677bc7a83abb546eee486e06c8a8785ca8b2f\n2021-04-19 16:36:11        340 7ef1ba9c8afd0dcf7b89fa127ef14bff68090b5ac92cfe3f68c574df5fc360e3\n2021-04-19 16:37:12        339 da8f0a37c03ddbb6c9a0fcb5b4837e8862fd8e031bcfcfab563c9e59ea58854d\n2021-04-19 16:33:10        339 e2369d441df69bc2809b9c973e43284cde123f8885fe386a7403113f4946c6fa"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#restore-from-backup",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#restore-from-backup",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Restore from backup",
    "text": "Restore from backup\nBackups are encrypted, so it is not possible to access the data directly from object store. We need to restore it to a volume.\nFor testing purposes, login to the volume via JupyterHub and delete some files. Then stop the single user server from the JupyterHub dashboard.\nConfigure and launch the restoring operation:\nkubectl -n jhub create -f stash_restore.yaml\nThis overwrites the content of the target volume with the content of the backup. See the Stash documentation on how to restore to a different volume.\n&gt; kubectl -n jhub get restoresession\nNAME      REPOSITORY   PHASE       AGE\nrestore   osn-repo     Succeeded   2m18s\nThen login back to JupyterHub and check that the files previously deleted.\nIn the default configuration stash_restore.yaml restores the last backup, independently of username, so if you are backing up volumes of different users, you should tag by usernames, see below, and then restore a specific id (just replace latest in the YAML file with the first 10 or so characters of the ID). See an example of the full restore workflow with screenshots at the end of this Github issue."
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#setup-for-production-in-a-small-deployment",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#setup-for-production-in-a-small-deployment",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Setup for production in a small deployment",
    "text": "Setup for production in a small deployment\nIn a small deployment with tens of users, we can individually identify which users we want to backup, and choose a schedule. The backup service works even the user is currently logged in, anyway, it is good practice to schedule a daily backup at 3am or 4am in the appropriate timezone. We should create 1 BackupConfiguration object for each user, 10 minutes apart, each targeting a different PersistentVolumeClaim."
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#template-backup-configuration-creation",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#template-backup-configuration-creation",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Template backup configuration creation",
    "text": "Template backup configuration creation\nIf you like danger, you can also automate the creation of the BackupConfiguration objects. You can create a text file named users_to_backup.txt with 1 username per line of the JupyterHub users you want to backup.\nThen customize the stash_backupconfiguration_template.yaml configuration file, make sure you decide a retention policy, for more information see the Stash or Restic documentation. Unfortunately Stash considers all backups together under 1 retention policy, so if I set to keep 1 weekly backup, it will retain 1 weekly backup of just one of the users instead of all of them. I worked around this issue tagging myself the backups after the fact using the restic command line tool, see the next section.\nThen you can launch it:\nbash setup_backups.sh\n******** Setup xxxxxxx at 8:0\nbackupconfiguration.stash.appscode.com/backup-xxxxxxx created\n******** Setup xxxxxxx at 8:10\nbackupconfiguration.stash.appscode.com/backup-xxxxxxx created\n******** Setup xxxxxxx at 8:20\nbackupconfiguration.stash.appscode.com/backup-xxxxxxx created\n******** Setup xxxxxxx at 8:30\nbackupconfiguration.stash.appscode.com/backup-xxxxxxx created\n******** Setup xxxxxxx at 8:40\nbackupconfiguration.stash.appscode.com/backup-xxxxxxx created\nThere is no chance this will work the first time, so:\nkubectl delete backupconfiguration --all"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#categorize-the-backups-by-username",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#categorize-the-backups-by-username",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Categorize the backups by username",
    "text": "Categorize the backups by username\nUnfortunately I couldn’t find a way to tag the backups with the username which own the volume. So I added this line:\necho $JUPYTERHUB_USER &gt; ~/.username;\nto the zero-to-jupyterhub configuration YAML under:\nsingleuser:\n  lifecycleHooks:\n    postStart:\n      exec:\n        command:\nSo when the user logs in, we write their username into the volume. Then we can use restic outside of Kubernetes to tag the backups once in a while with the correct usernames, see the restic_tag_usernames.sh script.\nOnce we have tags, we can handle pruning old backups manually using the restic forget command."
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#manage-backups-outside-of-kubernetes",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#manage-backups-outside-of-kubernetes",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Manage backups outside of Kubernetes",
    "text": "Manage backups outside of Kubernetes\nStash manages backups with restic. It is also possible to access and manage the backups using restic on a machine outside of Kubernetes.\nInstall restic from the official website\nExport the AWS variables:\nexport AWS_ACCESS_KEY_ID=\nexport AWS_SECRET_ACCESS_KEY=\nHave the RESTIC password ready for the prompt:\nrestic -r s3:https://ncsa.oss-data/jetstream-backup/ snapshots\nenter password for repository: \nrepository 18a1c421 opened successfully, password is correct\ncreated new cache in /home/zonca/.cache/restic\nID        Time                 Host        Tags        Paths\n------------------------------------------------------------------\n026bcce3  2021-05-10 13:17:17  host-0                  /stash-data\n4f71a384  2021-05-10 13:18:16  host-0                  /stash-data\n34ff4677  2021-05-10 13:19:18  host-0                  /stash-data\n9f7337fe  2021-05-10 13:20:08  host-0                  /stash-data\nc130e039  2021-05-10 13:21:08  host-0                  /stash-data\n------------------------------------------------------------------\n5 snapshots\nYou can even browse the backups without downloading the data:\nsudo mkdir /mnt/temp\nsudo chown $USER /mnt/temp\nrestic -r s3:https://ncsa.osn.xsede.org/xxxxxx/jetstream-backup/ mount /mnt/temp\n/mnt/temp/snapshots/latest/stash-data $ ls\na  b  Healpix_3.70_2020Jul23.tar.gz  MosfireDRP-2018release.zip  plot_cl_TT.ipynb  Untitled1.ipynb  Untitled2.ipynb  Untitled.ipynb"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#troubleshooting",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#troubleshooting",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIssue: Volume available but also attached in Openstack, works fine on JupyterHub but backing up fails, this can happen while testing.\nSolution: Delete the PVC, the PV and the volume via Openstack, login through JupyterHub to get another volume assigned.\nIssue: Volumes cannot be mounted because they are in “Reserved” state in Openstack\nSolution: Run openstack volume set --state available &lt;uuid&gt;, this is an open issue affecting Jetstream"
  },
  {
    "objectID": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#setup-monitoring",
    "href": "posts/2021-04-19-jetstream-backup-kubernetes-volumes-object-store.html#setup-monitoring",
    "title": "Backup Kubernetes volumes to OpenStorageNetwork object store",
    "section": "Setup monitoring",
    "text": "Setup monitoring\nSee the new tutorial on how to setup a system to monitor that the backups are being executed"
  },
  {
    "objectID": "posts/2021-06-17-learn-italian-in-san-diego.html",
    "href": "posts/2021-06-17-learn-italian-in-san-diego.html",
    "title": "Learn Italian in San Diego",
    "section": "",
    "text": "My wife Maura D’Andrea recently opened a new Italian language school in San Diego.\nAll instructors are mother-tongue Italian, they teach to kids and adults.\nSee the available Italian in-person classes in San Diego, they are split in 2 tracks, one for people looking to learn Italian, one for kids that have Italian heritage and want to follow the same program taught in Italy’s public school system.\nClasses are online and in-person in University City, San Diego."
  },
  {
    "objectID": "posts/2021-06-27-dreamhost-email.html",
    "href": "posts/2021-06-27-dreamhost-email.html",
    "title": "Configure dreamhost email",
    "section": "",
    "text": "Configuration to read Dreamhost email from Gmail:"
  },
  {
    "objectID": "posts/2021-06-27-dreamhost-email.html#receive",
    "href": "posts/2021-06-27-dreamhost-email.html#receive",
    "title": "Configure dreamhost email",
    "section": "Receive",
    "text": "Receive\n\nServer pop.dreamhost.com\nAlways use SSL\nPort 995"
  },
  {
    "objectID": "posts/2021-06-27-dreamhost-email.html#send",
    "href": "posts/2021-06-27-dreamhost-email.html#send",
    "title": "Configure dreamhost email",
    "section": "Send",
    "text": "Send\n\nServer smtp.dreamhost.com\nUse SSL\nPort 465"
  },
  {
    "objectID": "posts/2021-08-04-pysm3-paper.html",
    "href": "posts/2021-08-04-pysm3-paper.html",
    "title": "New paper about PySM 3",
    "section": "",
    "text": "Update November 2021\nPaper was published on JOSS:\n\n“The Python Sky Model 3 software” on JOSS\n“The Python Sky Model 3 software” on the Arxiv\nSubmission to JOSS\n\nsubmitted to JOSS, see also this tutorial on how to publish a JOSS paper to Arxiv\nIt is about pysm3 a #Python package (heavily optimized through numba) to simulate sky maps of the Cosmic Microwave Background (CMB) and galactic emission as observed by instruments. Relies on Planck & WMAP data."
  },
  {
    "objectID": "posts/2021-10-01-monitor-github-traffic.html",
    "href": "posts/2021-10-01-monitor-github-traffic.html",
    "title": "Monitor traffic on Github repositories",
    "section": "",
    "text": "Github only shows traffic data for the last 2 weeks. Through the API is possible to gather those data every 2 weeks and save them for later collection and reduction."
  },
  {
    "objectID": "posts/2021-10-01-monitor-github-traffic.html#available-data",
    "href": "posts/2021-10-01-monitor-github-traffic.html#available-data",
    "title": "Monitor traffic on Github repositories",
    "section": "Available data",
    "text": "Available data\nIt’s 3 different stats:\n\nclone: number of clones and unique clones per day\nreferrer: websites that linked to the repository in the last 2 weeks\ntraffic: views and unique visitors"
  },
  {
    "objectID": "posts/2021-10-01-monitor-github-traffic.html#scripts",
    "href": "posts/2021-10-01-monitor-github-traffic.html#scripts",
    "title": "Monitor traffic on Github repositories",
    "section": "Scripts",
    "text": "Scripts\nI have created a set of scripts based on github-traffic-stats\nFollow the instructions in the README.md:\n\nhttps://github.com/zonca/save-github-traffic-stats"
  },
  {
    "objectID": "posts/2021-10-12-kubernetes-hadoop.html",
    "href": "posts/2021-10-12-kubernetes-hadoop.html",
    "title": "Deploy Hadoop on Kubernetes on Jetstream",
    "section": "",
    "text": "We are deploying the good old Hadoop on top of Kubernetes on Jetstream. Don’t ask why.\nAs usual we start with a full-fledged Kubernetes deployment on Jetstream (1) deployed via Kubespray"
  },
  {
    "objectID": "posts/2021-10-12-kubernetes-hadoop.html#deploy-hadoop-via-helm",
    "href": "posts/2021-10-12-kubernetes-hadoop.html#deploy-hadoop-via-helm",
    "title": "Deploy Hadoop on Kubernetes on Jetstream",
    "section": "Deploy Hadoop via helm",
    "text": "Deploy Hadoop via helm\nFortunately we have a Helm chart which deploys all the Hadoop components. It is deprecated since November 2020, but it still works fine on Kubernetes 1.19.7.\nClone the usual repository with gh:\ngh repo clone zonca/jupyterhub-deploy-kubernetes-jetstream\ncd hadoop/\nVerify the configuration in stable_hadoop_values.yaml, I’m currently keeping it simple, so no persistence.\nInstall Hadoop via Helm:\nbash install_hadoop.sh\nOnce the pods are running, you should see:\n&gt; kubectl get pods\nNAME                      READY   STATUS    RESTARTS   AGE\nhadoop-hadoop-hdfs-dn-0   1/1     Running   0          144m\nhadoop-hadoop-hdfs-nn-0   1/1     Running   0          144m\nhadoop-hadoop-yarn-nm-0   1/1     Running   0          144m\nhadoop-hadoop-yarn-rm-0   1/1     Running   0          144m"
  },
  {
    "objectID": "posts/2021-10-12-kubernetes-hadoop.html#launch-a-test-job",
    "href": "posts/2021-10-12-kubernetes-hadoop.html#launch-a-test-job",
    "title": "Deploy Hadoop on Kubernetes on Jetstream",
    "section": "Launch a test job",
    "text": "Launch a test job\nGet a terminal on the YARN node manager:\nbash login_yarn.sh\nYou have now access to the Hadoop 2.9.0 cluster. Launch a test MapReduce job to compute pi:\nbin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.0.jar pi 16 1000"
  },
  {
    "objectID": "posts/2021-10-12-kubernetes-hadoop.html#access-the-yarn-dashboard",
    "href": "posts/2021-10-12-kubernetes-hadoop.html#access-the-yarn-dashboard",
    "title": "Deploy Hadoop on Kubernetes on Jetstream",
    "section": "Access the YARN Dashboard",
    "text": "Access the YARN Dashboard\nYou can also export the YARN dashboard from the cluster to your local machine.\nbash expose_yarn.sh\nConnect locally to port 8088 to check the status of the jobs.\nMake sure this port is never exposed publicly. I learned the hard way that there are botnets scanning the internet and compromising the YARN service for crypto-mining, see this article for details."
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html",
    "href": "posts/2021-11-08-fund-healpy.html",
    "title": "Fund healpy via Github Sponsors",
    "section": "",
    "text": "Github Sponsor page"
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#summary",
    "href": "posts/2021-11-08-fund-healpy.html#summary",
    "title": "Fund healpy via Github Sponsors",
    "section": "Summary",
    "text": "Summary\n\nhealpy is low-level necessary tooling for working with cosmology data\nThere is never funding for maintaining healpy\nConsider using grants to fund healpy through Github Sponsors\nThis will fund extra working hours for Andrea Zonca to work on the project"
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#preamble",
    "href": "posts/2021-11-08-fund-healpy.html#preamble",
    "title": "Fund healpy via Github Sponsors",
    "section": "Preamble",
    "text": "Preamble\nIn ~2007 I was a Graduate Student at the Italian Istitute of Astrophysics, I was using numpy, just created a couple of years prior. I remember that Travis Oliphant, the developer of numpy, was asking for a contribution to get a PDF version of the manual. I realized I could have helped development of this key Python package and I was able to purchase it on our funding for the Planck mission."
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#current-status-of-healpy",
    "href": "posts/2021-11-08-fund-healpy.html#current-status-of-healpy",
    "title": "Fund healpy via Github Sponsors",
    "section": "Current status of healpy",
    "text": "Current status of healpy\nI have been the lead maintainer of healpy since 2010, when I was working as a Postdoc at University of California Santa Barbara. The package is now close to a large re-factoring. Martin Reinecke, the developer of HEALPix C++, released a Python package, ducc0, which includes all the low-level functionality needed by healpy. The plan is to create a major rewrite of healpy, changing the interface, and provide a higher level interface to ducc0 as a pure Python package which has all the features of the current healpy versions, i.e. file I/O, plotting, interface to spherical harmonics transforms."
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#funding-for-healpy",
    "href": "posts/2021-11-08-fund-healpy.html#funding-for-healpy",
    "title": "Fund healpy via Github Sponsors",
    "section": "Funding for healpy",
    "text": "Funding for healpy\nThere have never been funding specifically focused on healpy, I have generally took time out of my working week to maintain the package, instead of working on the project I was funded for. This is getting more and more complicated as I advance my career and get more responsibilities.\nWe recently applied for a NASA grant that would fund healpy work at 20% Full Time Equivalent for 3 years, it got “very good” reviews, but was not funded."
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#using-github-sponsors",
    "href": "posts/2021-11-08-fund-healpy.html#using-github-sponsors",
    "title": "Fund healpy via Github Sponsors",
    "section": "Using Github Sponsors",
    "text": "Using Github Sponsors\nSo my idea for long-term sustainability of healpy is to ask projects using healpy on a daily basis to support the maintenance of the package on their grants. After all, back in the day, everybody was paying for IDL licenses. Now, if Principal Investigators, possibly prompted by PostDocs/Grad or Undergrad students, could spend a bit of software money from their grants into funding maintenance of healpy, we could have a healthier sustainability plan for healpy.\nGithub has launched the Github Sponsors program specifically to fund open-source software. People can make a recurring or one-time contribution to a project or to a developer.\nGithub Sponsor page"
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#how-funds-will-be-used",
    "href": "posts/2021-11-08-fund-healpy.html#how-funds-will-be-used",
    "title": "Fund healpy via Github Sponsors",
    "section": "How funds will be used",
    "text": "How funds will be used\nFunds will cover extra hours on top of my work week that I will dedicate exclusively to healpy, in particular:\n\nfix bugs\nreview Pull Requests with code contributions\nwork on the re-factoring from a HEALPix C++ to a ducc backend\nwrite tutorials on how to use healpy for CMB data analysis\nanswer healpy-related questions on Stack Overflow"
  },
  {
    "objectID": "posts/2021-11-08-fund-healpy.html#feedback",
    "href": "posts/2021-11-08-fund-healpy.html#feedback",
    "title": "Fund healpy via Github Sponsors",
    "section": "Feedback",
    "text": "Feedback\nIf you have any feedback, please tweet @andreazonca or email zonca on the domain sdsc.edu."
  },
  {
    "objectID": "posts/2022-01-27-dask-gateway-jupyterhub.html",
    "href": "posts/2022-01-27-dask-gateway-jupyterhub.html",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "",
    "text": "Tutorial OBSOLETE Please check the updated version of this tutorial.\nThis tutorial follows the work by the Pangeo collaboration, the main difference is that I prefer to keep JupyterHub and the Dask infrastructure in 2 separate Helm recipes.\nI assume to start from a Kubernetes cluster already running and JupyterHub deployed on top of it via Helm. And SSL encryption also activated (it isn’t probably necessary, but I haven’t tested that). I tested on Jetstream, but this is agnostic of that."
  },
  {
    "objectID": "posts/2022-01-27-dask-gateway-jupyterhub.html#preparation",
    "href": "posts/2022-01-27-dask-gateway-jupyterhub.html#preparation",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Preparation",
    "text": "Preparation\nClone on the machine you use to run helm and kubectl the repository with the configuration files and scripts:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/\nThen you need to setup one API token, create it with:\nopenssl rand -hex 32\nThen paste it both in dask_gateway/config_jupyterhub.yaml and dask_gateway/config_dask-gateway.yaml, look for the string TOKEN and replace it."
  },
  {
    "objectID": "posts/2022-01-27-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "href": "posts/2022-01-27-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Launch dask gateway",
    "text": "Launch dask gateway\nSee the dask gateway documentation for reference:\n$ helm repo add daskgateway https://dask.org/dask-gateway-helm-repo/\n$ helm repo update\nenter the dask_gateway folder and run:\n$ bash install_dask-gateway.sh\nYou might want to check config_dask-gateway.yaml for extra configuration options, but for initial setup and testing it shouldn’t be necessary.\nAfter this you should see the 3 dask gateway pods running, e.g.:\n$ kubectl -n jhub get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\napi-dask-gateway-64bf5db96c-4xfd6          1/1     Running   2          23m\ncontroller-dask-gateway-7674bd545d-cwfnx   1/1     Running   0          23m\ntraefik-dask-gateway-5bbd68c5fd-5drm8      1/1     Running   0          23m"
  },
  {
    "objectID": "posts/2022-01-27-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "href": "posts/2022-01-27-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Modify the JupyterHub configuration",
    "text": "Modify the JupyterHub configuration\nOnly 2 options need to be changed in JupyterHub:\n\nWe need to run a image which has the same version of dask-gateway we installed on Kubernetes (currently 0.9.0)\nWe need to proxy dask-gateway through JupyterHub so the users can access the Dask dashboard\n\nIf you are using my install_jhub.sh script to deploy JupyterHub, you can modify it and add another values option at the end, --values dask_gateway/config_jupyterhub.yaml.\nYou can modify the image you are using for Jupyterhub in dask_gateway/config_jupyterhub.yaml.\nTo assure that there are not compatibility issues, the “Client” (JupyterHub session), the dask gateway server, the scheduler and the workers should all have the same version of Python and the same version of dask, distributed and dask_gateway. If this is not possible, you can test different combinations and they might work. The Pangeo notebook image I am using has a dask version too new compared to Dask Gateway 0.9.0, so I downgrade it directly in the example Notebook.\nThen redeploy JupyterHub:\nbash install_jhub.sh && cd dask_gateway && bash install_dask-gateway.sh\nCheck that the service is working correctly, if open a browser tab and access https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/api/health, you should see:\n{\"status\": \"pass\"}\nIf this is not working, you can open login to JupyterHub, get a terminal and first check if the service is working:\n&gt;  curl http://traefik-dask-gateway/services/dask-gateway/api/health\nShould give:\n{\"status\": \"pass\"}"
  },
  {
    "objectID": "posts/2022-01-27-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "href": "posts/2022-01-27-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Create a dask cluster",
    "text": "Create a dask cluster\nYou can now login to JupyterHub and check you can connect properly to dask-gateway:\nfrom dask_gateway import Gateway\ngateway = Gateway(\n    address=\"http://traefik-dask-gateway/services/dask-gateway/\",\n    public_address=\"https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/\",\n    auth=\"jupyterhub\")\ngateway.list_clusters()\nThen create a cluster and use it:\ncluster = gateway.new_cluster()\ncluster.scale(2)\nclient = cluster.get_client()\nClient is a standard distributed client and all subsequent calls to dask will go through the cluster.\nPrinting the cluster object gives the link to the Dask dashboard.\nFor a full example and screenshots of the widgets and of the dashboard see:\nhttps://gist.github.com/zonca/355a7ec6b5bd3f84b1413a8c29fbc877\n(Click on the Raw button to download notebook and upload it to your session)."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "",
    "text": "Obsolete: please use the updated release of this tutorial.\nThis is the first tutorial targeted at Jetstream 2. The system is in early access and will be soon made available, see https://jetstream-cloud.org/.\nMy latest tutorial on Jetstream 1 executed Kubespray 2.15.0, here we are also switching to Kubespray 2.18.0, which installs Kubernetes v1.22.5, released in December 2021.\nFor an overview of my work on deploying Kubernetes and JupyterHub on Jetstream, see my Gateways 2020 paper."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#create-jetstream-virtual-machines-with-terraform",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "Create Jetstream Virtual machines with Terraform",
    "text": "Create Jetstream Virtual machines with Terraform\nTerraform allows to execute recipes that describe a set of OpenStack resources and their relationship. In the context of this tutorial, we do not need to learn much about Terraform, we will configure and execute the recipe provided by kubespray.\n\nRequirements\nI have been testing with python-openstackclient version 5.8.0, but any recent openstack client should work. install terraform by copying the correct binary to /usr/local/bin/, see https://www.terraform.io/intro/getting-started/install.html. The requirement is a terraform version &gt; 0.12, I tested with 0.14.4. Newer versions, for example I tried 1.1.7, do not work with Kubespray.\n\n\nRequest API access\nThe procedure to configure API access has changed since Jetstream, make sure you follow the instructions in the Jetstream 2 documentation to create application credentials\nAlso make sure you are not hitting any of the issues in the Troubleshooting page, in particular, it is a good idea to set your password within single quotes to avoid special characters being interpreted by the shell:\nexport OS_APPLICATION_CREDENTIAL_SECRET='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\nTest with:\nopenstack flavor list\nThis should return the list of available “sizes” of the Virtual Machines.\nYou also need to add to the app*openrc.sh also this line:\nexport OS_APPLICATION_CREDENTIAL_NAME=$OS_APPLICATION_CREDENTIAL_ID\notherwise Ansible will fail with you must either set external_openstack_username or external_openstack_application_credential_name.\n\n\nClone kubespray\nI needed to make a few modifications to kubespray to adapt it to Jetstream:\ngit clone https://github.com/zonca/jetstream_kubespray\ngit checkout -b branch_v2.18.0 origin/branch_v2.18.0\nSee an overview of my changes compared to the standard kubespray release 2.18.0, compared to Jetstream 1, there are no actual changes to the kubespray software itself, it is just a matter of configuring it. Anyway, it is convenient to just clone the repository.\nNotice also that the most important change compared to Jetstream 1 is that we are using the new External Openstack provider, instead of the internal Openstack provider which is being discontinued. This also pushed me to use the Cinder CSI plugin, which also has a more modern architecture.\n\n\nGet a projects.jetstream-cloud.org subdomain\nOne option is to use the Designate Openstack service deployed by Jetstream to get an automatically created domain for the instances. In this case the DNS name will be of the form:\nkubejs2-1.tg-xxxxxxxxx.projects.jetstream-cloud.org\nwhere tg-xxxxxxxxx is the ID of your Jestream 2 allocation, you need to specify it at the bottom of cluster.tfvars before running Terraform. The first part of the URL is the instance name, we shortened it removing k8s-master because domains too long do not work with Letsencrypt.\nAfter having executed Terraform, you can pip install on your local machine the package python-designateclient to check what records were created (mind the final period):\nopenstack recordset list tg-xxxxxxxxx.projects.jetstream-cloud.org.\nAs usual with stuff related to DNS, there are delays, so your record could take up to 1 hour to work, or if you delete the instance and create it again with another IP it could take hours to update.\nInstead, if you have a way of getting a domain outside of Jetstream, better reserve a floating IP, see below.\n\n\nReserve a floating IP\nWe prefer not to have a floating IP handled by Terraform, otherwise it would be released every time we need to redeploy the cluster, better create it beforehand:\nopenstack floating ip create public\nThis will return a public floating IP address, it can also be accessed with:\nopenstack floating ip list\nIt is useful to save the IP into the app*openrc.sh, so that every time you load the credentials you also get the address of the master node.\nexport IP=149.xxx.xxx.xxx\n\n\nRun Terraform\nInside jetstream_kubespray, choose a name for the cluster and copy from my template:\nexport CLUSTER=yourclustername\ncp -r inventory/kubejetstream inventory/$CLUSTER\ncd inventory/$CLUSTER\nalso export CLUSTER=yourclustername is useful to add to the app*openrc.sh.\nOpen and modify cluster.tfvars, choose your image (by default Ubuntu 20) and number of nodes and the flavor of the nodes, by default they are medium instances (\"4\").\nPaste the floating ip created previously into k8s_master_fips, unless you are using a projects.jetstream-cloud.org subdomain. Make also sure you add your auto allocated router ID, see instructions inside cluster.tfvars.\nInitialize Terraform:\nbash terraform_init.sh\nCreate the resources:\nbash terraform_apply.sh\nTerraform is very fast in building all the resources, sometimes resources are not ready yet, so the Apply command fails, just run it again, it happens regularly, nothing to worry about.\nYou can SSH into the master node with:\nssh ubuntu@$IP\nInspect with Openstack the resources created:\nopenstack server list\nopenstack network list\nYou can cleanup the virtual machines and all other Openstack resources (all data is lost) with bash terraform_destroy.sh. The floating IP won’t be released so we can create a cluster again from scratch with the same IP address."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-and-test-ansible",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-and-test-ansible",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "Install and test Ansible",
    "text": "Install and test Ansible\nChange folder back to the root of the jetstream_kubespray repository,\nFirst make sure you have a recent version of ansible installed, you also need additional modules, so first run:\npip install -r requirements.txt\nThis pip script installs a predefined version of ansible, currently 2.10.15, so it is useful to create a virtualenv or a conda environment and install packages inside that.\nThen following the kubespray documentation, we setup ssh-agent so that ansible can SSH from the machine with public IP to the others:\neval $(ssh-agent -s)\nssh-add ~/.ssh/id_rsa\nTest the connection through ansible:\nansible -i inventory/$CLUSTER/hosts -m ping all"
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-kubernetes-with-kubespray",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "Install Kubernetes with kubespray",
    "text": "Install Kubernetes with kubespray\nIn inventory/$CLUSTER/group_vars/k8s_cluster/k8s-cluster.yml, set the public floating IP of the master instance in supplementary_addresses_in_ssl_keys.\nFinally run the full playbook, it is going to take a good 10 minutes, go make coffee:\nbash k8s_install.sh\nIf the playbook fails with “cannot lock the administrative directory”, it is due to the fact that the Virtual Machine is automatically updating so it has locked the APT directory. Just wait a minute and launch it again. It is always safe to run ansible multiple times.\nIf the playbook gives any error, try to retry the above command, sometimes there are temporary failed tasks, Ansible is designed to be executed multiple times with consistent results.\nYou should have now a Kubernetes cluster running, test it:\n$ ssh ubuntu@$IP\n$ sudo su\n$ kubectl get pods --all-namespaces\nNAMESPACE       NAME                                           READY   STATUS    RESTARTS   AGE\ningress-nginx   ingress-nginx-controller-4xd64                 1/1     Running   0          27m\nkube-system     coredns-8474476ff8-9gd8w                       1/1     Running   0          27m\nkube-system     coredns-8474476ff8-qtshk                       1/1     Running   0          27m\nkube-system     csi-cinder-controllerplugin-9fb5946bf-hwfhp    6/6     Running   0          26m\nkube-system     csi-cinder-nodeplugin-r69nl                    3/3     Running   0          26m\nkube-system     dns-autoscaler-5ffdc7f89d-s4sj4                1/1     Running   0          27m\nkube-system     kube-apiserver-kubejs2-k8s-master-1            1/1     Running   1          66m\nkube-system     kube-controller-manager-kubejs2-k8s-master-1   1/1     Running   1          66m\nkube-system     kube-flannel-2clqv                             1/1     Running   0          29m\nkube-system     kube-flannel-2wbtq                             1/1     Running   0          29m\nkube-system     kube-proxy-hmz6t                               1/1     Running   0          30m\nkube-system     kube-proxy-xkhjx                               1/1     Running   0          30m\nkube-system     kube-scheduler-kubejs2-k8s-master-1            1/1     Running   1          66m\nkube-system     nginx-proxy-kubejs2-k8s-node-1                 1/1     Running   0          64m\nkube-system     nodelocaldns-jwxg8                             1/1     Running   0          27m\nkube-system     nodelocaldns-z4sjl                             1/1     Running   0          27m\nkube-system     openstack-cloud-controller-manager-6q28z       1/1     Running   0          29m\nkube-system     snapshot-controller-786647474f-7x8zx           1/1     Running   0          25m\n\nCompare that you have all those services running also in your cluster. We have also configured NGINX to proxy any service that we will later deploy on Kubernetes, test it with:\n$ wget localhost\n--2022-03-31 06:51:20--  http://localhost/\nResolving localhost (localhost)... 127.0.0.1\nConnecting to localhost (localhost)|127.0.0.1|:80... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2022-03-31 06:51:20 ERROR 404: Not Found.\nError 404 is a good sign, the service is up and serving requests, currently there is nothing to deliver. If any of the tests hangs or cannot connect, there is probably a networking issue."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#set-the-default-storage-class",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#set-the-default-storage-class",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "Set the default storage class",
    "text": "Set the default storage class\nKubespray sets up the cinder-csi storage class, but it is not set as default, we can fix it with:\nkubectl patch storageclass cinder-csi -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\nso that we don’t need to explicitely configure it in the applications we deploy."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#optional-setup-kubectl-locally",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "(Optional) Setup kubectl locally",
    "text": "(Optional) Setup kubectl locally\nInstall kubectl locally, I am currently using 1.20.\nWe also set kubeconfig_localhost: true, which copies the kubectl configuration admin.conf to:\ninventory/$CLUSTER/artifacts\nI have a script to replace the IP with the floating IP of the master node, for this script to work make sure you have exported the variable IP:\nbash k8s_configure_kubectl_locally.sh\nFinally edit again the app*openrc.sh and add:\nexport KUBECONFIG=$(pwd -P)/\"jetstream_kubespray/inventory/$CLUSTER/artifacts/admin.conf\""
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#optional-setup-helm-locally",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#optional-setup-helm-locally",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "(Optional) Setup helm locally",
    "text": "(Optional) Setup helm locally\nInstall helm 3 from the release page on Github\nI tested with v3.8.1."
  },
  {
    "objectID": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-jupyterhub",
    "href": "posts/2022-03-30-jetstream2_kubernetes_kubespray.html#install-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream 2 with Kubespray 2.18.0",
    "section": "Install Jupyterhub",
    "text": "Install Jupyterhub\nFollow up in the next tutorial: Install JupyterHub on Jetstream 2 on top of Kubernetes"
  },
  {
    "objectID": "posts/2022-04-04-dask-gateway-jupyterhub.html",
    "href": "posts/2022-04-04-dask-gateway-jupyterhub.html",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "",
    "text": "Tutorial obsolete, see the new version of the tutorial\nUpdated 28 April 2022: switched to Dask Gateway 2022.4.0\nIn this tutorial we will install Dask Gateway on Kubernetes and configure JupyterHub so Jupyter Notebook users can launch private Dask cluster and connect to them.\nI assume to start from a Kubernetes cluster already running and JupyterHub deployed on top of it via Helm. And SSL encryption also activated (it isn’t probably necessary, but I haven’t tested that). I tested on Jetstream 2, but the recipe should be agnostic of that."
  },
  {
    "objectID": "posts/2022-04-04-dask-gateway-jupyterhub.html#preparation",
    "href": "posts/2022-04-04-dask-gateway-jupyterhub.html#preparation",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Preparation",
    "text": "Preparation\nClone on the machine you use to run helm and kubectl the repository with the configuration files and scripts:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/\nThen you need to setup one API token, create it with:\nopenssl rand -hex 32\nThen paste it both in dask_gateway/config_jupyterhub.yaml and dask_gateway/config_dask-gateway.yaml, look for the string TOKEN and replace it."
  },
  {
    "objectID": "posts/2022-04-04-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "href": "posts/2022-04-04-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Launch dask gateway",
    "text": "Launch dask gateway\nWe can install version 2022.4.0 with:\n$ bash install_dask-gateway.sh\nYou might want to check config_dask-gateway.yaml for extra configuration options, but for initial setup and testing it shouldn’t be necessary.\nAfter this you should see the 3 dask gateway pods running, e.g.:\n$ kubectl -n jhub get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\napi-dask-gateway-64bf5db96c-4xfd6          1/1     Running   2          23m\ncontroller-dask-gateway-7674bd545d-cwfnx   1/1     Running   0          23m\ntraefik-dask-gateway-5bbd68c5fd-5drm8      1/1     Running   0          23m"
  },
  {
    "objectID": "posts/2022-04-04-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "href": "posts/2022-04-04-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Modify the JupyterHub configuration",
    "text": "Modify the JupyterHub configuration\nOnly 2 options need to be changed in JupyterHub:\n\nWe need to run a image which has the same version of dask-gateway we installed on Kubernetes (currently 0.9.0)\nWe need to proxy dask-gateway through JupyterHub so the users can access the Dask dashboard\n\nIf you are using my install_jhub.sh script to deploy JupyterHub, you can modify it and add another values option at the end, --values dask_gateway/config_jupyterhub.yaml.\nYou can modify the image you are using for Jupyterhub in dask_gateway/config_jupyterhub.yaml.\nTo assure that there are not compatibility issues, the “Client” (JupyterHub session), the dask gateway server, the scheduler and the workers should all have the same version of Python and the same version of dask, distributed and dask_gateway. If this is not possible, you can test different combinations and they might work.\nThen redeploy JupyterHub:\nbash install_jhub.sh && cd dask_gateway && bash install_dask-gateway.sh\nCheck that the service is working correctly, if open a browser tab and access https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/api/health, you should see:\n{\"status\": \"pass\"}\nIf this is not working, you can open login to JupyterHub, get a terminal and first check if the service is working:\n&gt;  curl http://traefik-dask-gateway/services/dask-gateway/api/health\nShould give:\n{\"status\": \"pass\"}"
  },
  {
    "objectID": "posts/2022-04-04-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "href": "posts/2022-04-04-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Create a dask cluster",
    "text": "Create a dask cluster\nYou can now login to JupyterHub and check you can connect properly to dask-gateway:\nfrom dask_gateway import Gateway\ngateway = Gateway(\n    address=\"http://traefik-dask-gateway/services/dask-gateway/\",\n    public_address=\"https://js-XXX-YYY.jetstream-cloud.org/services/dask-gateway/\",\n    auth=\"jupyterhub\")\ngateway.list_clusters()\nThen create a cluster and use it:\ncluster = gateway.new_cluster()\ncluster.scale(2)\nclient = cluster.get_client()\nClient is a standard distributed client and all subsequent calls to dask will go through the cluster.\nPrinting the cluster object gives the link to the Dask dashboard.\nFor a full example and screenshots of the widgets and of the dashboard see:\nhttps://gist.github.com/zonca/355a7ec6b5bd3f84b1413a8c29fbc877\n(Click on the Raw button to download notebook and upload it to your session)."
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html",
    "title": "Science Gateway with Dask and Zarr",
    "section": "",
    "text": "This material was presented on April 2022 at the MiniGateways 2022 conference organized by the wonderful Science Gateways Community Institute (SGCI).\nSee the slides and the abstract on Figshare, please also cite the Figshare record if you are referencing this material.\nIn this tutorial, we will glue several intersting technologies together to show a toy Science Gateway deployment which runs inside Kubernetes, uses Dask to scale up distributed computations across multiple workers and writes output data to Object store using the Zarr format."
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#requirements",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#requirements",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Requirements",
    "text": "Requirements\n\nKubernetes deployment on Jetstream 2\nOptionally have JupyterHub installed as well to interact with the gateway locally\nDask Gateway to handle Dask clusters\nAWS style credentials for writing to object store, see the tutorial about Zarr on Jetstream 2"
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#architecture",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#architecture",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#authentication",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#authentication",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Authentication",
    "text": "Authentication\nAs usual first checkout the zonca/jupyterhub-deploy-kubernetes-jetstream repository.\nDask gateway has been deployed with JupyterHub based autentication, therefore we need to create a Token from the JupyterHub control panel at https://jupyterhub-address.edu/hub/token\nRequest a new token with no expiration named for example gatewaydaskzarr, then save it into the my_aws_config file, the same used for the AWS credentials:\njhub_token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nWe need to provide AWS credentials both to the gateway application and to the dask workers, so we store them in a Kubernetes Secret:\ncd gateway-dask-zarr\nbash create_secret.sh\nWe then configure Dask Gateway so it mounts this secret into the workers, add:\nc.KubeClusterConfig.worker_extra_container_config = {\n            \"envFrom\": [\n                        {\"secretRef\": {\"name\": \"awsconfig\"}}\n                            ]\n                        }\nto dask_gateway/config_dask-gateway.yaml and redeploy Dask Gateway with Helm."
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#deploy-the-gateway",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#deploy-the-gateway",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Deploy the gateway",
    "text": "Deploy the gateway\nThe gateway itself is a toy Flask app, see the gateway.py file in the gateway-dask-zarr folder.\nWe can create a Kubernetes Deployment with:\nkubectl create -f deploy_gateway.yaml\n\nIt pulls the zonca/gateway-dask-zarr:latest which has been build with the Dockerfile in the gateway-dask-zarr folder\nIt mounts the secret with AWS and JupyterHub credentials\nIn the initialization phase it creates a Dask cluster with 3 workers that will be used for all the jobs\n\nWe can test that the deployment has been successful by logging in to the JupyterHub deployment on Jetstream and running the first cells of this notebook on Gist."
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#run-jobs",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#run-jobs",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Run jobs",
    "text": "Run jobs\nRunning the following cells in the test notebook above we can launch a job, every time we send a get request to the URL:\nhttp://gateway-svc/submit_job/&lt;job_id&gt;\nthe gateway:\n\nGets a “Client” instance connected to the Dask cluster\nPrepares a 1000x1000 Zarr array with 100 chunks of size 100x100\nInstructs the 3 Dask workers to create a random array distributely\nInstructs the 3 Dask workers to write that array concurrently to Object Store as a Zarr file\n\nTherefore no computation is executed in the Flask App, it is offloaded to the Dask workers. Data transfer as well directly flows from the workers to Object Store."
  },
  {
    "objectID": "posts/2022-04-05-science-gateway-dask-zarr.html#inspect-the-results",
    "href": "posts/2022-04-05-science-gateway-dask-zarr.html#inspect-the-results",
    "title": "Science Gateway with Dask and Zarr",
    "section": "Inspect the results",
    "text": "Inspect the results\nFinally, in the same notebook, we can access Object Store directly, load and plot the data."
  },
  {
    "objectID": "posts/2022-04-15-jetstream2-su-calculator.html",
    "href": "posts/2022-04-15-jetstream2-su-calculator.html",
    "title": "Jetstream2 SU calculator",
    "section": "",
    "text": "Jupyter Notebook to compute daily, monthly, yearly consumption of SU based on the number and type of Virtual Machines:\nhttps://gist.github.com/zonca/3cc70af86d24a0bcc176f2b18d0cc9dd#file-jetstream2_su_calculator-ipynb\nRun in the browser via Google Colab:\nhttps://colab.research.google.com/gist/zonca/3cc70af86d24a0bcc176f2b18d0cc9dd/jetstream2_su_calculator.ipynb"
  },
  {
    "objectID": "posts/2022-05-02-ssh-github-action.html",
    "href": "posts/2022-05-02-ssh-github-action.html",
    "title": "Access running Github action with SSH",
    "section": "",
    "text": "Sometimes Github actions are failing and it is difficult to reproduce the error locally, in particular if you have a different OS.\nFortunately we can use the Debugging with SSH Github action, make a temporary branch, and add this step before the step that produces an error:\n  - name: Setup upterm session\n    uses: lhotari/action-upterm@v1\n    with:\n      ## limits ssh access and adds the ssh public key for the user which triggered the workflow\n      limit-access-to-actor: false\nNOTE: Anybody can connect to this session, so make sure you don’t have sensitive data\nCreate a pull request to trigger execution of the Github action workflow.\nThen check the logs of your Linux and Mac OS builds, you should find a connection string of the form:\nssh &lt;somestring&gt;:&lt;somestring&gt;=@uptermd.upterm.dev\nType it in a terminal to connect to the virtual machine running on Github and debug the issue interactively.\nIf you get a “Permission denied (public key)” on Linux, see this workaround, pasted here for convenience (you don’t need to add this key to your github account):\nssh-keygen -o -a 100 -t ed25519 -f ~/.ssh/id_ed25519 -C \"yourusername@company\"\nssh -i ~/.ssh/id_ed25519 &lt;somestring&gt;:&lt;somestring&gt;=@uptermd.upterm.dev\nSee the pull request I used for testing\nOnce done, take a look at the current running Actions and cancel any leftover runs."
  },
  {
    "objectID": "posts/2022-06-06-mariadb.html",
    "href": "posts/2022-06-06-mariadb.html",
    "title": "Deploy MariaDB on Jetstream 2 on top of Kubernetes",
    "section": "",
    "text": "In this tutorial we will install a MariaDB instance backed by a persistent volume on Jetstream 2. It will be in the jhub namespace, so that it can be accessed by the JupyterHub users and from no other namespace.\nAs usual all configuration files and scripts are in my reference repository:"
  },
  {
    "objectID": "posts/2022-06-06-mariadb.html#install-via-helm",
    "href": "posts/2022-06-06-mariadb.html#install-via-helm",
    "title": "Deploy MariaDB on Jetstream 2 on top of Kubernetes",
    "section": "Install via Helm",
    "text": "Install via Helm\nBitnami provides a nicely prepackaged MariaDB instance via Helm, modify the mariadb/values.yaml file, in particular set all the passwords to randomly generated values.\nI have configured the recipe so that:\n\ndatabase name is mariadbk8s\nnon root username is mariadbuser\n\nInstall it with\nbash install_mariadb.sh"
  },
  {
    "objectID": "posts/2022-06-06-mariadb.html#load-data-from-a-sql-dump",
    "href": "posts/2022-06-06-mariadb.html#load-data-from-a-sql-dump",
    "title": "Deploy MariaDB on Jetstream 2 on top of Kubernetes",
    "section": "Load data from a SQL dump",
    "text": "Load data from a SQL dump\nOnce the database is running, follow the printout of the Helm recipe on how to get a temporary pod to connect to the database.\nOnce that is running, you will have a terminal running, there you can get your SQL dump for example from gist:\ncd /tmp\ncurl https://gist.github.com/zonca/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/raw/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/dump.sql --output dump.sql -L\nFinally ingest the data (will need to paste the root password from values.yaml):\nmysql -h mariadb.jhub.svc.cluster.local -uroot -p mariadbk8s &lt; dump.sql"
  },
  {
    "objectID": "posts/2022-06-06-mariadb.html#add-support-in-user-containers",
    "href": "posts/2022-06-06-mariadb.html#add-support-in-user-containers",
    "title": "Deploy MariaDB on Jetstream 2 on top of Kubernetes",
    "section": "Add support in user containers",
    "text": "Add support in user containers\nFinally you need to make sure that the mariadb-client package is installed in the Jupyter single user OS, and in the Python environment you will need the mariadb package and possibly sqlalchemy.\nFor example Centos 7 needs the MariaDB custom repositories and the packages:\nMariaDB-devel MariaDB-connect-engine\nThe connection string for SQLAlchemy will be:\nfrom urllib.parse import quote_plus as urlquote\npw = urlquote('xxxxxxxxxxxxxxxxxxxx')\nengine = sqlalchemy.create_engine(f\"mariadb+mariadbconnector://mariadbuser:{pw}@mariadb.jhub.svc.cluster.local:3306/mariadbk8s\")"
  },
  {
    "objectID": "posts/2020-03-16-raise-check-flag-numpy.html",
    "href": "posts/2020-03-16-raise-check-flag-numpy.html",
    "title": "Raise and check a flag array with numpy",
    "section": "",
    "text": "Often to describe data quality of timelines or images, we use array of integers where each of its bit has a specific meaning, so that we can identify what issues affect each data point.\nFor example we have 10 data points, and we assign an array of 8 bits for data quality. Generally 0 means a good data point, any bit raised is sign of some problem in the data, this is more compressed then using different boolean arrays, and allows to make batch np.bitwise_and and np.bitwise_or operations.\nimport numpy as np\nflag = np.zeros(10, dtype=np.uint8)\nThe array uses just 8 bits per element\n%whos\n\nVariable   Type       Data/Info\n-------------------------------\nflag       ndarray    10: 10 elems, type `uint8`, 10 bytes\nnp         module     &lt;module 'numpy' from '/ho&lt;...&gt;kages/numpy/__init__.py'&gt;\nRaising a bit seems as easy as adding 2**bit value to the array, for example the 4th bit is 16, so:\nflag[2:5] += 2**4\nflag\n\narray([ 0,  0, 16, 16, 16,  0,  0,  0,  0,  0], dtype=uint8)\nThe issue is that only works if that bit was 0, if it was already raised, we would actually zero it and set the higher bit to 1:\nflag[2] += 2**4\nflag\n\narray([ 0,  0, 32, 16, 16,  0,  0,  0,  0,  0], dtype=uint8)"
  },
  {
    "objectID": "posts/2020-03-16-raise-check-flag-numpy.html#use-bitwise-operations",
    "href": "posts/2020-03-16-raise-check-flag-numpy.html#use-bitwise-operations",
    "title": "Raise and check a flag array with numpy",
    "section": "Use bitwise operations",
    "text": "Use bitwise operations\nFortunately numpy supports bitwise operations that make this easier, see the 2 functions below:\n\ndef raise_bit_inplace(flag, bit=0):\n    \"\"\"Raise bit of the flag array in place\n    \n    This function modifies the input array,\n    it also works on slices\n    \n    Parameters\n    ----------\n    flag : np.array\n        flag bit-array, generally unsigned integer\n    bit : int\n        bit number to raise\n    \"\"\"\n    flag[:] = np.bitwise_or(flag, 2**bit)\n\n\ndef raise_bit(flag, bit=0):\n    \"\"\"Raise bit of the flag array\n\n    Parameters\n    ----------\n    flag : np.array\n        flag bit-array, generally unsigned integer\n    bit : int\n        bit number to raise\n        \n    Returns\n    -------\n    output_flag : np.array\n        input array with the requested bit raised\n    \"\"\"\n    return np.bitwise_or(flag, 2**bit)\n\n\ndef check_bit(flag, bit=0):\n    \"\"\"Check if bit of the flag array is raised\n\n    The output is a boolean array which could\n    be used for slicing another array.\n    \n    Parameters\n    ----------\n    flag : np.array\n        flag bit-array, generally unsigned integer\n    bit : int\n        bit number to check\n        \n    Returns\n    -------\n    is_raised : bool np.array\n        True if the bit is raised, False otherwise    \n    \"\"\"\n    return np.bitwise_and(flag, int(2**bit)) &gt; 0\n\n\nis_bit4_raised = check_bit(flag, bit=4)\n\n\nis_bit4_raised\n\narray([False, False, False,  True,  True, False, False, False, False,\n       False])\n\n\n\nassert np.all(is_bit4_raised[3:5])\n\nThey also work with slices of an array:\n\nraise_bit_inplace(flag[6:], bit=1)\n\n\nflag\n\narray([ 0,  0, 32, 16, 16,  0,  2,  2,  2,  2], dtype=uint8)\n\n\n\n# Running it twice doesn't change the value of the flag\nraise_bit_inplace(flag[6:], bit=1)\n\n\nflag\n\narray([ 0,  0, 32, 16, 16,  0,  2,  2,  2,  2], dtype=uint8)\n\n\n\ncheck_bit(flag, 1)\n\narray([False, False, False, False, False, False,  True,  True,  True,\n        True])\n\n\nFirst blog post using a Jupyter Notebook with fastpages!!"
  },
  {
    "objectID": "posts/2020-06-19-white-noise-hitmap-fullsky.html",
    "href": "posts/2020-06-19-white-noise-hitmap-fullsky.html",
    "title": "Handle white noise with healpy 1 Full sky coverage",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport astropy.units as u\nIn this series of notebooks, we will understand how to handle white noise in the case of an experiment with sky observations which are both not-uniform and have partial sky coverage.\nLet’s first start assuming a sensitivity of an experiment array of detectors of \\(10 \\mu K \\sqrt(s)\\).\n# Number based on Simons Observatory SAT UHF1 array of detectors\n\nnet = 10. * u.Unit(\"uK * sqrt(s)\")\n5 years with a efficiency of 20%:\nintegration_time_total = 5 * u.year * .2"
  },
  {
    "objectID": "posts/2020-06-19-white-noise-hitmap-fullsky.html#uniform-full-sky-survey",
    "href": "posts/2020-06-19-white-noise-hitmap-fullsky.html#uniform-full-sky-survey",
    "title": "Handle white noise with healpy 1 Full sky coverage",
    "section": "Uniform full sky survey",
    "text": "Uniform full sky survey\nAs a reference, let’s first start with the trivial case of uniform full sky coverage, i.e. we spend the same amount of observation time in each pixel.\n\nnside = 512\nnpix = hp.nside2npix(nside)\n\n\nstandard_deviation_per_pixel = (net / np.sqrt(integration_time_total/npix)).decompose()\n\n\nstandard_deviation_per_pixel\n\n\\(3.1572473 \\times 10^{-6} \\; \\mathrm{K}\\)\n\n\n\nm = np.random.normal(scale = standard_deviation_per_pixel.value, size=npix) * standard_deviation_per_pixel.unit\n\n\nm = m.to(u.uK)\n\n\nhp.mollview(m, unit=m.unit, title=\"White noise map\")"
  },
  {
    "objectID": "posts/2020-06-19-white-noise-hitmap-fullsky.html#power-spectrum",
    "href": "posts/2020-06-19-white-noise-hitmap-fullsky.html#power-spectrum",
    "title": "Handle white noise with healpy 1 Full sky coverage",
    "section": "Power spectrum",
    "text": "Power spectrum\nFinally we can compute the angular power spectrum with anafast, i.e. the power as a function of the angular scales, from low \\(\\ell\\) values for large angular scales, to high \\(\\ell\\) values for small angular scales.\nAt low \\(\\ell\\) there is not much statistics and the power spectrum is biased, but if we exclude lower ells, we can have an estimate of the white noise \\(C_\\ell\\) coefficients. We can then compare with the theoretical power computed as:\n\\[ C_\\ell = \\Omega_{pix}\\sigma^2 \\]\nWhere: \\(\\Omega_{pix}\\) is the pixel are in square-radians and \\(\\sigma^2\\) is the white noise variance.\n\ncl = hp.anafast(m)\n\n\ncl[100:].mean()\n\n3.9283892627207396e-05\n\n\n\nm.std()\n\n\\(3.1567443 \\; \\mathrm{\\mu K}\\)\n\n\n\npixel_area = hp.nside2pixarea(nside)\n\n\nwhite_noise_cl = (standard_deviation_per_pixel**2 * pixel_area).to(u.uK**2)\n\n\nwhite_noise_cl\n\n\\(3.9820426 \\times 10^{-5} \\; \\mathrm{\\mu K^{2}}\\)\n\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.hlines(white_noise_cl.value, 0, len(cl), label=\"White noise level\")\nplt.title(\"Fullsky white noise spectrum\")\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\");"
  },
  {
    "objectID": "posts/2020-06-21-white-noise-hitmap-not-uniform.html",
    "href": "posts/2020-06-21-white-noise-hitmap-not-uniform.html",
    "title": "Handle white noise with healpy 3 not-uniform and partial sky coverage",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport astropy.units as u\nhp.disable_warnings()\nIn this third notebook, we will handle a case of not-uniform and partial sky coverage.\n# Number based on Simons Observatory SAT UHF1 array of detectors\n\nnet = 10. * u.Unit(\"uK * sqrt(s)\")\n5 years with a efficiency of 20%:\nintegration_time_total = 5 * u.year * .2"
  },
  {
    "objectID": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#download-a-hitmap",
    "href": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#download-a-hitmap",
    "title": "Handle white noise with healpy 3 not-uniform and partial sky coverage",
    "section": "Download a hitmap",
    "text": "Download a hitmap\nWe can download a simulated hitmap for a Simons Observatory band, for now however, we assume a uniform coverage.\n\nhitmap_url = \"https://portal.nersc.gov/project/sobs/so_mapsims_data/v0.2/healpix/ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\"\n\n\n!wget $hitmap_url\n\n--2020-12-11 10:35:42--  https://portal.nersc.gov/project/sobs/so_mapsims_data/v0.2/healpix/ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\nResolving portal.nersc.gov (portal.nersc.gov)... 128.55.206.24, 128.55.206.26\nConnecting to portal.nersc.gov (portal.nersc.gov)|128.55.206.24|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3212515 (3.1M) [application/x-gzip]\nSaving to: ‘ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz.10’\n\nST0_UHF1_01_of_20.n 100%[===================&gt;]   3.06M  --.-KB/s    in 0.1s    \n\n2020-12-11 10:35:43 (30.0 MB/s) - ‘ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz.10’ saved [3212515/3212515]\n\n\n\n\nhitmap = hp.read_map(\"ST0_UHF1_01_of_20.nominal_telescope_all_time_all_hmap.fits.gz\")\n\n\nhitmap /= hitmap.max()\n\n\nhp.mollview(hitmap)"
  },
  {
    "objectID": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#generic-partial-sky-survey",
    "href": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#generic-partial-sky-survey",
    "title": "Handle white noise with healpy 3 not-uniform and partial sky coverage",
    "section": "Generic partial sky survey",
    "text": "Generic partial sky survey\nWe have now a sky coverage which not uniform\n\nnside = 512\nnpix = hp.nside2npix(nside)\n\n\nhitmap = hitmap / hitmap.sum() * integration_time_total.to(u.s)\n\n\nhitmap_plot = hitmap.value.copy()\nhitmap_plot[hitmap == 0] = hp.UNSEEN\n\n\nhp.mollview(hitmap_plot, unit=hitmap.unit)\n\n\n\n\n\n\n\n\n\nvariance_per_pixel = \\\n    (net**2 / hitmap).decompose()\n\n/home/zonca/zonca/p/software/healpy/healpyvenv/lib/python3.7/site-packages/astropy/units/quantity.py:477: RuntimeWarning: divide by zero encountered in true_divide\n  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n\n\n\nvariance_per_pixel[np.isinf(variance_per_pixel)] = 0\n\n\nm = np.random.normal(scale = np.sqrt(variance_per_pixel),\n                     size=len(variance_per_pixel)) * np.sqrt(variance_per_pixel).unit\n\n\nvariance_per_pixel.max()\n\n\\(4.51983 \\times 10^{-7} \\; \\mathrm{K^{2}}\\)\n\n\n\nm.value[hitmap==0] = hp.UNSEEN\n\n\nm = m.to(u.uK)\n\n\nm.value[hitmap==0] = hp.UNSEEN\n\n\nhp.mollview(m, unit=m.unit, min=-10, max=10, title=\"White noise map\")"
  },
  {
    "objectID": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#power-spectrum",
    "href": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#power-spectrum",
    "title": "Handle white noise with healpy 3 not-uniform and partial sky coverage",
    "section": "Power spectrum",
    "text": "Power spectrum\n\nsky_fraction = hp.mask_good(m).sum()/len(m)\n\n\ncl = hp.anafast(m) / sky_fraction\n\n\ncl[100:].mean()\n\n0.0044214096150382255\n\n\n\npixel_area = hp.nside2pixarea(nside)\n\n\nwhite_noise_cl = (variance_per_pixel[variance_per_pixel&gt;0].mean() * pixel_area).to(u.uK**2)\n\n\nwhite_noise_cl_uniform = 1.5341266e-5 * u.uK**2\n\n\nplt.figure(figsize=(6,4))\nplt.loglog(cl, label=\"Map power spectrum\", alpha=.7)\nplt.hlines(white_noise_cl.value, 0, len(cl), color=\"blue\",\n           label=\"White noise level\")\nplt.hlines(white_noise_cl_uniform.value, 0, len(cl),\n           label=\"White noise level uniform\")\nplt.title(\"Fullsky white noise spectrum\")\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\");\n\n\n\n\n\n\n\n\n\nsky_fraction\n\n0.38526121775309247"
  },
  {
    "objectID": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#pixel-weighting-in-the-power-spectrum",
    "href": "posts/2020-06-21-white-noise-hitmap-not-uniform.html#pixel-weighting-in-the-power-spectrum",
    "title": "Handle white noise with healpy 3 not-uniform and partial sky coverage",
    "section": "Pixel weighting in the power spectrum",
    "text": "Pixel weighting in the power spectrum\nWhen we have un-uniform noise across the map, it is advantageous to weight the pixels differently before taking the power spectrum in order to downweight the noisiest pixels.\nIf we weight by the square root of the number of hits per pixels, then we are normalizing the standard deviation per pixel to be the same across all pixels, in fact we recover the same noise level we had when we were spreading the hits uniformly in the sky patch.\nHowever, the optimal is actually to weight by the inverse variance, which means to weight by the hitmap, to estimate the value expected for this we need to weight the variance by the square of the hitmap (variance is in power so weighting the map by a quantity is equivalent to weighting the variance by its square).\n\ncl_apodized = hp.anafast(m * np.sqrt(hitmap)) / np.mean(hitmap)\n\n\ncl_apodized_inv_variance = hp.anafast(m * hitmap) / np.mean(hitmap**2)\n\n\ncl_apodized_inv_variance[100:].mean() / white_noise_cl_uniform.value\n\n\\(0.49056891 \\; \\mathrm{\\frac{1}{s^{2}}}\\)\n\n\n\nwhite_noise_cl / white_noise_cl_uniform\n\n\\(291.08959 \\; \\mathrm{}\\)\n\n\n\nwhite_noise_cl\n\n\\(0.0044656828 \\; \\mathrm{\\mu K^{2}}\\)\n\n\n\nwhite_noise_cl_uniform\n\n\\(1.5341266 \\times 10^{-5} \\; \\mathrm{\\mu K^{2}}\\)\n\n\n\nnp.average(variance_per_pixel, weights=hitmap) * pixel_area * 1e12\n\n\\(1.5341266 \\times 10^{-5} \\; \\mathrm{K^{2}}\\)\n\n\n\nwhite_noise_cl_inv_variance = np.average(variance_per_pixel, weights=hitmap**2) * pixel_area * 1e12\n\n\nplt.figure(figsize=(10,6))\nplt.loglog(cl, label=\"White noise equally weighted\", alpha=.7)\nplt.hlines(white_noise_cl.value, 0, len(cl), color=\"blue\", ls=\":\",\n           label=\"White noise level equally weighted\")\n\nplt.loglog(cl_apodized, label=\"White noise inverse weighted with stddev\", alpha=.7)\n\nplt.hlines(white_noise_cl_uniform.value, 0, len(cl), color=\"red\", ls=\":\",\n           label=\"White noise level for map with uniform hits\")\n\nplt.loglog(cl_apodized_inv_variance, label=\"White noise inverse weighted with variance\", alpha=.7)\nplt.hlines(white_noise_cl_inv_variance.value, 0, len(cl), color=\"darkgreen\", ls=\":\",\n           label=\"White noise level inverse weighted\")\nplt.title(\"Fullsky white noise spectrum\")\nplt.legend()\nplt.xlabel(\"$\\ell$\")\nplt.ylabel(\"$C_\\ell [\\mu K ^ 2]$\");"
  },
  {
    "objectID": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html",
    "href": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html",
    "title": "Investigate broken implementation of bandpass unit conversion in PySM 3",
    "section": "",
    "text": "import pysm3\nimport numpy as np\nimport healpy as hp\nfrom astropy.tests.helper import assert_quantity_allclose\nimport pysm3.units as u\nInvestigation of the broken implementation of the bandpass unit conversion function in PySM 3 and check of its impact on existing Simons Observatory and CMB-S4 simulated datasets.\nSee the related issue and the pull request with the fix. This affects all PySM 3 versions &lt;3.2.2, it will be fixed in 3.3.0.\n$ = g() I()d$\nIf we consider emission of the CMB in \\(K_{CMB}\\) units, \\(I_{CMB}\\) is not a function of \\(\\nu\\):\n$ {CMB}[K_{CMB}] = g() I{CMB}[K_{CMB}] d= I_{CMB}[K_{CMB}] g() d= I_{CMB}[K_{CMB}]$\n$ {CMB}[K_{RJ}] = I{CMB}[K_{CMB}] g() C_{K_{CMB}}^{K_{RJ}}() d$\nHowever we assume that the bandpass of the instrument \\(g(\\nu)\\) is given in power units, i.e. \\(Jy/sr\\), the python normalize_weights functions does:\n$ g [K_{RJ}/K_{RJ}] = $\nCMB bandpass integrated converted to \\(K_{RJ}\\).\n$ {CMB}[K_{RJ}] = I{CMB}[K_{CMB}] g()[K_{RJ}/K_{RJ}] C_{K_{CMB}}^{K_{RJ}}() d= I_{CMB}[K_{CMB}] C_{K_{CMB}}^{K_{RJ}}() d$\nYou can think the last equation as getting a value in \\(K_{CMB}\\), turn it into \\(K_{RJ}\\) and then to \\(Jy~sr^{-1}\\), and do the relative weighting then.\nHowever we assume that the bandpass of the instrument \\(g(\\nu)\\) is given in power units, i.e. \\(Jy/sr\\), the python normalize_weights functions does:\n$ g [K_{RJ}/K_{RJ}] = $"
  },
  {
    "objectID": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#from-k_cmb-to-k_rj",
    "href": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#from-k_cmb-to-k_rj",
    "title": "Investigate broken implementation of bandpass unit conversion in PySM 3",
    "section": "From \\(K_{CMB}\\) to \\(K_{RJ}\\):",
    "text": "From \\(K_{CMB}\\) to \\(K_{RJ}\\):\nIf the output is requested in \\(K_{CMB}\\), we basically have to undo that steps, so:\n$ {CMB}[K_{RJ}] = I{CMB}[K_{CMB}] C_{K_{CMB}}^{K_{RJ}}() d$\n\nfixed_bandpass_unit_conversion\n$ {CMB}[K_{CMB}] = {CMB}[K_{RJ}] { C_{K_{CMB}}{Jy~sr{-1}}() g() d} $\n\n\nbroken_bandpass_unit_conversion\n$ {CMB}[K_{CMB}] = {CMB}[K_{RJ}] C_{K_{RJ}}^{K_{CMB}}() g_{RJ}() d() = {CMB}[K_{RJ}] C{K_{RJ}}^{K_{CMB}}() d()$\n\nfrom pysm3.utils import normalize_weights\n\n\ndef broken_bandpass_unit_conversion(freqs, weights, output_unit):\n    \"\"\"Unit conversion from uK_RJ to output unit given a bandpass\n\n    Parameters\n    ----------\n    freqs : astropy.units.Quantity\n        Frequency array in a unit compatible with GHz\n    \"\"\"\n    freqs = check_freq_input(freqs)\n    factors = (np.ones(len(freqs), dtype=np.float) * u.uK_RJ).to_value(\n        output_unit, equivalencies=u.cmb_equivalencies(freqs * u.GHz)\n    )\n    if len(freqs) &gt; 1:\n        w = normalize_weights(freqs, weights)\n        factor = np.trapz(factors * w, freqs)\n    else:\n        factor = factors[0]\n    return factor * u.Unit(u.Unit(output_unit) / u.uK_RJ)\n\n\n    nside = 32\n    freqs = np.array([250, 300, 350]) * u.GHz\n    weights = np.ones(len(freqs))\n    sky = pysm3.Sky(nside=nside, preset_strings=[\"c2\"])\n    CMB_rj_int = sky.get_emission(freqs, weights)\n    CMB_thermo_int = CMB_rj_int*fixed_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    expected_map = pysm3.read_map(\n        \"pysm_2/lensed_cmb.fits\", field=(0, 1), nside=nside, unit=u.uK_CMB\n    )\n    for pol in [0, 1]:\n        #assert_quantity_allclose(expected_map[pol], CMB_thermo_int[pol], rtol=1e-4)\n        print(\"Error: {:.2f} %\".format(100-np.median((CMB_thermo_int[pol]/expected_map[pol]).value)*100))\n\nError: -0.00 %\nError: -0.00 %\n\n\n\nfrom pysm3.utils import check_freq_input\n\n\ndef fixed_bandpass_unit_conversion(freqs, weights, output_unit):\n    \"\"\"Unit conversion from uK_RJ to output unit given a bandpass\n    Parameters\n    ----------\n    freqs : astropy.units.Quantity\n        Frequency array in a unit compatible with GHz\n    \"\"\"\n    freqs = check_freq_input(freqs)\n    weights_to_rj = (weights * u.uK_RJ).to_value(\n            (u.Jy / u.sr), equivalencies=u.cmb_equivalencies(freqs * u.GHz)\n        )\n    weights_to_out = (weights * output_unit).to_value(\n            (u.Jy / u.sr), equivalencies=u.cmb_equivalencies(freqs * u.GHz)\n        )\n    if len(freqs) &gt; 1:\n        factor = np.trapz(weights_to_rj, freqs)/np.trapz(weights_to_out, freqs)\n    else:\n        factor = (1. * u.uK_RJ).to_value(\n            output_unit, equivalencies=u.cmb_equivalencies(freqs * u.GHz)\n        )\n    return factor * u.Unit(u.Unit(output_unit) / u.uK_RJ)"
  },
  {
    "objectID": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-20-tophat-bandpasses",
    "href": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-20-tophat-bandpasses",
    "title": "Investigate broken implementation of bandpass unit conversion in PySM 3",
    "section": "Unit conversion error for 20% tophat bandpasses",
    "text": "Unit conversion error for 20% tophat bandpasses\n\nperc_error = {}\nfor center_freq in np.array([10, 20, 40, 70, 100, 150, 200, 250, 270, 300, 350, 400])*u.GHz:\n    freqs = np.linspace(center_freq*.8, center_freq*1.2, 10)\n    weights = np.ones(10)\n    rj_to_cmb = broken_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    fixed_rj_to_cmb = fixed_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    perc_error[center_freq] = (rj_to_cmb - fixed_rj_to_cmb) / fixed_rj_to_cmb * 100\n    print(\"{: &lt;3.0f}, broken: {:.3f}, fixed: {:.3f}, error {:.2f}%\".format(center_freq, rj_to_cmb, fixed_rj_to_cmb, perc_error[center_freq]))\n\n10  GHz, broken: 1.003 uK_CMB / uK_RJ, fixed: 1.003 uK_CMB / uK_RJ, error 0.00%\n20  GHz, broken: 1.011 uK_CMB / uK_RJ, fixed: 1.011 uK_CMB / uK_RJ, error 0.00%\n40  GHz, broken: 1.045 uK_CMB / uK_RJ, fixed: 1.045 uK_CMB / uK_RJ, error 0.01%\n70  GHz, broken: 1.143 uK_CMB / uK_RJ, fixed: 1.142 uK_CMB / uK_RJ, error 0.08%\n100 GHz, broken: 1.310 uK_CMB / uK_RJ, fixed: 1.306 uK_CMB / uK_RJ, error 0.32%\n150 GHz, broken: 1.808 uK_CMB / uK_RJ, fixed: 1.782 uK_CMB / uK_RJ, error 1.47%\n200 GHz, broken: 2.770 uK_CMB / uK_RJ, fixed: 2.661 uK_CMB / uK_RJ, error 4.06%\n250 GHz, broken: 4.627 uK_CMB / uK_RJ, fixed: 4.260 uK_CMB / uK_RJ, error 8.62%\n270 GHz, broken: 5.800 uK_CMB / uK_RJ, fixed: 5.221 uK_CMB / uK_RJ, error 11.10%\n300 GHz, broken: 8.297 uK_CMB / uK_RJ, fixed: 7.178 uK_CMB / uK_RJ, error 15.59%\n350 GHz, broken: 15.749 uK_CMB / uK_RJ, fixed: 12.560 uK_CMB / uK_RJ, error 25.39%\n400 GHz, broken: 31.306 uK_CMB / uK_RJ, fixed: 22.588 uK_CMB / uK_RJ, error 38.59%"
  },
  {
    "objectID": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-simons-observatory-channels",
    "href": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-simons-observatory-channels",
    "title": "Investigate broken implementation of bandpass unit conversion in PySM 3",
    "section": "Unit conversion error for Simons Observatory channels",
    "text": "Unit conversion error for Simons Observatory channels\n\nimport h5py\n\n\nSO_chs = h5py.File(\"../mapsims/mapsims/data/simonsobs_instrument_parameters_2020.06.h5\", mode='r')\n\n\nSO_chs[\"LT0_UHF1\"].keys()\n\n&lt;KeysViewHDF5 ['bandpass_frequency_GHz', 'bandpass_weight']&gt;\n\n\n\nSO_chs[\"LT0_UHF1\"].attrs.keys()\n\n&lt;KeysViewHDF5 ['band', 'band_id', 'center_frequency_GHz', 'fwhm_arcmin', 'noise_band_index', 'telescope', 'tube', 'tube_id']&gt;\n\n\n\nperc_error = {}\nfor ch in SO_chs.values():\n    freqs = ch[\"bandpass_frequency_GHz\"] * u.GHz\n    weights = ch[\"bandpass_weight\"]\n    rj_to_cmb = broken_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    fixed_rj_to_cmb = fixed_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    perc_error[center_freq] = (rj_to_cmb - fixed_rj_to_cmb) / fixed_rj_to_cmb * 100\n    print(\"{} {:4s}, {:6.2f} GHz, broken: {:.3f}, fixed: {:.3f}, error {:.2f}%\".format(ch.attrs[\"telescope\"], ch.attrs[\"band\"], ch.attrs[\"center_frequency_GHz\"], rj_to_cmb, fixed_rj_to_cmb, perc_error[center_freq]))\n\nLA UHF1, 225.70 GHz, broken: 3.377 uK_CMB / uK_RJ, fixed: 3.293 uK_CMB / uK_RJ, error 2.54%\nLA UHF2, 285.40 GHz, broken: 6.166 uK_CMB / uK_RJ, fixed: 5.990 uK_CMB / uK_RJ, error 2.93%\nLA UHF1, 225.70 GHz, broken: 3.377 uK_CMB / uK_RJ, fixed: 3.293 uK_CMB / uK_RJ, error 2.54%\nLA UHF2, 285.40 GHz, broken: 6.166 uK_CMB / uK_RJ, fixed: 5.990 uK_CMB / uK_RJ, error 2.93%\nLA MFF1,  92.00 GHz, broken: 1.248 uK_CMB / uK_RJ, fixed: 1.247 uK_CMB / uK_RJ, error 0.12%\nLA MFF2, 147.50 GHz, broken: 1.729 uK_CMB / uK_RJ, fixed: 1.721 uK_CMB / uK_RJ, error 0.49%\nLA MFF1,  92.00 GHz, broken: 1.248 uK_CMB / uK_RJ, fixed: 1.247 uK_CMB / uK_RJ, error 0.12%\nLA MFF2, 147.50 GHz, broken: 1.729 uK_CMB / uK_RJ, fixed: 1.721 uK_CMB / uK_RJ, error 0.49%\nLA MFS1,  88.60 GHz, broken: 1.229 uK_CMB / uK_RJ, fixed: 1.228 uK_CMB / uK_RJ, error 0.11%\nLA MFS2, 146.50 GHz, broken: 1.720 uK_CMB / uK_RJ, fixed: 1.711 uK_CMB / uK_RJ, error 0.54%\nLA MFS1,  88.60 GHz, broken: 1.229 uK_CMB / uK_RJ, fixed: 1.228 uK_CMB / uK_RJ, error 0.11%\nLA MFS2, 146.50 GHz, broken: 1.720 uK_CMB / uK_RJ, fixed: 1.711 uK_CMB / uK_RJ, error 0.54%\nLA LF1 ,  25.70 GHz, broken: 1.018 uK_CMB / uK_RJ, fixed: 1.018 uK_CMB / uK_RJ, error 0.00%\nLA LF2 ,  38.90 GHz, broken: 1.043 uK_CMB / uK_RJ, fixed: 1.043 uK_CMB / uK_RJ, error 0.01%\nSA UHF1, 225.70 GHz, broken: 3.377 uK_CMB / uK_RJ, fixed: 3.293 uK_CMB / uK_RJ, error 2.54%\nSA UHF2, 285.40 GHz, broken: 6.166 uK_CMB / uK_RJ, fixed: 5.990 uK_CMB / uK_RJ, error 2.93%\nSA MFF1,  92.00 GHz, broken: 1.248 uK_CMB / uK_RJ, fixed: 1.247 uK_CMB / uK_RJ, error 0.12%\nSA MFF2, 147.50 GHz, broken: 1.729 uK_CMB / uK_RJ, fixed: 1.721 uK_CMB / uK_RJ, error 0.49%\nSA MFS1,  88.60 GHz, broken: 1.229 uK_CMB / uK_RJ, fixed: 1.228 uK_CMB / uK_RJ, error 0.11%\nSA MFS2, 146.50 GHz, broken: 1.720 uK_CMB / uK_RJ, fixed: 1.711 uK_CMB / uK_RJ, error 0.54%\nSA LF1 ,  25.70 GHz, broken: 1.018 uK_CMB / uK_RJ, fixed: 1.018 uK_CMB / uK_RJ, error 0.00%\nSA LF2 ,  38.90 GHz, broken: 1.043 uK_CMB / uK_RJ, fixed: 1.043 uK_CMB / uK_RJ, error 0.01%"
  },
  {
    "objectID": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-cmb-s4-channels",
    "href": "posts/2020-08-24-debug-bandpass-unit-conversion-pysm3.html#unit-conversion-error-for-cmb-s4-channels",
    "title": "Investigate broken implementation of bandpass unit conversion in PySM 3",
    "section": "Unit conversion error for CMB-S4 channels",
    "text": "Unit conversion error for CMB-S4 channels\n\nS4_chs = h5py.File(\"../s4mapbasedsims/202006_foregrounds_extragalactic_cmb_tophat/cmbs4_tophat.h5\", mode='r')\n\n\nperc_error = {}\nfor ch in S4_chs.values():\n    freqs = ch[\"bandpass_frequency_GHz\"] * u.GHz\n    weights = ch[\"bandpass_weight\"]\n    rj_to_cmb = broken_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    fixed_rj_to_cmb = fixed_bandpass_unit_conversion(\n        freqs, weights, u.uK_CMB\n    )\n    perc_error[center_freq] = (rj_to_cmb - fixed_rj_to_cmb) / fixed_rj_to_cmb * 100\n    print(\"{} {:5s}, {:6.2f} GHz, broken: {:.3f}, fixed: {:.3f}, error {:.2f}%\".format(\n        ch.attrs[\"telescope\"], ch.attrs[\"band\"], ch.attrs[\"center_frequency_GHz\"], rj_to_cmb, fixed_rj_to_cmb, perc_error[center_freq]))\n\nLAT HFL1 , 225.00 GHz, broken: 3.364 uK_CMB / uK_RJ, fixed: 3.275 uK_CMB / uK_RJ, error 2.71%\nLAT HFL2 , 278.00 GHz, broken: 5.632 uK_CMB / uK_RJ, fixed: 5.523 uK_CMB / uK_RJ, error 1.98%\nSAT HFS1 , 220.00 GHz, broken: 3.163 uK_CMB / uK_RJ, fixed: 3.109 uK_CMB / uK_RJ, error 1.71%\nSAT HFS2 , 270.00 GHz, broken: 5.269 uK_CMB / uK_RJ, fixed: 5.099 uK_CMB / uK_RJ, error 3.34%\nLAT LFL1 ,  27.00 GHz, broken: 1.019 uK_CMB / uK_RJ, fixed: 1.019 uK_CMB / uK_RJ, error 0.00%\nLAT LFL2 ,  39.00 GHz, broken: 1.044 uK_CMB / uK_RJ, fixed: 1.044 uK_CMB / uK_RJ, error 0.01%\nSAT LFS1 ,  30.00 GHz, broken: 1.024 uK_CMB / uK_RJ, fixed: 1.024 uK_CMB / uK_RJ, error 0.00%\nSAT LFS2 ,  40.00 GHz, broken: 1.044 uK_CMB / uK_RJ, fixed: 1.044 uK_CMB / uK_RJ, error 0.01%\nSAT MFHS1,  95.00 GHz, broken: 1.263 uK_CMB / uK_RJ, fixed: 1.262 uK_CMB / uK_RJ, error 0.10%\nSAT MFHS2, 155.10 GHz, broken: 1.822 uK_CMB / uK_RJ, fixed: 1.813 uK_CMB / uK_RJ, error 0.51%\nLAT MFL1 ,  93.00 GHz, broken: 1.262 uK_CMB / uK_RJ, fixed: 1.259 uK_CMB / uK_RJ, error 0.22%\nLAT MFL2 , 145.00 GHz, broken: 1.707 uK_CMB / uK_RJ, fixed: 1.697 uK_CMB / uK_RJ, error 0.62%\nSAT MFLS1,  85.00 GHz, broken: 1.207 uK_CMB / uK_RJ, fixed: 1.206 uK_CMB / uK_RJ, error 0.06%\nSAT MFLS2, 145.10 GHz, broken: 1.696 uK_CMB / uK_RJ, fixed: 1.690 uK_CMB / uK_RJ, error 0.40%\nLAT ULFL1,  20.00 GHz, broken: 1.011 uK_CMB / uK_RJ, fixed: 1.011 uK_CMB / uK_RJ, error 0.00%"
  },
  {
    "objectID": "posts/2023-10-27-jupyterhub-github-authentication.html",
    "href": "posts/2023-10-27-jupyterhub-github-authentication.html",
    "title": "Deploy Github Authenticator in JupyterHub",
    "section": "",
    "text": "Updated June 2024: added more options to config file\nQuick reference on how to deploy the Github Authenticator in JupyterHub, the main reference is the Zero to JupyterHub docs.\nFirst create a Oauth app on Github, see under “Settings” and “Developer options”, set your Hub Callback url, see for example the configuration file below.\nSave this configuration file as config_github_auth.yaml following the template available on Github\nSee the comments in the file for other interesting configuration options, and check the OAuthenticator documentation for more.\nAdd the configuration file to the helm upgrade --install call as --values config_github_auth.yaml, you can have multiple --values arguments."
  },
  {
    "objectID": "posts/2020-10-02-example-healpy-query_disc.html",
    "href": "posts/2020-10-02-example-healpy-query_disc.html",
    "title": "Example of the healpy query disc function",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nlonlat=True switches ang2vec from requiring colatitude \\(\\theta\\) and longitude \\(\\phi\\) in radians to longitude and latitude in degrees (notice that also the order changes)\n# in degrees\nlon = 60\nlat = 30\nvec = hp.ang2vec(lon, lat, lonlat=True)\nnside = 256\nlarge_disc = hp.query_disc(nside, vec, radius=np.radians(20))\nsmall_disc = hp.query_disc(nside, vec, radius=np.radians(8))\ntiny_disc = hp.query_disc(nside, vec, radius=np.radians(2))\nquery_disc returns a list of pixels, by default in RING ordering, let’s check their length:\nlist(map(len, [large_disc, small_disc, tiny_disc]))\n\n[23715, 3829, 237]"
  },
  {
    "objectID": "posts/2020-10-02-example-healpy-query_disc.html#create-a-map-and-plot-it-in-mollweide-projection",
    "href": "posts/2020-10-02-example-healpy-query_disc.html#create-a-map-and-plot-it-in-mollweide-projection",
    "title": "Example of the healpy query disc function",
    "section": "Create a map and plot it in Mollweide projection",
    "text": "Create a map and plot it in Mollweide projection\n\nm = np.zeros(hp.nside2npix(nside))\n\n\nm[large_disc] = 1\nm[small_disc] = 2\nm[tiny_disc] = 3\n\n\nhp.mollview(m)\nhp.graticule()\n\n0.0 180.0 -180.0 180.0\nThe interval between parallels is 30 deg -0.00'.\nThe interval between meridians is 30 deg -0.00'."
  },
  {
    "objectID": "posts/2021-03-11-rotate-maps-healpy.html",
    "href": "posts/2021-03-11-rotate-maps-healpy.html",
    "title": "Tutorial on how to rotate maps in healpy",
    "section": "",
    "text": "import healpy as hp\nimport numpy as np\nimport os\nimport astropy.units as u\nimport matplotlib.pyplot as plt\nhp.disable_warnings()\n%matplotlib inline\nIn this notebook we will explore different ways of rotating maps in healpy."
  },
  {
    "objectID": "posts/2021-03-11-rotate-maps-healpy.html#create-an-input-map",
    "href": "posts/2021-03-11-rotate-maps-healpy.html#create-an-input-map",
    "title": "Tutorial on how to rotate maps in healpy",
    "section": "Create an input map",
    "text": "Create an input map\nWe simulate a simple galactic plane like horizontal band and a strong source at the galactic center.\n\nnside = 128\nm = np.zeros(hp.nside2npix(nside))\n\n\ngal_width = 20 * u.deg\n\n\ngalaxy_plane_pixels = hp.query_strip(nside, np.pi/2 - gal_width.to_value(u.radian),\n                                     np.pi/2+ gal_width.to_value(u.radian))\n\n\nm[galaxy_plane_pixels] = 10\n\n\nsource_vector = hp.ang2vec(theta=np.pi/2, phi=0)\n\n\nsource_vector\n\narray([1.000000e+00, 0.000000e+00, 6.123234e-17])\n\n\n\ngalactic_center_pixels = hp.query_disc(nside, source_vector, radius=(5*u.deg).to_value(u.radian))\n\n\nm[galactic_center_pixels] = 50\n\n\nhp.mollview(m)\n\n\n\n\n\n\n\n\nWe do not want to have sharp steps in the map, therefore we smooth it with a 5 degrees beam:\n\nm_smoothed = hp.smoothing(m, fwhm=(5*u.deg).to_value(u.radian))\n\n\nhp.mollview(m_smoothed, title=\"Smoothed map\")\nhp.graticule();"
  },
  {
    "objectID": "posts/2021-03-11-rotate-maps-healpy.html#rotation-in-plotting",
    "href": "posts/2021-03-11-rotate-maps-healpy.html#rotation-in-plotting",
    "title": "Tutorial on how to rotate maps in healpy",
    "section": "Rotation in plotting",
    "text": "Rotation in plotting\nThe most common reason for applying a rotation is when we want to change reference frame, healpy supports Galactic, Ecliptic and Equatorial. If we just need it for plotting, we can use the coord keyword for all plotting functions, for example, “GC” converts the map from Galactic to Equatorial:\n\nhp.mollview(m_smoothed, coord=\"GC\")\nhp.graticule();\n\n\n\n\n\n\n\n\nInstead if we want to specify a custom rotation, we can use the rot keyword. It accepts, longitute and latitude (in this order) in degrees, that point will be the center of the new reference frame. So instead if we want to move the source at the galactic center to Longitude 10 and Latitude 20, see the red dot below:\n\nhp.mollview(m_smoothed)\nhp.projplot(10,20, 'ro', lonlat=True)\nhp.graticule();\n\n\n\n\n\n\n\n\nWe need to provide \\(-10 ^{\\circ}, -20 ^{\\circ}\\) to rot:\n\nhp.mollview(m_smoothed, rot=[-10, -20])\nhp.graticule();\n\n\n\n\n\n\n\n\nWe can also provide a third argument to rot to rotate the orientation of the galactic plane:\n\nhp.mollview(m_smoothed, rot=[-10, -20, 45])\nhp.graticule();\n\n\n\n\n\n\n\n\nThe same works for gnomview, we can center the map at the point specified by rot, then adjust the sky area with xsize and reso\n\nhp.gnomview(m_smoothed, rot=[2, 6], xsize=2000, ysize=800, reso=2)\nhp.graticule();"
  },
  {
    "objectID": "posts/2021-03-11-rotate-maps-healpy.html#rotate-the-actual-array",
    "href": "posts/2021-03-11-rotate-maps-healpy.html#rotate-the-actual-array",
    "title": "Tutorial on how to rotate maps in healpy",
    "section": "Rotate the actual array",
    "text": "Rotate the actual array\nInstead if we need to do further processing to the map and rotate the actual array, we need to use a Rotator, it accepts the same argument we saw above:\n\nrot_gal2eq = hp.Rotator(coord=\"GC\")\n\nLet’s take as an example instead a custom rotator, let’s say that we want to move the source at the galactic center to the coordinates \\(10 ^{\\circ}, 20^{\\circ}\\).\nThe definition of rotation in healpy is that the point provided in rot will be the center of the map.\n\nlongitude = -10 * u.deg\nlatitude = -20 * u.deg\nrot_custom = hp.Rotator(rot=[longitude.to_value(u.deg), latitude.to_value(u.deg)])\n\nand this works fine as expected:\n\nrot_custom(-10, -20, lonlat=True)\n\narray([ 0.00000000e+00, -1.42108547e-14])\n\n\nHowever, this causes a small error for the rotation of \\((0^\\circ,0^\\circ)\\):\n\nrot_custom(0, 0, lonlat=True)\n\narray([10.62758414, 19.68349808])\n\n\nThe correct way of defining this rotation is to use \\((10^\\circ,20^\\circ)\\) and then specify the inv keyword to use the inverse transform:\n\nlongitude = 10 * u.deg\nlatitude = 20 * u.deg\nrot_custom = hp.Rotator(rot=[longitude.to_value(u.deg), latitude.to_value(u.deg)], inv=True)\n\nIt can also be used to rotate vectors:\n\nrot_custom(0, 0, lonlat=True)\n\narray([10., 20.])\n\n\nFinally we can rotate the full map, there are 2 ways, either in spherical harmonics or pixel space. It is generally better to do the rotation in spherical harmonics space, because rotating in pixel space does interpolation so has a smoothing effect on the map, anyway it could be used in particular cases for example when the map is not full-sky.\nThey are both methods of the Rotator object, get the input map and return the rotated map:\n\nm_smoothed_rotated_alms = rot_custom.rotate_map_alms(m_smoothed)\n\n\nm_smoothed_rotated_alms.max()\n\n47.4755560735065\n\n\n\nm_smoothed_rotated_pixel = rot_custom.rotate_map_pixel(m_smoothed)\n\n\nm_smoothed_rotated_pixel.max()\n\n47.38040671208857\n\n\n\nhp.mollview(m_smoothed_rotated_alms, title=\"Map rotated in spherical harmonics space\")\nhp.projplot(10,20, 'ro', lonlat=True)\nhp.graticule();\n\n\n\n\n\n\n\n\n\nhp.mollview(m_smoothed_rotated_pixel, title=\"Map rotated in pixel space\")\nhp.projplot(10,20, 'ro', lonlat=True)\nhp.graticule();"
  },
  {
    "objectID": "posts/2021-03-11-rotate-maps-healpy.html#compare-rotation-in-spherical-harmonics-and-pixel-space",
    "href": "posts/2021-03-11-rotate-maps-healpy.html#compare-rotation-in-spherical-harmonics-and-pixel-space",
    "title": "Tutorial on how to rotate maps in healpy",
    "section": "Compare rotation in spherical harmonics and pixel space",
    "text": "Compare rotation in spherical harmonics and pixel space\nIf we compare the maps, there is significant difference, we can rotate back and check the residuals with the two methods.\n\nhp.mollview(m_smoothed_rotated_alms-m_smoothed_rotated_pixel, title=\"Difference between the 2 rotated maps\")\nhp.graticule();\n\n\n\n\n\n\n\n\n\nrot_custom_back = hp.Rotator(rot=[longitude.to_value(u.deg), latitude.to_value(u.deg)])\n\n\nhp.mollview(m_smoothed-rot_custom_back.rotate_map_alms(m_smoothed_rotated_alms), title=\"residuals after rotating back (Spherical Harmonics)\")\nhp.graticule();\n\n\n\n\n\n\n\n\n\nhp.mollview(m_smoothed-rot_custom_back.rotate_map_pixel(m_smoothed_rotated_pixel), title=\"residuals after rotating back (Pixel space)\")\nhp.graticule();\n\n\n\n\n\n\n\n\nThere is a huge difference between the methods, check the scales of the colorbars, this is made worse by the fact that we are using pixels that are quite big, it wouldn’t be as bad with smaller pixels."
  },
  {
    "objectID": "posts/2021-06-18-healpy-rotate-mask.html",
    "href": "posts/2021-06-18-healpy-rotate-mask.html",
    "title": "Investigate rotation of masks in healpy",
    "section": "",
    "text": "Investigating a question about healpy on Stackoverflow\n\nimport healpy as hp\nimport numpy as np\n%matplotlib inline\n\n\nhp.disable_warnings()\n\n\nnside = 16\nnpix = hp.nside2npix(nside)\n\n\nm = hp.ma(np.arange(npix, dtype=np.float32))\n\n\nmask = np.zeros(npix, dtype=np.bool)\nmask[hp.query_strip(nside, np.radians(75), np.radians(105))] = 1\nmask[hp.query_disc(nside, hp.dir2vec(0,0, lonlat=True), np.radians(40))] = 1\n\n\nm.mask = mask\n\n\nhp.mollview(m);\n\n\n\n\n\n\n\n\n\ngal2eq = hp.Rotator(coord=[\"G\",\"E\"])\n\n\nm_rotated = gal2eq.rotate_map_pixel(m)\n\n\nmask_rotated = gal2eq.rotate_map_pixel(m.mask)\n\n\nhp.mollview(mask_rotated)\n\n\n\n\n\n\n\n\n\nhp.mollview(m_rotated.mask)\n\n\n\n\n\n\n\n\nNow in the first case healpy fills the map with UNSEEN and then interpolation is handled by HEALPix C++. I don’t know how internally HEALPix handles that. In the second case we pass a map of 0 and 1 and HEALPix does the interpolation, but we don’t trigger any special case of handling UNSEEN values.\n\n# consider all values less than 1 masked\nnp.testing.assert_array_equal(m_rotated.mask, mask_rotated == 1)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-13-aa7e79abeb94&gt; in &lt;module&gt;\n      1 # consider all values less than 1 masked\n----&gt; 2 np.testing.assert_array_equal(m_rotated.mask, mask_rotated == 1)\n\n    [... skipping hidden 2 frame]\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 105 / 3072 (3.42%)\n x: array([False, False, False, ..., False, False, False])\n y: array([False, False, False, ..., False, False, False])\n\n\n\n\n# consider only values of 1 masked\nnp.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; 0)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-14-47376e435ed3&gt; in &lt;module&gt;\n      1 # consider only values of 1 masked\n----&gt; 2 np.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; 0)\n\n    [... skipping hidden 2 frame]\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 169 / 3072 (5.5%)\n x: array([False, False, False, ..., False, False, False])\n y: array([False, False, False, ..., False, False, False])\n\n\n\n\n# try a value close to 1\nnp.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; .9)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-15-acaca61476b4&gt; in &lt;module&gt;\n      1 # try a value close to 1\n----&gt; 2 np.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; .9)\n\n    [... skipping hidden 2 frame]\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 26 / 3072 (0.846%)\n x: array([False, False, False, ..., False, False, False])\n y: array([False, False, False, ..., False, False, False])\n\n\n\n\n# try a value close to 1\nnp.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; .999)\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-16-7f299a009890&gt; in &lt;module&gt;\n      1 # try a value close to 1\n----&gt; 2 np.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; .999)\n\n    [... skipping hidden 2 frame]\n\nAssertionError: \nArrays are not equal\n\nMismatched elements: 1 / 3072 (0.0326%)\n x: array([False, False, False, ..., False, False, False])\n y: array([False, False, False, ..., False, False, False])\n\n\n\n\n# try a value close to 1\nnp.testing.assert_array_equal(m_rotated.mask, mask_rotated &gt; .9999)"
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "",
    "text": "In this tutorial we will install Dask Gateway, currently version 2023.9.0, on Kubernetes and configure JupyterHub so Jupyter Notebook users can launch private Dask cluster and connect to them.\nI assume to start from a Kubernetes cluster already running and JupyterHub deployed on top of it via Helm. And SSL encryption also activated (it isn’t probably necessary, but I haven’t tested that). I tested on Jetstream 2, but the recipe should be agnostic of that."
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html#preparation",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html#preparation",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Preparation",
    "text": "Preparation\nClone on the machine you use to run helm and kubectl the repository with the configuration files and scripts:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/"
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html#launch-dask-gateway",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Launch dask gateway",
    "text": "Launch dask gateway\nWe can install version 2023.9.0 with:\n$ bash install_dask-gateway.sh\nYou might want to check config_dask-gateway.yaml for extra configuration options, but for initial setup and testing it shouldn’t be necessary.\nAfter this you should see the 3 dask gateway pods running, e.g.:\n$ kubectl -n jhub get pods\nNAME                                       READY   STATUS    RESTARTS   AGE\napi-dask-gateway-64bf5db96c-4xfd6          1/1     Running   2          23m\ncontroller-dask-gateway-7674bd545d-cwfnx   1/1     Running   0          23m\ntraefik-dask-gateway-5bbd68c5fd-5drm8      1/1     Running   0          23m"
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html#modify-the-jupyterhub-configuration",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Modify the JupyterHub configuration",
    "text": "Modify the JupyterHub configuration\nOnly 2 options need to be changed in JupyterHub:\n\nWe need to run a image which has the same version of dask-gateway we installed on Kubernetes (currently 0.9.0)\n\nIf you are using my install_jhub.sh script to deploy JupyterHub, you can modify it and add another values option at the end, --values dask_gateway/config_jupyterhub.yaml.\nYou can modify the image you are using for Jupyterhub in dask_gateway/config_jupyterhub.yaml.\nTo assure that there are not compatibility issues, the “Client” (JupyterHub session), the dask gateway server, the scheduler and the workers should all have the same version of Python and the same version of dask, distributed and dask_gateway. If this is not possible, you can test different combinations and they might work.\nThen redeploy JupyterHub:\nbash install_jhub.sh && cd dask_gateway && bash install_dask-gateway.sh\nLogin to JupyterHub, get a terminal and first check if the service is working:\n&gt;  curl http://traefik-dask-gateway/services/dask-gateway/api/health\nShould give:\n{\"status\": \"pass\"}\nif curl is not available in your image, you can do the same in a Python Notebook:\nimport requests\nrequests.get(\"http://traefik-dask-gateway/services/dask-gateway/api/health\").content"
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html#identify-the-dask-gateway-address",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html#identify-the-dask-gateway-address",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Identify the dask gateway address",
    "text": "Identify the dask gateway address\nJetstream 2 now supports “Load Balancing as a service”, therefore the dask gateway address gets a public IP that can be accessed from outside, this is very convenient for users viewing the Dask Dashboard.\nFirst let’s get the IP:\nkubectl --namespace=jhub get service traefik-dask-gateway\nNAME                   TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)        AGE\ntraefik-dask-gateway   LoadBalancer   10.233.43.51   149.165.xxx.xxx   80:32752/TCP   36m\nExternal IP is the address to be used in the next section."
  },
  {
    "objectID": "posts/2023-09-28-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "href": "posts/2023-09-28-dask-gateway-jupyterhub.html#create-a-dask-cluster",
    "title": "Deploy Dask Gateway with JupyterHub on Kubernetes",
    "section": "Create a dask cluster",
    "text": "Create a dask cluster\nYou can now login to JupyterHub and check you can connect properly to dask-gateway:\nfrom dask_gateway import Gateway\ngateway = Gateway(\n    address=\"http://xxx.xxx.xxx.xxx\",\n    auth=\"jupyterhub\")\ngateway.list_clusters()\nThen create a cluster and use it:\ncluster = gateway.new_cluster()\ncluster.scale(2)\nclient = cluster.get_client()\nClient is a standard distributed client and all subsequent calls to dask will go through the cluster.\nPrinting the cluster object gives the link to the Dask dashboard.\nFor a full example see this Jupyter Notebook\n(Click on the Raw button to download notebook and upload it to your session)."
  },
  {
    "objectID": "posts/2024-04-04-run-pytorch-gpu-expanse.html",
    "href": "posts/2024-04-04-run-pytorch-gpu-expanse.html",
    "title": "Run PyTorch with GPU support on Expanse",
    "section": "",
    "text": "In this tutorial we will create a Python environment with PyTorch and see how to get Jupyterlab running on a GPU node.\nFirst of all we want to create an isolated Python environment, I generally favor micromamba, see the documentation on how to install it\nOnce installed, create an environment:\nmicromamba create -n pytorch python==3.10 jupyterlab\nDo not install pytorch with Mamba, it won’t recognize the GPU, not sure why.\nInstall pytorch with pip:\nmicromamba activate pytorch\npip install pytorch\nThe tool to launch JupyterLab on Expanse currently doesn’t support mamba, so the easiest way is to activate this environment at login, therefore add:\nmicromamba activate pytorch\nat the end of .bashrc.\nFinally we can launch a job on the GPU-shared partition with Galyleo to get JupyterLab proxied to a public url:\n/cm/shared/apps/sdsc/galyleo/galyleo.sh launch -Q -p gpu-shared -A sds166 -t 120 -c 8 -M 16 -G 1 -j lab\nCheck in a Notebook that pytorch detects the GPU:\nimport torch\ntorch.cuda.is_available()"
  },
  {
    "objectID": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html",
    "href": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html",
    "title": "Migrate from fastpages to quarto preserving git history",
    "section": "",
    "text": "Most of my blog posts are in Markdown…however, there are cases where plotting and code are very important, and nothing beats a Jupyter Notebook for that.\nThis blog was based on fastpages, a static blog generator by fast.ai. Unfortunately they are discontinuing it and they recommended to migrate to Quarto, so here we are.\nHamel Husain wrote a great tutorial on how to migrate from Fastpages to Quarto (and saved me so much work):\nhttps://nbdev.fast.ai/tutorials/blogging.html#migrating-from-fastpages\nFollowing I describe a couple of tips that were useful for my migration."
  },
  {
    "objectID": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#preserve-git-history",
    "href": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#preserve-git-history",
    "title": "Migrate from fastpages to quarto preserving git history",
    "section": "Preserve git history",
    "text": "Preserve git history\nI wanted to keep the commit history of my blog, so I replaced this step:\ncp -r ../blog/_notebooks/* posts\ncp -r ../blog/_posts/* posts\nwith a more complicated procedure based on git filter-branch. git expertise recommended, continue at your own risk.\nI was inspired by this tutorial about git, even if I simplified a bit the procedure.\nAfter we initialize the website/blog based on quarto, we commit it to the main branch of the repository (for the new blog).\n\nIn the same repository we create another branch which points to the old fastpages-powered blog:\n\ngit remote add old git@github.com:you/yourfastpagesrepo.git\ngit fetch old\ngit checkout -b oldposts old/master\n\nThen we only keep the history of the _posts folder\ngit filter-branch –subdirectory-filter _posts – –all\nFinally we move it into a folder\nmkdir oldposts mv * oldposts git add . git commit -m “move posts into folder”\nNow we can merge it back into the main branch\n\ngit checkout main\ngit merge oldposts --allow-unrelated-histories\ngit mv oldposts/* posts/\n\nIf necessary, repeat this step with the _notebooks folder"
  },
  {
    "objectID": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#no-categories-in-url",
    "href": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#no-categories-in-url",
    "title": "Migrate from fastpages to quarto preserving git history",
    "section": "No categories in URL",
    "text": "No categories in URL\nIn my old blog I didn’t have categories in my URL, so the aliases generated by nbdev_migrate were wrong.\nI modified migrate.py in my nbdev install.\nIn _fp_convert, I modified the line about aliases into:\nfm['aliases'] = [f'{fs'}]"
  },
  {
    "objectID": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#screenshots-of-the-old-blog",
    "href": "posts/2022-09-27-migrate-fastpages-quarto-preserve-history.html#screenshots-of-the-old-blog",
    "title": "Migrate from fastpages to quarto preserving git history",
    "section": "Screenshots of the old blog",
    "text": "Screenshots of the old blog\nSometimes I get nostalgic."
  },
  {
    "objectID": "posts/2024-04-17-overleaf-github-action.html",
    "href": "posts/2024-04-17-overleaf-github-action.html",
    "title": "2-way Synchronization of Overleaf and Github via a Github Action",
    "section": "",
    "text": "Updated: 2025-02-20\nIn the past I wrote tutorials on how to handle Github and Overleaf, the latest for example is this. However the Github button in Overleaf is fragile, it often gets disconnected for unknown reasons, and it cannot be reconnected, it forces deletion of the Github repository. Moreover, it seems it is now a Premium feature. Finally, it needs to be triggered manually, and nobody remembers to do that.\nI therefore decided to write a Github Action that can be configured to run automatically every hour to pull changes from Overleaf into Github using the git interface, which instead seems solid.\nThe action also runs on Pull Requests, and in this case runs the rebase operation on Overleaf to check that there are no merge conflicts, but it does not push neither to the local branch nor to Overleaf. Therefore it is useful for contributors to manually rebase on main once in a while to keep the pull request current, it is not strictly necessary, we can wait for the pull request to be merged to handle that. Also notice that the synchronization action can be triggered manually in the “Github Actions” section of the repository.\nWhen the action runs on the Github main branch, if after the rebasing on Overleaf there are any local commits, those commits are also pushed to Overleaf (this is not a force-push, so it will fail is something is not matching, this is intendend behaviour).\nThe Action needs 2 secrets to be configured in the repository, OVERLEAF_PROJECT_ID is in the Overleaf project url https://www.overleaf.com/project/xxxxxxxxxxxxxxxxxxxxxxxx, OVERLEAF_TOKEN can be generated in Account Settings.\nHere is the entire Action:\nname: Overleaf Sync with Git\non:\n  schedule:\n    - cron: \"24 * * * *\"\n  push:\n    branches:\n      - master\n  workflow_dispatch:\n      \njobs:\n  sync-overleaf:\n    name: Synchronize with Overleaf\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n      with:\n          fetch-depth: 0 # need to fetch history to rebase\n    - name: Configure Overleaf remote\n      run: git remote add overleaf https://git:${OVERLEAF_TOKEN}@git.overleaf.com/${OVERLEAF_PROJECT_ID}\n      env:\n        OVERLEAF_PROJECT_ID: ${{ secrets.OVERLEAF_PROJECT_ID }}\n        OVERLEAF_TOKEN: ${{ secrets.OVERLEAF_TOKEN }}\n    - name: Configure git user\n      run: git config user.email \"github-action-overleaf@example.com\" && git config user.name \"Github Actions\"\n    - name: Pull changes from Overleaf\n      run: git pull --rebase overleaf master \n    - name: Push changes to Github and Overleaf (only from master)\n      run: git push --force origin master && git push overleaf HEAD:master\n      if: ${{ github.ref == 'refs/heads/master' }}"
  },
  {
    "objectID": "posts/2023-02-24-nersc-share-files-html.html",
    "href": "posts/2023-02-24-nersc-share-files-html.html",
    "title": "Share data from NERSC publicly via web",
    "section": "",
    "text": "NERSC allows to make data which is on their Community File System publicly available via a web interface.\nThis is handled via “Project folders”, so the requirement is to be part of a “project”, for example most scientists working on Cosmic Microwave Background are part of the cmb project (and of the cmb Unix group at NERSC):\n&gt; groups\nzonca cmb\n\n&gt; export PRJ=cmb\nSo you should be able to create folders on the Community File System, if not already present, also ma\n&gt; mkdir -p /global/project/projectdirs/$PRJ/www\nAlso make sure it files are world-readable, folders are world-readable and world-executable:\n&gt; chmod 755 !$\n&gt; chmod 644 !$/*\nNow any files and any subfolder of www will be available publicly displaying filenames and file sizes and will allow navigation at the address:\n&gt; echo https://portal.nersc.gov/project/$PRJ\nSee for example:\n* [https://portal.nersc.gov/project/cmb/dirlist/](https://portal.nersc.gov/project/cmb/dirlist/)"
  },
  {
    "objectID": "posts/2023-03-16-latex-github-codespaces.html",
    "href": "posts/2023-03-16-latex-github-codespaces.html",
    "title": "Work on a Latex Document in Github Codespaces",
    "section": "",
    "text": "I was exploring using Github Codespaces instead of Overleaf to work on a Latex document.\nThe most important missing component is the Latex editor, and the other downside is that it is free only for 90 hours per month (with a Pro academic account). However, the other useful features of Overleaf are present:\nOn top of that, Codespace is a full machine, so you could run a Python script or a Jupyter Notebook to generate images for example.\nSee a screenshot of the environment"
  },
  {
    "objectID": "posts/2023-03-16-latex-github-codespaces.html#how-to-set-github-codespaces-for-latex",
    "href": "posts/2023-03-16-latex-github-codespaces.html#how-to-set-github-codespaces-for-latex",
    "title": "Work on a Latex Document in Github Codespaces",
    "section": "How to set Github Codespaces for Latex",
    "text": "How to set Github Codespaces for Latex\nOn the repository where you store the Latex project, click on the Code button and open a new Codespace.\nIn the terminal, install the Latex environment:\nsudo apt install texlive texlive-science texlive-latex-extra latexmk\nIn the “Extensions” window, install the extension “Latex Workshop”.\nOpen the “Tex” menu on the left bar to build the project with latexmk and open a tab within VS Code to visualize the PDF."
  },
  {
    "objectID": "posts/2023-04-23-hyper-supreme-cam-cosmology.html",
    "href": "posts/2023-04-23-hyper-supreme-cam-cosmology.html",
    "title": "Cosmology webinar Hyper Supreme Cam",
    "section": "",
    "text": "Webinar about Cosmology results, in particular tension in \\(S_8\\) by the Hyper Supreme Cam collaboration:\nVideo on Youtube"
  },
  {
    "objectID": "posts/2022-10-24-numba-closures.html",
    "href": "posts/2022-10-24-numba-closures.html",
    "title": "Closures in Numba",
    "section": "",
    "text": "!pip install numba\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.56.3)\nRequirement already satisfied: llvmlite&lt;0.40,&gt;=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.39.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba) (4.13.0)\nRequirement already satisfied: numpy&lt;1.24,&gt;=1.18 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\nRequirement already satisfied: typing-extensions&gt;=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;numba) (4.1.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;numba) (3.9.0)\nfrom numba.core import types\nfrom numba.typed import Dict\nfrom numba import njit"
  },
  {
    "objectID": "posts/2022-10-24-numba-closures.html#parameter-is-a-simple-type",
    "href": "posts/2022-10-24-numba-closures.html#parameter-is-a-simple-type",
    "title": "Closures in Numba",
    "section": "Parameter is a simple type",
    "text": "Parameter is a simple type\nIn this case numba supports a closure without any issue, in this case have a function which defines a cut on an array, we can create different versions of this function dinamically.\n\ndef MB_cut_factory(limit):\n    def cut(value):\n        return value &lt; limit\n    return cut\n\n\nMB_cut_factory(4)(3)\n\nTrue\n\n\n\nnjit(MB_cut_factory(4))(3)\n\nTrue"
  },
  {
    "objectID": "posts/2022-10-24-numba-closures.html#parameter-is-a-complex-type",
    "href": "posts/2022-10-24-numba-closures.html#parameter-is-a-complex-type",
    "title": "Closures in Numba",
    "section": "Parameter is a complex type",
    "text": "Parameter is a complex type\nIf the parameter is a complex type, unfortunately numba throws a NotImplementedError:\n\ndict_ranges = Dict.empty(\n    key_type=types.int64,\n    value_type=types.Tuple((types.float64, types.float64))\n    )\n\ndict_ranges[3] = (1, 3)\n\ndef MB_cut_factory(dict_ranges):\n    def cut(series, value):\n        return dict_ranges[series][0] &lt; value &lt; dict_ranges[series][1]\n    return cut\n\nMB_cut_factory(dict_ranges)(3,2)\n\nTrue\n\n\n\nnjit(MB_cut_factory(dict_ranges))(3,2)\n\n\n---------------------------------------------------------------------------\nNumbaNotImplementedError                  Traceback (most recent call last)\n&lt;ipython-input-10-843baa832585&gt; in &lt;module&gt;\n----&gt; 1 njit(MB_cut_factory(dict_ranges))(3,2)\n\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws)\n    466                 e.patch_message(msg)\n    467 \n--&gt; 468             error_rewrite(e, 'typing')\n    469         except errors.UnsupportedError as e:\n    470             # Something unsupported is present in the user code, add help info\n\n/usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in error_rewrite(e, issue_type)\n    407                 raise e\n    408             else:\n--&gt; 409                 raise e.with_traceback(None)\n    410 \n    411         argtypes = []\n\nNumbaNotImplementedError: Failed in nopython mode pipeline (step: native lowering)\n&lt;numba.core.base.OverloadSelector object at 0x7f3e92fe2c90&gt;, (DictType[int64,UniTuple(float64 x 2)]&lt;iv=None&gt;,)\nDuring: lowering \"$2load_deref.0 = freevar(dict_ranges: {3: (1.0, 3.0)})\" at &lt;ipython-input-9-1926a0a30147&gt; (10)"
  },
  {
    "objectID": "posts/2022-10-24-numba-closures.html#the-ugly-workaround",
    "href": "posts/2022-10-24-numba-closures.html#the-ugly-workaround",
    "title": "Closures in Numba",
    "section": "The ugly workaround",
    "text": "The ugly workaround\nUsing exec we can brutally create the function definition injecting the dictionary as a string into the function definition itself.\nIt is ugly but works and gives back a function that can be tested in pure Python before passing it to numba for optimization.\nNotice we need to use globals() in the call to exec to have the cut function available in the namespace.\n\ndef MB_cut_factory(dict_ranges):\n    exec(\"def cut(series, value):\\n    dict_ranges=\" +\\\n         dict_ranges.__str__() +\\\n        \"\\n    return dict_ranges[series][0] &lt; value &lt; dict_ranges[series][1]\", globals())\n    return cut\n\n\nMB_cut_factory(dict_ranges)(3,2)\n\nTrue\n\n\n\nnjit(MB_cut_factory(dict_ranges))(3,2)\n\nTrue"
  },
  {
    "objectID": "posts/2022-10-24-numba-closures.html#questions-on-stackoverflow",
    "href": "posts/2022-10-24-numba-closures.html#questions-on-stackoverflow",
    "title": "Closures in Numba",
    "section": "Questions on Stackoverflow",
    "text": "Questions on Stackoverflow\nTrying to find solutions I posted 2 related questions to Stackoverflow, plase contribute there if you have better suggestions:\n\nnumba and variables defined in a closure\nTransform a partial function into a normal function"
  },
  {
    "objectID": "posts/2023-11-02-desouza-award.html",
    "href": "posts/2023-11-02-desouza-award.html",
    "title": "Received DeSouza award for work on deploying software infrastructure on Jetstream",
    "section": "",
    "text": "I am honored to have received the 2023 DeSouza award from Unidata, a NSF-funded center providing data, software and support for the field of Geoscience. The award celebrates the impact of Jeremy Fischer’s and my work on deploying a software platform based on Kubernetes and JupyterHub (in particular Zero to JupyterHub) on the NSF-funded cloud computing system Jetstream 2.\n\nPage about the award on the Unidata website\nVideo of the seminar on Youtube\nSlides of the seminar"
  },
  {
    "objectID": "posts/2024-05-12-pysm-aso-sdsc.html",
    "href": "posts/2024-05-12-pysm-aso-sdsc.html",
    "title": "News item on SDSC website about Advanced Simons Observatory and PySM",
    "section": "",
    "text": "In collaboration with the SDSC Communications team I prepared a news item for the SDSC Website announcing my participation in the Advanced Simons Observatory project focusing on my work on supporting PySM.\nSee https://www.sdsc.edu/News%20Items/PR20240503_Advanced_Simons_Observatory.html\nNotice that I have published the notebook I used to generate the 6 images."
  },
  {
    "objectID": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html",
    "href": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "",
    "text": "This tutorial is a minor update of https://www.zonca.dev/posts/2020-07-10-nfs-server-kubernetes-jetstream.\nAlso consider that a more robust and low-maintenance way of providing shared data volumes is to rely on Manila shares provided by Jetstream 2, see the tutorial\nIn this tutorial I’ll show how to create a data volume on Jetstream and share it using a NFS server to all JupyterHub users. All JupyterHub users run as the jovyan user, therefore each folder in the shared filesystem can be either read-only, or writable by every user. The main concern is that a user could delete by mistake data of another user, however the users still have access to their own home folder."
  },
  {
    "objectID": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-the-nfs-server",
    "href": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-the-nfs-server",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test the NFS server",
    "text": "Test the NFS server\nEdit test_nfs_mount.yaml to set the right IP for the NFS server, then:\nkubectl create -f test_nfs_mount.yaml\nand access the terminal to test:\nexport N=default #set namespace\nbash ../terminal_pod.sh test-nfs-mount\ndf -h\n\n...\n10.254.204.67:/       9.8G   36M  9.8G   1% /share\n...\nWe have the root user, we can use the terminal to copy or rsync data into the shared volume. We can also create writable folders owned by the user 1000 which maps to jovyan in JupyterHub:\nsh-4.2# mkdir readonly_folder\nsh-4.2# touch readonly_folder/aaa\nsh-4.2# mkdir writable_folder\nsh-4.2# chown 1000:100 writable_folder\nsh-4.2# ls -l /share\ntotal 24\ndrwx------. 2 root root  16384 Jul 10 06:32 lost+found\ndrwxr-xr-x. 2 root root   4096 Jul 10 06:43 readonly_folder\ndrwxr-xr-x. 2 1000 users  4096 Jul 10 06:43 writable_folder"
  },
  {
    "objectID": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#preserve-the-data-volume-across-redeployments",
    "href": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#preserve-the-data-volume-across-redeployments",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Preserve the data volume across redeployments",
    "text": "Preserve the data volume across redeployments\nThe NFS data volume could contain a lot of data that you would want to preserve in case you need to completely tear down the Kubernetes cluster.\nFirst we find out what is the ID of the PersistentVolume associated with the NFS volume:\nkubectl get pv | grep nfs\npvc-ee1f02aa-11f8-433f-806f-186f6d622a30   10Gi       RWO            Delete           Bound    default/nfs-share-folder-claim   standard                5m55s\nThen you can save the PersistentVolume and the PersistentVolumeClaim to YAML:\nkubectl get pvc nfs-share-folder-claim -o yaml &gt; existing_nfs_volume_claim.yaml\nkubectl get pv pvc-ee1f02aa-11f8-433f-806f-186f6d622a30 -o yaml &gt; existing_nfs_volume.yaml\nNext we can delete the servers directly from Openstack, be careful not to delete the PersistentVolume or the PersistentVolumeClaim in Kubernetes or the underlying volume in Openstack will be deleted, also do not delete the namespace associated with those resources.\nFinally redeploy everything, and instead of launching create_nfs_volume.yaml, we create first the PersistentVolume then the PersistentVolumeClaim:\nkubectl create -f existing_nfs_volume.yaml\nkubectl create -f existing_nfs_volume_claim.yaml"
  },
  {
    "objectID": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-in-jupyter",
    "href": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-in-jupyter",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test in Jupyter",
    "text": "Test in Jupyter\nNow connect to JupyterHub and check in a terminal:\njovyan@jupyter-zonca2:/share$ pwd\n/share\njovyan@jupyter-zonca2:/share$ whoami\njovyan\njovyan@jupyter-zonca2:/share$ touch readonly_folder/ccc\ntouch: cannot touch 'readonly_folder/ccc': Permission denied\njovyan@jupyter-zonca2:/share$\njovyan@jupyter-zonca2:/share$ touch writable_folder/ccc\njovyan@jupyter-zonca2:/share$ ls -l writable_folder/\ntotal 0\n-rw-r--r--. 1 jovyan root 0 Jul 10 06:50 ccc"
  },
  {
    "objectID": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-the-ssh-server",
    "href": "posts/2023-02-06-nfs-server-kubernetes-jetstream.html#test-the-ssh-server",
    "title": "Deploy a NFS server to share data between JupyterHub users on Jetstream",
    "section": "Test the SSH server",
    "text": "Test the SSH server\nFrom a machine external to Jetstream:\nssh -i path/to/private/key -p 30022 datacopier@js-xxx-xxx.jetstream-cloud.org\nWelcome to OpenSSH Server\n\nssh-server:~$ whoami\ndatacopier\nssh-server:~$ sudo su\nssh-server:/config# whoami\nroot\nssh-server:/config# cd /share\nssh-server:/share# ls\nlost+found  readonly_folder  writable_folder\nssh-server:/share# touch readonly_folder/moredata\nssh-server:/share#"
  },
  {
    "objectID": "posts/2024-01-12-pdf-from-screenshots.html",
    "href": "posts/2024-01-12-pdf-from-screenshots.html",
    "title": "Make a PDF from a series of screenshots",
    "section": "",
    "text": "You might need to digitize some web content by taking screenshots and then bundling them up in a PDF.\nHere I am using Linux from the command line, leveraging convert from Imagemagick and pdftk.\nScreenshots are usually in png format, we first want to compress them by turning them into PDF:\nfor f in *.png\ndo\n    convert \"$f\" \"$f\".jpg\ndone\nnext we want to convert to PDF, but we need to set the right page size. 1224x792 is the size of 2 sheets in letter format in portrait orientation side by side:\nfor f in *.jpg\ndo\n    convert \"$f\" -density 72 -page 1224x792  \"${f// /_}\".pdf\ndone\nFinally we can concatenate the pdf page to a single document:\npdftk *.pdf cat output document.pdf"
  },
  {
    "objectID": "posts/2024-04-25-whatsapp-poll.html",
    "href": "posts/2024-04-25-whatsapp-poll.html",
    "title": "Automate sending polls via WhatsApp",
    "section": "",
    "text": "Polls in WhatsApp are so convenient for the users, but so painful for group admins. Cannot modify them, cannot re-use or replicate them.\nBest workaround I found is to use WPPConnect/WA-JS\nThis creates a sort of API in the browser console that can be used to just copy-paste from an editor to create a poll.\n\nInstall Tampermonkey in your browser (I tested with Chrome)\nSelect “Create new script”\nPaste from https://github.com/wppconnect-team/wa-js?tab=readme-ov-file#tampermonkey-or-greasemonkey, do not need to write anything where it says “Your code here”\n\nNow after we login to WhatsApp Web and select the target group chat, we can open the “Developer tools”, paste and execute:\nc = WPP.chat.getActiveChat()\nWPP.chat.sendCreatePollMessage(\nc.id,\n'Title of the poll',\n['9am', '9:30am', '10am', '2pm', '2:30pm', '3pm', 'No', 'Maybe']\n);\nThis is very convenient especially if you send many polls that have mostly the same options.\nYou can also generate options dynamically in Javascript:\nc = WPP.chat.getActiveChat()\nam = ['9am', '10am', '11am']\npm = ['2pm', '2:30pm', '3pm']\nday = ['Sat', 'Sun']\nother = ['No', 'Maybe']\n\noptions = []\nfor (i = 0; i &lt; day.length; i++) {\n    if (day[i] == 'Sun') {\n        for (j = 0; j &lt; am.length; j++) {\n            options.push(day[i] + ' ' + am[j])\n        }\n    }\n    for (j = 0; j &lt; pm.length; j++) {\n        options.push(day[i] + ' ' + pm[j])\n    }\n}\noptions = options.concat(other)\n\nif (options.length &gt; 12) {\n    throw new Error('Number of options exceeds the limit of 12.');\n} else {\n    WPP.chat.sendCreatePollMessage(\n        c.id,\n        'Event title',\n        options\n    );\n}"
  },
  {
    "objectID": "posts/2023-10-27-buy-us-reasury-bonds-fidelity.html",
    "href": "posts/2023-10-27-buy-us-reasury-bonds-fidelity.html",
    "title": "Buy US Treasury bonds on Fidelity",
    "section": "",
    "text": "The Fidelity interface is complicated, here a description of the steps necessary to buy newly issued US Treasury bonds at the next available auction. Currently auctions are every Thursday.\n\nLogin to fidelity\nClick on “News & Research”, “Fixed income, Bonds and CDs”\nClick on “New Issues”, expand “Treasury”\nLook if any of the Treasury bonds have a high “Expected yield” and a suitable duration\nIf not, wait a week and try again\nIf yes, click on “Trade”\nSelect the account, in the quantity put how many thousand dollars you would like to buy. So 3 means $3000. Set AutoRoll to No.\nWait a few days until the auction is held and you will get an email from Fidelity notifying you what is the final yield.\nWait until maturity to get the full amount of interest, however, if you need to sell beforehand, Fidelity allows to sell them most probably without a loss, just a lower interest rate.\n\n\nNote on ROTH IRA\nIf you buy these bonds on a ROTH IRA, the interest is tax free and can be invested in the future."
  },
  {
    "objectID": "posts/2022-11-03-singularity-github-actions.html",
    "href": "posts/2022-11-03-singularity-github-actions.html",
    "title": "Build and host Singularity containers on Github",
    "section": "",
    "text": "I present a demo repository configured to build Singularity containers using Github actions and then host them using the Github container registry:\n\nhttps://github.com/zonca/singularity_github_ci/\n\nSee how the Github action workflow is configured, based on the Singularity Builder GitHub CI.\nSee the logs of a Ubuntu container build process.\nOnce the container is built, it is shown in the “Packages” section of the Github repository, it can then be pulled locally with:\nsingularity pull oras://ghcr.io/zonca/singularity_github_ci:ubuntu-20.04"
  },
  {
    "objectID": "posts/2022-11-08-python-nersc-conda.html",
    "href": "posts/2022-11-08-python-nersc-conda.html",
    "title": "Setup a conda environment at NERSC",
    "section": "",
    "text": "Obsolete: see the new version\nNERSC recommends to use the “Global Common” to store Conda environments, because it is optimized to store lots of small files. Consider that it is mounted read-only on computing nodes. The filesystem is organized by groups, so choose one of your groups:\ngroups\nfor example for me it is the cmb group.\nGROUP=cmb\nmkdir -p /global/common/software/$GROUP/$USER/conda\ncd ~\nln -s /global/common/software/$GROUP/$USER c\nSo we can access it quickly under ~/c.\nThen we create a Conda environment with mamba, specifying the version of python and other packages:\nexport ENV=pycmb\nmodule load python3 # it should load the latest Anaconda\nmamba create --prefix /global/common/software/$GROUP/$USER/conda/$ENV python==3.10 numpy astropy matplotlib ipykernel numba  pytest toml cython scipy namaster -c conda-forge\nWe can also set that path for conda to automatically search into, this will pickup also future Conda environments on the same path:\nconda config --append envs_dirs /global/common/software/$GROUP/$USER/conda\nWe do not want that long path in our prompt, so:\nconda config --set env_prompt '({name}) '\nSo we can activate the environment specifying only the name:\nconda activate $ENV\nIn order to use it also on Jupyter@NERSC you will need to register the kernel:\nipython kernel install --name $ENV --user\nTip for CMB people, make sure you build healpy from source to get the best performance on Spherical Harmonics Transforms:\nCC=gcc CXX=g++ CFLAGS=\"-fPIC -O3 -march=native\" CXXFLAGS=\"-fPIC -O3 -march=native\" pip3 install --user --no-binary healpy --ignore-installed healpy"
  },
  {
    "objectID": "posts/2023-01-23-kubernetes-gpu-jetstream2.html",
    "href": "posts/2023-01-23-kubernetes-gpu-jetstream2.html",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "",
    "text": "This tutorial is obsolete, see the new version of the tutorial.\nThe Jetstream 2 cloud includes 90 GPU nodes with 4 NVIDIA A100 each. If we want to leverage the GPUs inside Kubernetes pods, for example JupyterHub users, we both need to have a GPU-enabled ContainerD runtime and a compatible Docker image based off NVIDIA images."
  },
  {
    "objectID": "posts/2023-01-23-kubernetes-gpu-jetstream2.html#deploy-kubernetes-with-nvidia-runtime",
    "href": "posts/2023-01-23-kubernetes-gpu-jetstream2.html#deploy-kubernetes-with-nvidia-runtime",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Deploy Kubernetes with NVIDIA runtime",
    "text": "Deploy Kubernetes with NVIDIA runtime\nKubespray has built-in support for NVIDIA runtime, therefore it is just a matter of following the standard Kubespray deployment tutorial, but using the branch_v2.18.0_gpu branch of the zonca/jetstream_kubespray Github repository instead of the branch_v2.18.0 branch. You can also check in detail what are the changes required by looking at this Pull Request\nNext we need to install the k8s-device-plugin, at the moment it is just necessary to execute:\nkubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.13.0/nvidia-device-plugin.yml\nHowever, make sure you check the latest k8s-device-plugin documentation.\nFor testing, you can run a simple GPU job:\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: gpu-pod\nspec:\n  restartPolicy: Never\n  containers:\n    - name: cuda-container\n      image: nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2\n      resources:\n        limits:\n          nvidia.com/gpu: 1 # requesting 1 GPU\n  tolerations:\n  - key: nvidia.com/gpu\n    operator: Exists\n    effect: NoSchedule\nEOF\nand check the logs:\nkubectl logs gpu-pod\nThe output should be:\n[Vector addition of 50000 elements]\nCopy input data from the host memory to the CUDA device\nCUDA kernel launch with 196 blocks of 256 threads\nCopy output data from the CUDA device to the host memory\nTest PASSED\nDone"
  },
  {
    "objectID": "posts/2023-01-23-kubernetes-gpu-jetstream2.html#access-gpus-from-jupyterhub",
    "href": "posts/2023-01-23-kubernetes-gpu-jetstream2.html#access-gpus-from-jupyterhub",
    "title": "Deploy Kubernetes on Jetstream 2 with GPU support",
    "section": "Access GPUs from JupyterHub",
    "text": "Access GPUs from JupyterHub\nA Docker image derived from the NVIDIA Tensorflow image is available on DockerHub as zonca/nvidia-tensorflow-jupyterhub, the relevant Dockerfile is available on Github.\nAlso notice that this is configured to run JupyterHub 3.0.0 which should be used in conjunction with the Zero to JupyterHub Helm chart version 2.0.0.\nThen it is just a matter of modifying the install_jhub.sh script to pickup the additional configuration file by adding:\n--values gpu/jupyterhub_gpu.yaml\nFor testing, I have modified the Tensorflow tutorial for beginners to run on GPU, it is available in this Gist.\nYou can download it to your local machine and upload it to the GPU-enabled single user instance on Jetstream.\nDuring execution, the 3rd cell should show the available GPU device:\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nthen the Notebook should execute to completion with no errors, printing for each operation the device which executed it, i.e. the GPU."
  },
  {
    "objectID": "posts/2023-08-16-python-for-hpc-tutorial.html",
    "href": "posts/2023-08-16-python-for-hpc-tutorial.html",
    "title": "Tutorial on Python for HPC at the SDSC Summer Institute",
    "section": "",
    "text": "Each August the San Diego Supercomputer Center organizes a week-long summer school that teaches a large number of topics related to Supercomputing and AI to early career scientists.\nFor the past ~10 year I have been teaching “Python for HPC”, this year tutorial is mostly based on numba and dask.\nSee the “Summer Institute” Github repository for all the notebooks I have used:\n\nhttps://github.com/sdsc/sdsc-summer-institute-2023/tree/main/4.2a_python_for_hpc\n\nNotice another couple of interesting resources:\n\nthe Manifest to build the Singularity image used for the tutorial on the Expanse supercomputer\nscripts to launch Dask on Expanse\nThe Singularity container is available to all Expanse users at /expanse/lustre/projects/sds166/zonca/dask-numba-si23.sif"
  },
  {
    "objectID": "posts/2024-04-30-flatcar-image-jetstream.html",
    "href": "posts/2024-04-30-flatcar-image-jetstream.html",
    "title": "Flatcar Container Linux on Jetstream",
    "section": "",
    "text": "Virtual Machine images provided by the Jetstream team are fully fledged and therefore quite large.\nJulien Chastang proposed the shift to a minimalist distribution to save disk space, resources and shorten deployment time, see the relevant issue discussion on the Jetstream Gitlab organization.\nWe are mostly interested in replacing Ubuntu as the default image for our Kubespray developments, so we are looking only at OS supported by Kubespray"
  },
  {
    "objectID": "posts/2024-04-30-flatcar-image-jetstream.html#flatcar-container-linux",
    "href": "posts/2024-04-30-flatcar-image-jetstream.html#flatcar-container-linux",
    "title": "Flatcar Container Linux on Jetstream",
    "section": "Flatcar Container Linux",
    "text": "Flatcar Container Linux\nBest features:\n\nMinimalist, image is only 450 MB\nOS specifically for running containers\nAutomatic unsupervised updates\n\nSee their docs about deploying on Openstack, uploading to Jetstream worked out of the box using this script.\nThe name of the image is FlatcarContainerLinux-3815-2-2, it is set as a Community image:\nopenstack image list --community | grep Flatcar\n| 3c4b5192-3dbc-4478-8340-7409da188669 | FlatcarContainerLinux-3815-2-2                   | active |\n\nExosphere\nThe instance boots correctly on Exosphere, I can ssh into the instance as exouser, however, on the Dashboard the image is stuck in the “Building” phase and a passphrase is not assigned to the user. Trying to run Docker commands fails for permission issues and without password cannot sudo.\n\n\nHorizon\nThe instance boots fine on Horizon as well, I can ssh with the user core using the keypair injected by Openstack. core by default can execute Docker commands, so can test with:\ndocker run --rm -p 80:80 -d nginx\nand curl localhost.\nOccupies minimal space:\ndf -h\nFilesystem      Size  Used Avail Use% Mounted on\ndevtmpfs        4.0M     0  4.0M   0% /dev\ntmpfs           1.5G     0  1.5G   0% /dev/shm\ntmpfs           596M  556K  595M   1% /run\n/dev/sda9        17G  215M   16G   2% /\nsysext          1.5G   12K  1.5G   1% /usr\noverlay          17G  215M   16G   2% /etc\n/dev/sda6       128M  940K  123M   1% /oem\ntmpfs           1.5G     0  1.5G   0% /media\ntmpfs           1.5G     0  1.5G   0% /tmp\n/dev/sda1       127M   59M   68M  47% /boot\ntmpfs           298M     0  298M   0% /run/user/500\nThe update machinery executes without errors:\ncore@flatcarhorizon ~ $ update_engine_client -update\nI0501 05:10:49.327685  1763 update_engine_client.cc:251] Initiating update check and install.\nI0501 05:10:49.330926  1763 update_engine_client.cc:256] Waiting for update to complete.\nLAST_CHECKED_TIME=1714540249\nPROGRESS=0.000000\nCURRENT_OP=UPDATE_STATUS_IDLE\nNEW_VERSION=0.0.0\nNEW_SIZE=0\nI0501 05:10:54.688055  1763 update_engine_client.cc:194] No update available\nBoth in Horizon and Exosphere I get a warning in dmesg, this is fine:\n[   54.197042] BTRFS warning: duplicate device /dev/sda3 devid 1 generation 33 scanned by (udev-worker) (1659)\n\n\nKubespray\nThe real objective of this test is to deploy Kubernetes on Jetstream using Flatcar, following the latest tutorial with minor modifications.\nWe can replace Ubuntu with Flatcar on Terraform by modifying in cluster.tfvars:\nimage = \"FlatcarContainerLinux-3815-2-2\"\nssh_user = \"core\"\nThen in Ansible, following the Kubespray docs, we need to set:\nbin_dir: /opt/bin\nat the top of group_vars/all/all.yml.\nDeployment time of 1 master node and 2 workers 21 minutes.\nDetailed timing from Ansible:\nkubernetes-apps/ansible : Kubernetes Apps | Lay Down CoreDNS templates -------------------------------- 33.72s\nkubernetes-apps/ingress_controller/ingress_nginx : NGINX Ingress Controller | Create manifests -------- 33.61s\nkubernetes/kubeadm : Join to cluster ------------------------------------------------------------------ 27.87s\nkubernetes-apps/csi_driver/cinder : Cinder CSI Driver | Generate Manifests ---------------------------- 23.79s\nkubernetes-apps/ansible : Kubernetes Apps | Start Resources ------------------------------------------- 22.23s\nkubernetes-apps/ingress_controller/ingress_nginx : NGINX Ingress Controller | Apply manifests --------- 16.56s\nkubernetes-apps/external_cloud_controller/openstack : External OpenStack Cloud Controller | Generate Manifests -- 13.77s\nkubernetes-apps/csi_driver/cinder : Cinder CSI Driver | Apply Manifests ------------------------------- 12.62s\nkubernetes/control-plane : kubeadm | Initialize first master ------------------------------------------ 10.90s\nkubernetes-apps/csi_driver/csi_crd : CSI CRD | Generate Manifests ------------------------------------- 10.34s\nkubernetes-apps/ansible : Kubernetes Apps | Lay Down nodelocaldns Template ---------------------------- 10.23s\ndownload : download_container | Download image if required --------------------------------------------- 9.86s\nkubernetes/preinstall : Ensure kube-bench parameters are set ------------------------------------------- 9.70s\netcd : reload etcd ------------------------------------------------------------------------------------- 8.40s\nkubernetes/preinstall : Create kubernetes directories -------------------------------------------------- 8.25s\netcd : Check certs | Register ca and etcd admin/member certs on etcd hosts ----------------------------- 8.14s\netcd : Check certs | Register ca and etcd admin/member certs on etcd hosts ----------------------------- 7.86s\nkubernetes-apps/snapshots/snapshot-controller : Snapshot Controller | Generate Manifests --------------- 6.93s\nnetwork_plugin/flannel : Flannel | Create Flannel manifests -------------------------------------------- 6.91s\nkubernetes/control-plane : Renew K8S control plane certificates monthly 1/2 ---------------------------- 6.77s\nNodes running happily with Flatcar:\nk get nodes -o wide\nNAME                       STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP       OS-IMAGE                                             KERNEL-VERSION   CONTAINER-RUNTIME\nkubejetstream-1            Ready    control-plane   35m   v1.25.6   10.1.90.173   149.xxx.xxx.xxx   Flatcar Container Linux by Kinvolk 3815.2.2 (Oklo)   6.1.85-flatcar   containerd://1.6.15\nkubejetstream-k8s-node-1   Ready    &lt;none&gt;          33m   v1.25.6   10.1.90.183   149.xxx.xxx.xxx   Flatcar Container Linux by Kinvolk 3815.2.2 (Oklo)   6.1.85-flatcar   containerd://1.6.15\nkubejetstream-k8s-node-2   Ready    &lt;none&gt;          33m   v1.25.6   10.1.90.19    149.xxx.xxx.xxx   Flatcar Container Linux by Kinvolk 3815.2.2 (Oklo)   6.1.85-flatcar   containerd://1.6.15\nAlso tested deploying successfully JupyterHub, enjoy!"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html",
    "href": "posts/2024-10-31-jetstream-llm-chat.html",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "",
    "text": "In this tutorial we will deploy a LLM Chat-GPT like service on a GPU node on Jetstream. For experimentation purposes we are using the smaller and cheapest GPU node available on Jetstream, the g3.small which has a virtual GPU with 8GB of memory, and deploy the meta-llama/Llama-3.2-1B-Instruct model which is a 1.3B parameter model.\nHowever the same instructions can be used to deploy any other model available on the Hugging Face model hub.\nThis tutorial is based on work by Tijmen de Haan, the author of Cosmosage."
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#choose-a-model",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#choose-a-model",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Choose a model",
    "text": "Choose a model\nJetstream has GPU nodes with 4 NVIDIA A100 GPUs, a user can create a Virtual Machine with 1 entire GPU or a fraction of it.\nThe most important requirement is the GPU memory available to load the model parameters, Jetstream provides:\n\n\n\nInstance Type\nGPU Memory (GB)\n\n\n\n\ng3.small\n8\n\n\ng3.medium\n10\n\n\ng3.large\n20\n\n\ng3.xl\n40\n\n\n\nSo g3.xl is the largest available and gets an entire A100 GPU with 40GB of memory.\nTherefore we need to make sure that the model we want to deploy fits in the available memory.\nLlama 3.2 1B model uses about 6.5GB of memory, so it fits in the g3.small instance.\nWe also need to make sure the model has been fine-tuned for responding to text prompts, generally those models are marked as Instruct."
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#create-a-jetstream-instance",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#create-a-jetstream-instance",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Create a Jetstream instance",
    "text": "Create a Jetstream instance\nLogin to Exosphere, request a Ubuntu 24 g3.small instance, name it chat and ssh into it using either the SSH key or the passphrase generated by Exosphere."
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#install-miniforge",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#install-miniforge",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Install miniforge",
    "text": "Install miniforge\nWe will use Miniforge to create 2 separate Python environments, one for the Hugging Face model serving and one for the web interface.\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#configure-vllm-to-serve-the-model",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#configure-vllm-to-serve-the-model",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Configure vllm to serve the model",
    "text": "Configure vllm to serve the model\nCreate the environment:\nconda create -y -n vllm python=3.11\nconda activate vllm\npip install vllm\nAs we are using a Llama model, we need specific authorization, you can login to Hugging Face and request access to the model on the model page.\nNext we can serve the model:\nhuggingface-cli login\nvllm serve \"meta-llama/Llama-3.2-1B-Instruct\" --max-model-len=8192\nIf this starts with no error, we can kill it with Ctrl-C and create a service for it.\nCreate a file /etc/systemd/system/vllm.service with the following content:\n[Unit]\nDescription=VLLM model serving\nAfter=network.target\n\n[Service]\nUser=exouser\nGroup=exouser\nWorkingDirectory=/home/exouser\n\n# Activating the conda environment and starting the service\nExecStart=/bin/bash -c \"source /home/exouser/miniforge3/etc/profile.d/conda.sh && conda activate vllm && vllm serve 'meta-llama/Llama-3.2-1B-Instruct' --max-model-len=8192 --enforce-eager\"\nRestart=always\nEnvironment=PATH=/home/exouser/miniforge3/envs/llm/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n[Install]\nWantedBy=multi-user.target\nThen enable and start the service:\nsudo systemctl enable vllm\nsudo systemctl start vllm\nIn case of errors:\n\nCheck the logs with sudo journalctl -u vllm\nCheck the status with sudo systemctl status vllm\n\nYou can also check the GPU usage with nvidia-smi:\nThu Oct 31 16:51:09 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  GRID A100X-8C                  On  | 00000000:04:00.0 Off |                    0 |\n| N/A   N/A    P0              N/A /  N/A |   6400MiB /  8192MiB |      0%      Default |\n|                                         |                      |             Disabled |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A     53332      C   ...miniforge3/envs/vllm/bin/python3.11     6399MiB |\n+---------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#configure-the-chat-interface",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#configure-the-chat-interface",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Configure the chat interface",
    "text": "Configure the chat interface\nThe chat interface is provided by Open Web UI.\nCreate the environment:\nconda create -y -n open-webui python=3.11\nconda activate open-webui\npip install open-webui\nopen-webui serve\nIf this starts with no error, we can kill it with Ctrl-C and create a service for it.\nCreate a file /etc/systemd/system/webui.service with the following content:\n[Unit]\nDescription=Open Web UI serving\nAfter=network.target\n\n[Service]\nUser=exouser\nGroup=exouser\nWorkingDirectory=/home/exouser\n\n# Activating the conda environment and starting the service\nExecStart=/bin/bash -c \"source /home/exouser/miniforge3/etc/profile.d/conda.sh && conda activate open-webui && open-webui serve\"\nRestart=always\nEnvironment=PATH=/home/exouser/miniforge3/envs/open-webui/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n[Install]\nWantedBy=multi-user.target\nThen enable and start the service:\nsudo systemctl enable webui\nsudo systemctl start webui"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#configure-web-server-for-https",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#configure-web-server-for-https",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Configure web server for HTTPS",
    "text": "Configure web server for HTTPS\nFinally we can use Caddy to serve the web interface with HTTPS.\nFollow the instructions to Install Caddy\nModify the Caddyfile to serve the web interface:\nsudo vim /etc/caddy/Caddyfile\nto:\nchat.xxx000000.projects.jetstream-cloud.org {\n\n        reverse_proxy localhost:8080\n}\nWhere xxx000000 is the allocation code of your Jetstream instance.\nThen reload Caddy:\nsudo systemctl reload caddy"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#connect-the-model-and-test-the-chat-interface",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#connect-the-model-and-test-the-chat-interface",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Connect the model and test the chat interface",
    "text": "Connect the model and test the chat interface\nPoint your browser to https://chat.xxx000000.projects.jetstream-cloud.org and you should see the chat interface.\nCreate an account, click on the profile icon on the top right and enter the “Admin panel” section, open “Settings” then “Connections”.\nUnder “OpenAI API” enter the URL http://localhost:8000/v1 and leave the API key empty.\nClick on the “Verify connection” button, then to “Save” on the bottom.\nFinally you can start chatting with the model!"
  },
  {
    "objectID": "posts/2024-10-31-jetstream-llm-chat.html#use-a-larger-model-using-quantization",
    "href": "posts/2024-10-31-jetstream-llm-chat.html#use-a-larger-model-using-quantization",
    "title": "Deploy a LLM Chat-GPT like service on Jetstream",
    "section": "Use a larger model using quantization",
    "text": "Use a larger model using quantization\nThe weights of LLMs can be quantized to a lower precision to reduce the GPU memory required to run them, often larger models with quantization outperform smaller models with no quantization. Hugging Face has several quantized models, the most popular are GGUF models, but vllm has just experimental support for that format, so better search explicitely for a model “quantized for vllm”, for example hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4.\nsudo systemctl stop vllm\nvllm serve \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\" --max_model_len 4096 --gpu_memory_utilization 1 --enforce-eager\nModify the systemd service replacing the relevant line with:\nExecStart=/bin/bash -c \"source /home/exouser/miniforge3/etc/profile.d/conda.sh && conda activate vllm && vllm serve 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4' --max-model-len=4096 --gpu_memory_utilization 1 --enforce-eager\"\nThen restart the service:\nsudo systemctl daemon-reload\nsudo systemctl start vllm\nCheck nvidia-smi, memory consumption should be about 7.5 GB.\nUnfortunately we needed to also decrease max-model-len to fit in such a small GPU, so the model will only support 4096 tokens, so it would be best to deploy this model on a slightly larger Virtual Machine and increase the number of tokens."
  },
  {
    "objectID": "posts/2024-05-02-pysm-point-source-pixell.html",
    "href": "posts/2024-05-02-pysm-point-source-pixell.html",
    "title": "Generate point source maps with pixell",
    "section": "",
    "text": "Testing the pixell sim_objects functionality to create maps of point sources pre-smoothed with a gaussian beam. The purpose is to include this functionality in PySM to be able to generate on the fly maps of source starting from a catalog.\n\nfrom pixell import enmap, utils, resample, curvedsky as cs, reproject, pointsrcs\nimport numpy as np\nimport healpy as hp\n\n\nfwhm = 5 * utils.degree\n\n\nshape, wcs = enmap.fullsky_geometry(res=fwhm / 3, proj=\"car\")\n\n\nshape\n\n(109, 216)\n\n\n\ndef fwhm2sigma(fwhm):\n    return fwhm / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n\n\ndef flux2amp(flux, fwhm):\n    sigma = fwhm2sigma(fwhm)\n    return flux / (2 * np.pi * sigma**2)\n\n\nassert flux2amp((2 * np.pi * fwhm2sigma(5) ** 2), 5) == 1\n\n\nn_sources = 1\nflux_sources = np.arange(n_sources) + 10\n\n\namplitude_sources = flux2amp(flux_sources, fwhm)\n\n\nsource_pos = np.array([[0], [np.pi / 3]])\n\n\nr, p = pointsrcs.expand_beam(fwhm2sigma(fwhm))\n\n\nsource_map = pointsrcs.sim_objects(shape, wcs, source_pos, amplitude_sources, ((r, p)))\n\n\nimport matplotlib.pyplot as plt\n\n\nsource_pos\n\narray([[0.        ],\n       [1.04719755]])\n\n\n\nplt.imshow(source_map)\n\n\n\n\n\n\n\n\n\nsource_map.argmax(unit=\"coord\")\n\narray([0.        , 1.04719755])\n\n\n\nsource_map.argmax(unit=\"pix\")\n\narray([54, 72])\n\n\n\nsource_pos[:, -1]\n\narray([0.        , 1.04719755])\n\n\n\n-source_map.argmax(unit=\"coord\") + source_pos[:, -1]\n\narray([ 0.00000000e+00, -6.66133815e-16])\n\n\n\nsource_map.max()\n\narray(1158.8864, dtype=float32)\n\n\n\nsource_map.min()\n\narray(0., dtype=float32)\n\n\n\ndef aperture_photometry(\n    thumbs, aperture_radius, annulus_width=None, modrmap=None, pixsizemap=None\n):\n    \"\"\"\n    Flux from aperture photometry.\n\n    from https://github.com/msyriac/orphics/blob/master/orphics/maps.py\n\n    Parameters\n    ----------\n    thumb : ndmap\n        An (...,Ny,Nx) ndmap (i.e. a pixell enmap) containing the thumbnails.\n    aperture_radius : float\n        Aperture inner radius in radians\n    annulus_width : float\n        Annulus width for mean subtraction in radians.\n        Defaults to sqrt(2)-1 times the aperture inner radius.\n    modrmap : ndmap, optional\n        An (Ny,Nx) ndmap containing distances of each pixel from the center in radians.\n    modrmap : ndmap, optional\n        An (Ny,Nx) ndmap containing pixel areas in steradians.\n\n    Returns\n    -------\n    flux : ndarray\n        (...,) array of aperture photometry fluxes.\n\n    \"\"\"\n    if modrmap is None:\n        modrmap = thumbs.modrmap()\n    if annulus_width is None:\n        annulus_width = (np.sqrt(2.0) - 1.0) * aperture_radius\n    # Get the mean background level from the annulus\n    mean = thumbs[\n        ...,\n        np.logical_and(\n            modrmap &gt; aperture_radius, modrmap &lt; (aperture_radius + annulus_width)\n        ),\n    ].mean()\n    if pixsizemap is None:\n        pixsizemap = thumbs.pixsizemap()\n    # Subtract the mean, multiply by pixel areas and sum\n    return (((thumbs - mean) * pixsizemap)[..., modrmap &lt;= aperture_radius]).sum(\n        axis=-1\n    )\n\n\nfrom astropy import units as u\n\n\nbox_half_size_rad = 2 * fwhm\nbox_center = [source_pos[0, -1], source_pos[1, -1]]\nbox = np.array(\n    [\n        [box_center[0] - box_half_size_rad, box_center[1] - box_half_size_rad],\n        [box_center[0] + box_half_size_rad, box_center[1] + box_half_size_rad],\n    ]\n)  # in radians\n\n\nbox_center\n\n[0.0, 1.0471975511965976]\n\n\n\ncutout = source_map.submap(box)\n\n\ncutout.max()\n\narray(1158.8864, dtype=float32)\n\n\n\ncutout.min()\n\narray(0., dtype=float32)\n\n\n\nplt.imshow(cutout)\n\n\n\n\n\n\n\n\n\naperture_photometry(cutout, 2 * fwhm)\n\n9.99300220192394"
  },
  {
    "objectID": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html",
    "href": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html",
    "title": "How to use Anki flashcards to help kids learn spelling",
    "section": "",
    "text": "Go to the Anki website and download the Anki desktop app for Windows, Mac, or Linux.\nInstall the app on your computer.\nOpen the app, click “Add” to create a new profile with the name of the child who will be using the app.\nUnder “Tools”, “Addons”, “Get Addons”, paste the code 111623432 to install the HyperTTS addon for text-to-speech support.\nRestart Anki to enable the addon.\nUnder “Tools”, “HyperTTS Service Configuration”, click “Enable” on “GoogleTranslate” (Free)\nBack to the initial Anki page, click on “Create Deck” to create a new deck for your child’s spelling words, call it “Spelling”.\nIf you want to sync the progress across devices, for example have the kids use a phone or tablet to learn, go to Ankiweb to create an online account, then click “Sync” in the Anki app to log in with the same credentials."
  },
  {
    "objectID": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#install-anki-and-initial-setup",
    "href": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#install-anki-and-initial-setup",
    "title": "How to use Anki flashcards to help kids learn spelling",
    "section": "",
    "text": "Go to the Anki website and download the Anki desktop app for Windows, Mac, or Linux.\nInstall the app on your computer.\nOpen the app, click “Add” to create a new profile with the name of the child who will be using the app.\nUnder “Tools”, “Addons”, “Get Addons”, paste the code 111623432 to install the HyperTTS addon for text-to-speech support.\nRestart Anki to enable the addon.\nUnder “Tools”, “HyperTTS Service Configuration”, click “Enable” on “GoogleTranslate” (Free)\nBack to the initial Anki page, click on “Create Deck” to create a new deck for your child’s spelling words, call it “Spelling”.\nIf you want to sync the progress across devices, for example have the kids use a phone or tablet to learn, go to Ankiweb to create an online account, then click “Sync” in the Anki app to log in with the same credentials."
  },
  {
    "objectID": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#add-spelling-words-to-anki",
    "href": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#add-spelling-words-to-anki",
    "title": "How to use Anki flashcards to help kids learn spelling",
    "section": "Add spelling words to Anki",
    "text": "Add spelling words to Anki\n\nIn the Anki Desktop app on your computer, from the main screen, click on the “Create Deck” button to create a new deck, name it “Spelling::Week1” or similar. This is going to create a subdeck, so that the kid can either study week by week or can study all the words at once.\nClick on the deck you just created, then click on “Add” to add a new card.\nIn the “Front” field, just type a filling character, for example “a”, then press “Tab” to move to the “Back” field. It will be overwritten.\nIn the “Back” field, type the spelling word, for example “apple”.\nClick “Add” to save the card (or press “Ctrl + Enter”). Add all the words.\nNext click on “Browse” to see all the cards you just added. You can edit them if needed.\nSelect all the words (Edit -&gt; Select All)\nClick on the “HyperTTS” menu, click on “Add audio…(Collection)”\nConfigure “HyperTTS” (just on the first execution, then Anki will remember the settings):\n\nSource field: Back\nTarget field: Front, sound tag only, remove other sound tags\nVoice: GoogleTranslate English (US)\n\nClick “Apply to notes” to add the audio to all the cards, this will take a few minutes.\nIf you need to sync across devices, click on “Sync” to upload the deck to the account Ankiweb."
  },
  {
    "objectID": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#study-spelling-words-with-anki",
    "href": "posts/2024-09-03-anki-flashcards-learn-spelling-kids.html#study-spelling-words-with-anki",
    "title": "How to use Anki flashcards to help kids learn spelling",
    "section": "Study spelling words with Anki",
    "text": "Study spelling words with Anki\nNow the kid can study the spelling words using the Anki desktop app or the AnkiWeb website or the AnkiDroid Android or IOS app (the IOS app is paid, all other options are free).\n\nOpen the Anki app\nIf they are not using the desktop app, click on “Sych” to download any new deck created under their account with the desktop app.\nSelect the deck you want to study.\nClick on “Study Now” to start studying the cards.\nThe kid will see a blank page and hear the work spoken by the text-to-speech engine, they can then try to spell the word out loud.\nClick on “Show Answer” to see the word spelled out\nClick on “Again”, “Good”, or “Easy” to rate how well they remembered the word. They should use “Easy” sparingly, only for words they can spell without hesitation.\n\nAnki will show the cards again based on the rating, so that the kid can review the words they have trouble with more often. With the standard settings, they should study their deck once a day.\nIf they want to study all cards again before a test, once they have completed they can click on “Custom Study” and select “Review ahead”, specify 20 days, and click “Study now”."
  },
  {
    "objectID": "posts/2024-09-11-python-for-hpc-tutorial.html",
    "href": "posts/2024-09-11-python-for-hpc-tutorial.html",
    "title": "Tutorial on Python for HPC at the SDSC Summer Institute 2024",
    "section": "",
    "text": "Each August the San Diego Supercomputer Center organizes a week-long summer school that teaches a large number of topics related to Supercomputing and AI to early career scientists.\nFor the past ~10 year I have been teaching “Python for HPC”, this year tutorial is mostly based on numba and dask.\nSee the “Summer Institute” Github repository for all the notebooks I have used:\n\nhttps://github.com/sdsc/sdsc-summer-institute-2024/tree/main/4.2a_python_for_hpc\n\nNotice another couple of interesting resources:\n\nthe Manifest to build the Singularity image used for the tutorial on the Expanse supercomputer\nscripts to launch Dask on Expanse\nThe Singularity container is available to all Expanse users at /expanse/lustre/projects/sds166/zonca/dask-numba-si24.sif"
  },
  {
    "objectID": "posts/2025-01-22-github-release-artifacts.html",
    "href": "posts/2025-01-22-github-release-artifacts.html",
    "title": "Using GitHub Releases to keep track of Large Artifacts",
    "section": "",
    "text": "GitHub Releases provide a way to attach large artifacts, such as binaries or large autogenerated files, without polluting the repository. A release points to a specific commit in a repository and can include files up to 2GB in size.\nFor example, if we are using the code of a branch to generate a large file, we can attach this file to a release. This way, the repository remains clean and the large file is easily accessible."
  },
  {
    "objectID": "posts/2025-01-22-github-release-artifacts.html#introduction",
    "href": "posts/2025-01-22-github-release-artifacts.html#introduction",
    "title": "Using GitHub Releases to keep track of Large Artifacts",
    "section": "",
    "text": "GitHub Releases provide a way to attach large artifacts, such as binaries or large autogenerated files, without polluting the repository. A release points to a specific commit in a repository and can include files up to 2GB in size.\nFor example, if we are using the code of a branch to generate a large file, we can attach this file to a release. This way, the repository remains clean and the large file is easily accessible."
  },
  {
    "objectID": "posts/2025-01-22-github-release-artifacts.html#creating-a-release",
    "href": "posts/2025-01-22-github-release-artifacts.html#creating-a-release",
    "title": "Using GitHub Releases to keep track of Large Artifacts",
    "section": "Creating a Release",
    "text": "Creating a Release\nThe simplest way to create a release is through the GitHub website:\n\nNavigate to the main page of your repository.\nUnder your repository name, click Releases.\nClick Draft a new release.\nClick Choose a tag, create a new tag which is a good reference for the artifact, best would be a version number using Semantic Versioning, as Target, choose the branch whose tip was used to generate the file, it can be a branch of a currently open pull request (in this case see the “Important considerations” below), and click Create tag.\nFill out the release title and description.\nTo include binaries or other large files, drag and drop them into the release form.\nClick Publish release.\nIt is possible to edit the release later to replace or add files."
  },
  {
    "objectID": "posts/2025-01-22-github-release-artifacts.html#important-considerations",
    "href": "posts/2025-01-22-github-release-artifacts.html#important-considerations",
    "title": "Using GitHub Releases to keep track of Large Artifacts",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nTagging Commits in PRs: If you put a tag on a commit that is in a pull request and then the pull request is rebased, the release will become orphaned. This is still acceptable if you just need a centralized spot to store an artifact without adding it to the repository."
  },
  {
    "objectID": "posts/2025-03-26-jetstream-usage-monitoring.html",
    "href": "posts/2025-03-26-jetstream-usage-monitoring.html",
    "title": "Jetstream2 Usage Monitoring",
    "section": "",
    "text": "Monitoring resource usage on cloud platforms is essential for managing allocations effectively. For users of Jetstream2, the Unidata team has developed a useful script to track Service Unit (SU) consumption over time."
  },
  {
    "objectID": "posts/2025-03-26-jetstream-usage-monitoring.html#introduction",
    "href": "posts/2025-03-26-jetstream-usage-monitoring.html#introduction",
    "title": "Jetstream2 Usage Monitoring",
    "section": "",
    "text": "Monitoring resource usage on cloud platforms is essential for managing allocations effectively. For users of Jetstream2, the Unidata team has developed a useful script to track Service Unit (SU) consumption over time."
  },
  {
    "objectID": "posts/2025-03-26-jetstream-usage-monitoring.html#jetstream2-usage-monitoring-script",
    "href": "posts/2025-03-26-jetstream-usage-monitoring.html#jetstream2-usage-monitoring-script",
    "title": "Jetstream2 Usage Monitoring",
    "section": "Jetstream2 Usage Monitoring Script",
    "text": "Jetstream2 Usage Monitoring Script\nThe script is available in the Unidata Science Gateway repository and provides several useful features for Jetstream2 users.\n\nFeatures\nThis Python script allows you to:\n\nGenerate an OpenStack token for authentication\nUse this token to query the Jetstream2 Accounting API\nParse the output to get your current amount of used SUs\nSave usage data to a CSV file for record-keeping\nCreate a plot visualizing your usage over time\nPerform a simple linear analysis to determine your usage rate and make predictions about future SU consumption\n\n\n\nHow It Works\nThe script interfaces with the Jetstream2 Accounting API to retrieve allocation and usage data. It then processes this data to provide insights into how quickly you’re consuming your allocation, helping you plan and manage your resources more effectively.\n\n\nGetting Started\nTo use this tool, you’ll need to:\n\nClone the repository\nInstall the required dependencies\nConfigure your OpenStack credentials\nRun the script to begin monitoring your usage\n\nThis is particularly useful for projects with fixed allocations that need to track and predict their resource consumption over time."
  },
  {
    "objectID": "posts/2025-03-26-jetstream-usage-monitoring.html#conclusion",
    "href": "posts/2025-03-26-jetstream-usage-monitoring.html#conclusion",
    "title": "Jetstream2 Usage Monitoring",
    "section": "Conclusion",
    "text": "Conclusion\nFor researchers and developers using Jetstream2, this monitoring tool provides a simple yet effective way to keep track of SU usage, helping to avoid unexpected allocation exhaustion and enabling better resource planning."
  },
  {
    "objectID": "posts/2025-01-29-github-auth-browser-device-flow.html",
    "href": "posts/2025-01-29-github-auth-browser-device-flow.html",
    "title": "Authenticate to GitHub in the Browser with the Device Flow",
    "section": "",
    "text": "Authenticating to GitHub in the browser using the device flow is essential for applications that cannot directly handle OAuth redirects. This method allows users to authenticate by entering a code on GitHub’s website, making it ideal for devices with limited input capabilities or for applications running in environments where traditional OAuth flows are not feasible.\nGitHub does not allow direct calls to the device flow from the browser for security reasons. The device flow requires a server process to handle the communication with GitHub’s API securely. This server process acts as an intermediary, managing the device code generation, polling for user authentication, and securely storing the access token. By deploying a server process, we ensure that sensitive information is not exposed to the client-side, maintaining the integrity and security of the authentication flow."
  },
  {
    "objectID": "posts/2025-01-29-github-auth-browser-device-flow.html#introduction",
    "href": "posts/2025-01-29-github-auth-browser-device-flow.html#introduction",
    "title": "Authenticate to GitHub in the Browser with the Device Flow",
    "section": "",
    "text": "Authenticating to GitHub in the browser using the device flow is essential for applications that cannot directly handle OAuth redirects. This method allows users to authenticate by entering a code on GitHub’s website, making it ideal for devices with limited input capabilities or for applications running in environments where traditional OAuth flows are not feasible.\nGitHub does not allow direct calls to the device flow from the browser for security reasons. The device flow requires a server process to handle the communication with GitHub’s API securely. This server process acts as an intermediary, managing the device code generation, polling for user authentication, and securely storing the access token. By deploying a server process, we ensure that sensitive information is not exposed to the client-side, maintaining the integrity and security of the authentication flow."
  },
  {
    "objectID": "posts/2025-01-29-github-auth-browser-device-flow.html#how-it-works",
    "href": "posts/2025-01-29-github-auth-browser-device-flow.html#how-it-works",
    "title": "Authenticate to GitHub in the Browser with the Device Flow",
    "section": "How It Works",
    "text": "How It Works\nThe device flow works by generating a device code that the user enters on GitHub’s website. The steps are as follows:\n\nThe client requests a device code from GitHub.\nGitHub responds with a device code and a user code.\nThe user navigates to GitHub’s device activation page and enters the user code.\nThe client polls GitHub to check if the user has completed the authentication.\nOnce authenticated, GitHub provides an access token."
  },
  {
    "objectID": "posts/2025-01-29-github-auth-browser-device-flow.html#deploying-on-render.com",
    "href": "posts/2025-01-29-github-auth-browser-device-flow.html#deploying-on-render.com",
    "title": "Authenticate to GitHub in the Browser with the Device Flow",
    "section": "Deploying on Render.com",
    "text": "Deploying on Render.com\nTo deploy the server code on Render.com, follow these steps:\n\nCreate a Github OAuth App\n\nGo to GitHub Developer Settings\nClick on New OAuth App\nFill in the details:\n\nApplication name: github-auth-proxy-render\nHomepage URL: https://github.com\nAuthorization callback URL: https://localhost\n\nMake sure you enable the Device flow in the OAuth Apps settings.\nGet the Client ID, we do not need a secret for the device flow.\n\n\n\nServer Deployment\n\nFork the repository github_auth_proxy_render.\nEdit the file server.js and replace the CLIENT_ID with the one you got from the GitHub OAuth App.\nCreate a new web service on Render.com.\nConnect your forked repository to the new web service.\nDeploy the service.\nKeep the server logs open for debugging purposes"
  },
  {
    "objectID": "posts/2025-01-29-github-auth-browser-device-flow.html#test-with-the-client",
    "href": "posts/2025-01-29-github-auth-browser-device-flow.html#test-with-the-client",
    "title": "Authenticate to GitHub in the Browser with the Device Flow",
    "section": "Test with the Client",
    "text": "Test with the Client\n\nEdit the file github_device_flow_auth.html and replace the urls with the ones from your own Render.com service.\nOpen the file with your browser\nClick on the button to start the device flow.\nFollow the instructions on the page.\nIf everything works as expected, you should see all of your used data dumped to the page\n\n\n\n\nGitHub Device Flow Screenshot"
  },
  {
    "objectID": "posts/2025-03-30-healpy-1.18.1.html",
    "href": "posts/2025-03-30-healpy-1.18.1.html",
    "title": "healpy 1.18.1 released",
    "section": "",
    "text": "Just released healpy 1.18.1, this version includes important bug fixes, please test and open issues at https://github.com/healpy/healpy/issues\nMajor improvements in this release:\n\nFixed overflow error in map2alm_lsq when working with nside 8192 maps\nFixed incompatibility of map2alm_lsq with scipy 1.15\nAll other changes at https://github.com/healpy/healpy/blob/main/CHANGELOG.rst\n\nBinary packages are available on PyPI and conda-forge for Linux and MacOS.\nAs usual, remember to cite our JOSS paper https://joss.theoj.org/papers/10.21105/joss.01298"
  },
  {
    "objectID": "posts/2025-01-29-jupyterlite-save-github.html",
    "href": "posts/2025-01-29-jupyterlite-save-github.html",
    "title": "Save Jupyterlite Notebooks to GitHub",
    "section": "",
    "text": "JupyterLite runs entirely inside the browser, which means it doesn’t have access to the underlying operating system. Its storage is persistent across restarts of the machine but could be deleted if the browser cache is cleared.\nIn this tutorial we demonstrate how to authenticate to Github in the browser interactively without pre-generating a token and how to save a snapshot of a notebook to a GitHub repository.\nTo authenticate with GitHub, we use a device auth flow, the same used by the gh CLI, i.e. you get a code, paste that into Github."
  },
  {
    "objectID": "posts/2025-01-29-jupyterlite-save-github.html#introduction",
    "href": "posts/2025-01-29-jupyterlite-save-github.html#introduction",
    "title": "Save Jupyterlite Notebooks to GitHub",
    "section": "",
    "text": "JupyterLite runs entirely inside the browser, which means it doesn’t have access to the underlying operating system. Its storage is persistent across restarts of the machine but could be deleted if the browser cache is cleared.\nIn this tutorial we demonstrate how to authenticate to Github in the browser interactively without pre-generating a token and how to save a snapshot of a notebook to a GitHub repository.\nTo authenticate with GitHub, we use a device auth flow, the same used by the gh CLI, i.e. you get a code, paste that into Github."
  },
  {
    "objectID": "posts/2025-01-29-jupyterlite-save-github.html#device-auth-flow",
    "href": "posts/2025-01-29-jupyterlite-save-github.html#device-auth-flow",
    "title": "Save Jupyterlite Notebooks to GitHub",
    "section": "Device Auth Flow",
    "text": "Device Auth Flow\nDue to security restrictions in the browser, we need to have a service deployed that submits requests to the GitHub API. See the tutorial Authenticate to GitHub in the Browser with the Device Flow on how to deploy that on Render.com."
  },
  {
    "objectID": "posts/2025-01-29-jupyterlite-save-github.html#steps-to-authenticate-and-save-notebooks",
    "href": "posts/2025-01-29-jupyterlite-save-github.html#steps-to-authenticate-and-save-notebooks",
    "title": "Save Jupyterlite Notebooks to GitHub",
    "section": "Steps to Authenticate and Save Notebooks",
    "text": "Steps to Authenticate and Save Notebooks\n\nGo to JupyterLab.\nUpload the 3 files github_auth_py.ipynb, github_uploader.py, test_save_notebook.ipynb from the repository https://github.com/zonca/github_auth_proxy_render/tree/main/python\nExecute github_auth_py.ipynb. This notebook will perform the device auth flow and store a token.\nNext, execute test_save_notebook.ipynb. This notebook demonstrates how to execute a notebook and then save it into a GitHub repository with a timestamp, ensuring a snapshot of the notebook is permanently stored on GitHub. This is using the Github API directly to upload a file to a repository.\n\nI also tested using PyGithub, pygit2 and dulwich, but none of them worked in JupyterLite."
  },
  {
    "objectID": "posts/2025-01-29-jupyterlite-save-github.html#conclusion",
    "href": "posts/2025-01-29-jupyterlite-save-github.html#conclusion",
    "title": "Save Jupyterlite Notebooks to GitHub",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a prototype to explore the capabilities and a starting point for thinking about strategies to deal with the ephemeral storage provided by JupyterLite."
  },
  {
    "objectID": "posts/2025-04-15-release-of-pysm-3.4.1.html",
    "href": "posts/2025-04-15-release-of-pysm-3.4.1.html",
    "title": "PySM 3.4.1 Released",
    "section": "",
    "text": "PySM version 3.4.1 is now available. It can be downloaded from PyPI. The release notes detail new features and improvements.\nTwo new models, rg2 and rg3, have been added:\n\nrg2: Websky radio galaxy model applying a beam on the fly to brightest sources.\nrg3: Pre-computed background sources for radio galaxies using catalog data.\n\nAdditionally, see these docs for a comparison of the old rg1 model with these new models, including plots."
  },
  {
    "objectID": "posts/2024-05-20-interactive-3d-plot-planck-map.html",
    "href": "posts/2024-05-20-interactive-3d-plot-planck-map.html",
    "title": "Interactive 3D plot of a Planck map with Matplotlib",
    "section": "",
    "text": "This is an update of my older tutorial\nUnfortunately I was not able to make Mayavi properly plot the data, rerunning the same code would give a map that looked like uniform noise.\nThis test instead uses matplotlib, unfortunately this is extremely slow, and the output map is really slow to rotate or interact at all. Anyway it is possible to tweak xsize,ysize to change the resolution as a trade-off between resolution and interactivity.\n\n\n\n\nScreenshot of the 3D plot with matplotlib"
  },
  {
    "objectID": "posts/2024-05-20-HEALPix-in-Megapixels.html",
    "href": "posts/2024-05-20-HEALPix-in-Megapixels.html",
    "title": "HEALPix in Megapixels",
    "section": "",
    "text": "How many megapixels is a HEALPix map?\n\n!pip install healpy\n\nCollecting healpy\n  Downloading healpy-1.16.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 16.1 MB/s eta 0:00:00\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from healpy) (3.7.1)\nRequirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.10/dist-packages (from healpy) (1.25.2)\nRequirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (from healpy) (5.3.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from healpy) (1.11.4)\nRequirement already satisfied: pyerfa&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;healpy) (2.0.1.4)\nRequirement already satisfied: PyYAML&gt;=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;healpy) (6.0.1)\nRequirement already satisfied: packaging&gt;=19.0 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;healpy) (24.0)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (1.4.5)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;healpy) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;healpy) (1.16.0)\nInstalling collected packages: healpy\nSuccessfully installed healpy-1.16.6\n\n\n\nimport healpy as hp\n\n\nmegapixels = []\nfor nside_power in range(8, 15):\n  nside = 2**nside_power\n  megapixels.append((nside, hp.nside2npix(nside)/1e6))\n\n\nmegapixels\n\n[(256, 0.786432),\n (512, 3.145728),\n (1024, 12.582912),\n (2048, 50.331648),\n (4096, 201.326592),\n (8192, 805.306368),\n (16384, 3221.225472)]"
  },
  {
    "objectID": "posts/2025-03-26-binderhub-pythia.html",
    "href": "posts/2025-03-26-binderhub-pythia.html",
    "title": "Deployment of BinderHub by Project Pythia on Jetstream 2",
    "section": "",
    "text": "Project Pythia has recently published a blog post about their deployment of BinderHub on Jetstream 2. This blog post discusses Project Pythia’s efforts to address “Jupyter notebook obsolescence” by creating reproducible computational environments for geoscience education and workflows. The team deployed a BinderHub on NSF-funded Jetstream2 cloud infrastructure using OpenStack and Kubernetes, enabling scalable resource allocation for workshops and large datasets. By leveraging Terraform and cloud-agnostic infrastructure tools, they streamlined deployment while navigating limitations in OpenStack Magnum. The project highlights the importance of sustainable, community-driven solutions for open geoscience collaboration.\nYou can read their detailed blog post here: Project Pythia BinderHub Deployment"
  },
  {
    "objectID": "posts/2025-03-26-binderhub-pythia.html#project-pythias-binderhub-deployment",
    "href": "posts/2025-03-26-binderhub-pythia.html#project-pythias-binderhub-deployment",
    "title": "Deployment of BinderHub by Project Pythia on Jetstream 2",
    "section": "",
    "text": "Project Pythia has recently published a blog post about their deployment of BinderHub on Jetstream 2. This blog post discusses Project Pythia’s efforts to address “Jupyter notebook obsolescence” by creating reproducible computational environments for geoscience education and workflows. The team deployed a BinderHub on NSF-funded Jetstream2 cloud infrastructure using OpenStack and Kubernetes, enabling scalable resource allocation for workshops and large datasets. By leveraging Terraform and cloud-agnostic infrastructure tools, they streamlined deployment while navigating limitations in OpenStack Magnum. The project highlights the importance of sustainable, community-driven solutions for open geoscience collaboration.\nYou can read their detailed blog post here: Project Pythia BinderHub Deployment"
  },
  {
    "objectID": "posts/2025-03-26-binderhub-pythia.html#my-contribution",
    "href": "posts/2025-03-26-binderhub-pythia.html#my-contribution",
    "title": "Deployment of BinderHub by Project Pythia on Jetstream 2",
    "section": "My Contribution",
    "text": "My Contribution\nI am happy to have participated in this effort by doing preliminary work on the deployment of Kubernetes and BinderHub on Jetstream 2. My earlier tutorial on Deploying BinderHub on top of Kubernetes on Jetstream 2 provided some of the groundwork that helped make Project Pythia’s deployment possible.\nIt’s exciting to see the continued development and implementation of these tools to support the scientific community."
  },
  {
    "objectID": "posts/2024-06-03-web-server-https-django-jetstream2.html",
    "href": "posts/2024-06-03-web-server-https-django-jetstream2.html",
    "title": "Deploy a web server with automatic HTTPS for static website and Django app on Jestream 2",
    "section": "",
    "text": "As part of my work to support the Jetstream 2 Openstack deployment at Indiana University, I have developed a tutorial that explains how to create a Virtual Machine that deploys the Caddy web server.\nCaddy is extremely easy to use compared to Apache or NGINX.\nIn the tutorial I show both how to deploy some static pages and to deploy a toy application in Python Django.\nSee the tutorial in the official Jetstream 2 documentation website."
  },
  {
    "objectID": "posts/2024-06-27-fastapi-conditional-auth.html",
    "href": "posts/2024-06-27-fastapi-conditional-auth.html",
    "title": "Conditional authentication on a single endpoint with FastAPI",
    "section": "",
    "text": "Building on my previous tutorial, I have now implemented a toy FastAPI application which requires authentication based on query parameters.\nIn this simple example /protected/0 does not require authentication, while /protected/1 gives permissions denied.\nOnce the users logs in successfully via Github using /auth/login and is member of the right Github organization, they can now access also the /protected/1 endpoint.\nSee the implementation in this Gist"
  },
  {
    "objectID": "posts/2024-07-17-run-healpy-browser-pyodide.html",
    "href": "posts/2024-07-17-run-healpy-browser-pyodide.html",
    "title": "Run (part of) healpy in the browser with pyodide",
    "section": "",
    "text": "Thanks to the help of @VeerioSDSC and Rick Wagner, I have been able to run part of healpy in the browser. For now only tested reading FITS maps and plotting with projview.\nCompiling the C++ and Cython extensions was difficult so for experimental purposes we opted for stripping healpy of all extensions making it a pure Python package, see https://github.com/healpy/pyhealpy\nThen it is possible to build a wheel of healpy compiled to Javascript with pyodide and emscripten, see: https://github.com/healpy/pyhealpy/blob/pyhealpy/README.md\nOnce the wheel is available it is possible to load it in a web page and use it, see the page source at:\nindex.html for plotting a fits map in the browser\nSee the generated website, open the Developer console to see the logs.\nDetails on how to use Github Actions to build and deploy."
  },
  {
    "objectID": "posts/2024-07-17-run-healpy-browser-pyodide.html#jupyterlite",
    "href": "posts/2024-07-17-run-healpy-browser-pyodide.html#jupyterlite",
    "title": "Run (part of) healpy in the browser with pyodide",
    "section": "JupyterLite",
    "text": "JupyterLite\nWe can also run JupyterLite in the browser and import this package, see a preliminary test at:\nhttps://github.com/healpy/pyhealpy/tree/pyhealpy/jupyterlite\nit is deployed at https://healpy.github.io/pyhealpy/jup/lab/index.html\nThis allows to run JupyterLite, load healpy, read maps loaded into JupyterLite with healpy, it is also possible to use jupyterlab-open-url-parameter to retrieve a map at runtime. However there are several limitations:\n\nloading maps via fromUrl is very slow and works only for tiny maps, even a 30 MB map is too large and makes JupyterLite hang for minutes\npyhealpy only supports maps in ring ordering, maps in nest ordering require ring2nest functionality which is not imported from the C++ package."
  },
  {
    "objectID": "posts/2024-08-28-dataportal-data-pysm.html",
    "href": "posts/2024-08-28-dataportal-data-pysm.html",
    "title": "Generate sample Cosmology data for Cheap and FAIR data portal with PySM",
    "section": "",
    "text": "Generate figures for PySM\nThis notebook generates some figures of Galactic and Extra-Galactic emissions using PySM. Mostly for displaying purposes.\nThis notebook is designed to work on Google Colab, remove the apt lines if executing locally but make sure you have a Latex environment.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n!ln -s /content/drive/MyDrive/dataportal_demo_data data\n\n\n# Install Latex to render labels\n!apt install texlive texlive-latex-extra texlive-fonts-recommended cm-super-minimal dvipng\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ncm-super-minimal is already the newest version (0.3.4-17).\ndvipng is already the newest version (1.15-1.1).\ntexlive is already the newest version (2021.20220204-1).\ntexlive-fonts-recommended is already the newest version (2021.20220204-1).\ntexlive-latex-extra is already the newest version (2021.20220204-1).\n0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n\n\n\n%pip install pysm3\n\nRequirement already satisfied: pysm3 in /usr/local/lib/python3.10/dist-packages (3.4.0)\nRequirement already satisfied: healpy&gt;=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pysm3) (1.17.3)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from pysm3) (0.60.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pysm3) (0.10.2)\nRequirement already satisfied: astropy in /usr/local/lib/python3.10/dist-packages (from pysm3) (6.1.2)\nRequirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.10/dist-packages (from healpy&gt;=1.16.0-&gt;pysm3) (1.26.4)\nRequirement already satisfied: pyerfa&gt;=2.0.1.1 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;pysm3) (2.0.1.4)\nRequirement already satisfied: astropy-iers-data&gt;=0.2024.7.1.0.34.3 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;pysm3) (0.2024.8.26.0.31.57)\nRequirement already satisfied: PyYAML&gt;=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;pysm3) (6.0.2)\nRequirement already satisfied: packaging&gt;=19.0 in /usr/local/lib/python3.10/dist-packages (from astropy-&gt;pysm3) (24.1)\nRequirement already satisfied: llvmlite&lt;0.44,&gt;=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba-&gt;pysm3) (0.43.0)\n\n\n\nimport pysm3\nfrom pysm3 import units as u\nimport healpy as hp\nimport numpy as np\n\n\nsky = pysm3.Sky(nside=128, preset_strings=[\"c3\"], output_unit=u.uK_CMB)\n\n\ncmb = sky.get_emission(100 * u.GHz)\n\n\nimport matplotlib.pyplot as plt\nplt.rcParams['text.usetex'] = True\n\n\nfreqs = [23, 100, 143, 353, 545, 857] * u.GHz\n\n\nfontsize={\"title\":30, \"cbar_label\":20, \"cbar_tick_label\":20}\n\n\n!rm -rf data/cmb\n!mkdir -p data/cmb\n\n\nimport astropy.units\n\n\nfor freq in freqs:\n  m = sky.get_emission(freq)\n  hp.projview(m[0].value, min=-250, max=250,\n            fontsize=fontsize,\n            unit=astropy.units.format.Latex.to_string(m.unit), title=f\"Cosmic Microwave Background {int(freq.value)} {freq.unit}\");\n  filename = f\"data/cmb/cmb_{int(freq.value):03d}{freq.unit}\"\n  plt.savefig(f\"{filename}.jpg\", bbox_inches=\"tight\")\n  hp.write_map(f\"{filename}.fits\", m, dtype=np.float32, overwrite=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%ls -lah cmb/\n\ntotal 4.0M\ndrwxr-xr-x 2 root root 4.0K Aug 28 21:02 ./\ndrwxr-xr-x 1 root root 4.0K Aug 28 21:05 ../\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_023GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_023GHz.jpg\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_100GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_100GHz.jpg\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_143GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_143GHz.jpg\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_353GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_353GHz.jpg\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_545GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_545GHz.jpg\n-rw-r--r-- 1 root root 583K Aug 28 21:02 cmb_857GHz.fits\n-rw-r--r-- 1 root root  89K Aug 28 21:02 cmb_857GHz.jpg\n\n\n\nsky = pysm3.Sky(nside=128, preset_strings=[\"s5\"], output_unit=u.mK_RJ)\nsync = sky.get_emission(23 * u.GHz)[0]\n\n\n!rm -rf data/synch\n!mkdir data/synch\n\n\nfor freq in freqs:\n  m = sky.get_emission(freq)\n  hp.projview(m[0].value,  min=2e-4, max=2,\n            fontsize=fontsize, norm=\"log\", cmap=\"planck\",\n            unit=astropy.units.format.Latex.to_string(m.unit), title=f\"Synchrotron {int(freq.value)} {freq.unit}\");\n  filename = f\"data/synch/synch_{int(freq.value):03d}{freq.unit}\"\n  plt.savefig(f\"{filename}.jpg\", bbox_inches=\"tight\")\n  hp.write_map(f\"{filename}.fits\", m, dtype=np.float32, overwrite=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!rm -rf data/dust\n!mkdir data/dust\n\n\nsky = pysm3.Sky(nside=128, preset_strings=[\"d10\"], output_unit=u.mK_RJ)\n\n\nfor freq in freqs:\n  m = sky.get_emission(freq)\n  hp.projview(m[0].value,  min=1e-3, max=10,\n            fontsize=fontsize, norm=\"log\", cmap=\"inferno\",\n            unit=astropy.units.format.Latex.to_string(m.unit), title=f\"Thermal dust {int(freq.value)} {freq.unit}\");\n  filename = f\"data/dust/dust_{int(freq.value):03d}{freq.unit}\"\n  plt.savefig(f\"{filename}.jpg\", bbox_inches=\"tight\")\n  hp.write_map(f\"{filename}.fits\", m, dtype=np.float32, overwrite=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!wget https://github.com/ajvanengelen/webskylensing/raw/master/data/unlensed_scalar_cls.npy\n\n--2024-08-28 21:10:32--  https://github.com/ajvanengelen/webskylensing/raw/master/data/unlensed_scalar_cls.npy\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/ajvanengelen/webskylensing/master/data/unlensed_scalar_cls.npy [following]\n--2024-08-28 21:10:32--  https://raw.githubusercontent.com/ajvanengelen/webskylensing/master/data/unlensed_scalar_cls.npy\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 594200 (580K) [application/octet-stream]\nSaving to: ‘unlensed_scalar_cls.npy.2’\n\nunlensed_scalar_cls 100%[===================&gt;] 580.27K  --.-KB/s    in 0.07s   \n\n2024-08-28 21:10:32 (8.37 MB/s) - ‘unlensed_scalar_cls.npy.2’ saved [594200/594200]\n\n\n\n\ncl = np.load(\"unlensed_scalar_cls.npy\")\n\n\nimport pandas as pd\n\n\ncl_dataframe = pd.DataFrame({\"TT\":cl[0,0], \"TE\":cl[0,1], \"EE\":cl[1,1], \"BB\":cl[2,2]})\n\n\ncl_dataframe.index.name = \"$\\ell$\"\n\n\nell_norm = cl_dataframe.index * (cl_dataframe.index + 1) / 2 / np.pi\n\n\n# prompt: multiply each column of cl_dataframe by ell_norm\n\ncl_dataframe = cl_dataframe.multiply(ell_norm, axis=\"index\")\n\n\ncl_dataframe.to_csv(\"data/cmb/cls.csv\")\n\n\n!head cls.csv\n\n$\\ell$,TT,TE,EE,BB\n0,0.0,0.0,0.0,0.0\n1,0.0,0.0,0.0,0.0\n2,995.3154178522041,2.5678813147237656,0.03068555924377693,0.0\n3,940.797373475778,2.8805688992805507,0.039375529374249686,0.0\n4,888.9080373827597,2.7009377515312614,0.034154868839899724,0.0\n5,850.2545787469378,2.2994395560609573,0.022813455622875737,0.0\n6,823.8512963911593,1.8516067949264907,0.012761672561211228,0.0\n7,807.0245320756949,1.4541692613753545,0.006882574522918757,0.0\n8,796.8574845365462,1.149021628441058,0.004419579920058423,0.0\n\n\n\nimport matplotlib.pyplot as plt\ncl_dataframe.plot(grid=True)\nplt.show()"
  },
  {
    "objectID": "posts/2024-10-02-ssh-agent.html",
    "href": "posts/2024-10-02-ssh-agent.html",
    "title": "Configure ssh-agent on Linux",
    "section": "",
    "text": "I keep Googling this again and again.\nFirst, ssh-agent does not need to run on the remote server, it is the local agent which is forwarded over ssh to the remote server.\nOn the local machine, configure ssh-agent so that it starts only once, credit to this gist, add this to .bashrc:\n# SSH agent\nssh_pid_file=\"$HOME/.config/ssh-agent.pid\"\nSSH_AUTH_SOCK=\"$HOME/.config/ssh-agent.sock\"\nif [ -z \"$SSH_AGENT_PID\" ]\nthen\n    # no PID exported, try to get it from pidfile\n    SSH_AGENT_PID=$(cat \"$ssh_pid_file\")\nfi\n\nif ! kill -0 $SSH_AGENT_PID &&gt; /dev/null\nthen\n    # the agent is not running, start it\n    rm \"$SSH_AUTH_SOCK\" &&gt; /dev/null\n    &gt;&2 echo \"Starting SSH agent, since it's not running; this can take a moment\"\n    eval \"$(ssh-agent -s -a \"$SSH_AUTH_SOCK\")\"\n    echo \"$SSH_AGENT_PID\" &gt; \"$ssh_pid_file\"\n    ssh-add -A 2&gt;/dev/null\n\n    &gt;&2 echo \"Started ssh-agent with '$SSH_AUTH_SOCK'\"\n# else\n#   &gt;&2 echo \"ssh-agent on '$SSH_AUTH_SOCK' ($SSH_AGENT_PID)\"\nfi\nexport SSH_AGENT_PID\nexport SSH_AUTH_SOCK\nIf ssh-agent ever gets stuck:\nrm $HOME/.config/ssh-agent*\nIn case we want to load ssh keys at login, add also this to .bashrc (do just ssh-add to add all keys under .ssh):\nif ! ssh-add -l &&gt;/dev/null; then\n      echo Adding keys...\n      ssh-add ~/.ssh/id_rsa\nfi\nFinally, we can configure automatic forwarding to some of our SSH remote servers in .ssh/config:\nHost myserver\n    HostName myserver.mydomain.com\n    User myusername\n    ForwardAgent yes\nNow login to the remote server and check keys are properly forwarded running:\nssh-add -L"
  },
  {
    "objectID": "posts/2024-12-04-jetstream-windows.html",
    "href": "posts/2024-12-04-jetstream-windows.html",
    "title": "Run Windows and WSL on Jetstream",
    "section": "",
    "text": "Need access to a Windows machine? You can leverage Jetstream 2, spin up a Virtual Machine with Windows Server 2022 and access the Windows graphical desktop through your browser on any operating system.\nOn Exosphere Launch a m3.small Windows Server 2022 instance backed by a 40GB disk, it will stay forever in “Building” phase, not to worry.\nFollow the instructions on the Jetstream documentation for recovering the admin password (generally a SSH key is already in PEM format so no need to run the ssh-keygen command) and accessing the console via Horizon. You can access same console also via Exosphere using the “Console” button.\nClick on the “Send Ctrl-Alt-Delete” button to unlock the screen, login with the administrator password, unfortunately copy-paste does not work so you need to type the password.\nReboot to apply the security updates, otherwise some of the next steps can be blocked by the system.\nFrom the start menu choose “Windows Powershell”\nwsl --list --online\nMost probably you should be using the default Ubuntu 22.04 with:\nwsl --update\nwsl --install\nReboot\nUnfortunately I couldn’t find a way to copy-paste between my system and the machine, therefore the best is to use the Windows browser to access whatever is the necessary documentation, copy and then paste into WSL with the middle button of the mouse (right+left click on trackpad).\nIt is also possible to activate OpenSSH Server in Windows and SSH into the VM, however WSL does not work inside that terminal so that does not seem to be an option."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "",
    "text": "UPDATED 2025-04-14: Added echo test\nUPDATED 2025-04-12: Set a fixed IP address for the NGINX Ingress controller.\nThis guide demonstrates how to deploy Kubernetes on Jetstream with Magnum and then install JupyterHub on top using zero-to-jupyterhub. Jetstream recently enabled the Cluster API on its OpenStack deployment as the backend for Magnum, making it faster and more straightforward to launch Kubernetes clusters."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#advantages-of-magnum-based-deployments",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#advantages-of-magnum-based-deployments",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Advantages of Magnum-Based Deployments",
    "text": "Advantages of Magnum-Based Deployments\nMagnum-based clusters offer several benefits over Kubespray:\n\nFaster Deployment: Instead of using Ansible to configure each VM, Magnum uses pre-prepared images. Clusters typically deploy in about 10 minutes, and workers can scale up in around 5 minutes.\n\nLoad Balancer Integration: The OpenStack load balancer service provides easy support for multiple master nodes, ensuring high availability.\n\nAutoscaling: The Cluster Autoscaler can automatically add or remove worker nodes based on workload."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#prerequisites",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#prerequisites",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstall OpenStack and Magnum Clients:\npip install python-openstackclient python-magnumclient python-octaviaclient\n\nThe OpenStack client is used to create and manage the cluster, the Magnum client is used to create the cluster template, and the Octavia client is used to manage the load balancer.\nThis tutorial used OpenStack 6.1.0, python-magnumclient 4.7.0, and python-octaviaclient 3.3.0.\n\nCreate an Updated App Credential: Jetstream recently updated permissions, so even if you have an already working app credential, create another one ‘Unrestricted (dangerous)’ application credential with all permissions, including the “loadbalancer” permission, in the project where you will be creating the cluster, and source it to expose the environment variables in your local environment where you’ll be running the OpenStack commands.\nInstall Kubernetes Tooling: Once the cluster is launched, manage it using standard Kubernetes tools. Any recent version should work:\n\nkubectl: see https://kubernetes.io/docs/tasks/tools/, this tutorial used 1.26.\nhelm: see https://helm.sh/docs/intro/install/, this tutorial used 3.8.1."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#create-the-cluster-with-magnum",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Create the Cluster with Magnum",
    "text": "Create the Cluster with Magnum\nCheck out the cluster templates available:\nopenstack coe cluster template list\nList clusters (initially empty):\nopenstack coe cluster list\nAs usual, checkout the repository with all the configuration files on the machine you will use the Jetstream API from, typically your laptop:\ngit clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream\ncd jupyterhub-deploy-kubernetes-jetstream\ncd kubernetes_magnum\nA cluster can be created with:\nexport K8S_CLUSTER_NAME=k8s\nbash create_cluster.sh\nSee inside the file for the most commonly used parameters. The script awaits for the cluster to complete deployment successfully, which should take about 10 minutes.\nAt this point, decide if you prefer to use autoscaling or not. Autoscaling means that the cluster will automatically add (up to a predefined maximum) or remove worker nodes based on the load on Kubernetes. This is the recommended way to run JupyterHub, as it will automatically scale up and down based on the number of users and their activity. With manual scaling, you run a command to add or remove nodes. Scaling always refers to the worker nodes; the control plane cannot be scaled, so we recommend using 3 control plane nodes for redundancy.\nThe first time this is executed, and then again if not executed for a while, it will take a lot more time to deploy, between 2 and 2.5 hours, probably because the images are not cached in the OpenStack cloud. After that first execution, it should regularly deploy in 10 minutes.\nThe cluster consumes resources when active. It can be switched off with:\nbash delete_cluster.sh\nConsider this deletes all Jetstream virtual machines and data that could be stored in JupyterHub.\nOnce the status is CREATE_COMPLETE, get the Kubernetes config file in the current folder:\nopenstack coe cluster config $K8S_CLUSTER_NAME --force\nexport KUBECONFIG=$(pwd)/config\nchmod 600 config\nNow the usual kubectl commands should work:\n&gt; kubectl get nodes\nNAME                                          STATUS   ROLES           AGE     VERSION\nk8s-mbbffjfee7zs-control-plane-6rn2z          Ready    control-plane   2m22s   v1.30.4\nk8s-mbbffjfee7zs-control-plane-nw4cc          Ready    control-plane   41s     v1.30.4\nk8s-mbbffjfee7zs-control-plane-w9jln          Ready    control-plane   5m8s    v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-gvvjm   Ready    &lt;none&gt;          2m21s   v1.30.4"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#scale-manually",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#scale-manually",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Scale Manually",
    "text": "Scale Manually\nScaling manually only works if autoscaling is disabled.\nList the node groups:\nopenstack coe nodegroup list $K8S_CLUSTER_NAME\nIncrease the number of worker nodes:\nopenstack coe cluster resize --nodegroup default-worker $K8S_CLUSTER_NAME 3\nConfirm that there are 3 worker nodes:\nkubectl get nodes"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#enable-the-autoscaler",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#enable-the-autoscaler",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Enable the Autoscaler",
    "text": "Enable the Autoscaler\nEven if we specify a max node count of 5 in the create_cluster.sh script, this is not propagated to the nodegroup. See:\nopenstack coe nodegroup show $K8S_CLUSTER_NAME default-worker\nWhile min_node_count is set to 1, max_node_count is set to None, which means that the autoscaler is still disabled.\nWe can enable it by setting the max_node_count property on the default-worker nodegroup manually:\nopenstack coe nodegroup update $K8S_CLUSTER_NAME default-worker replace /max_node_count=5\nNow we can test the autoscaler with a simple deployment that uses 4 GB of memory for each replica:\nkubectl create -f high_mem_dep.yaml\nkubectl scale deployment high-memory-deployment --replicas 6\nIn the log of the pods, we can notice that this triggered the creation of more nodes:\nNormal   TriggeredScaleUp        113s  cluster-autoscaler  pod triggered scale-up: [{MachineDeployment/magnum-83bd0e70b4ba4cd092c2fb82b1ce06fb/k8s-mbbffjfee7zs-default-worker 2-&gt;5 (max: 5)}]\nAnd in fact, within a couple of minutes:\n&gt; kubectl get nodes\nNAME                                          STATUS   ROLES           AGE    VERSION\nk8s-mbbffjfee7zs-control-plane-6rn2z          Ready    control-plane   26m    v1.30.4\nk8s-mbbffjfee7zs-control-plane-nw4cc          Ready    control-plane   25m    v1.30.4\nk8s-mbbffjfee7zs-control-plane-w9jln          Ready    control-plane   29m    v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-cpsqx   Ready    &lt;none&gt;          94s    v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-dqww9   Ready    &lt;none&gt;          91s    v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-gvvjm   Ready    &lt;none&gt;          26m    v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-pmptm   Ready    &lt;none&gt;          5m4s   v1.30.4\nk8s-mbbffjfee7zs-default-worker-jb5lm-sss6b   Ready    &lt;none&gt;          97s    v1.30.4\nWe still have 1 pending pod, but the autoscaler has a limit of 5 workers, so it won’t launch other nodes:\n&gt; kubectl get pods\nNAME                                     READY   STATUS    RESTARTS   AGE\nhigh-memory-deployment-85785c87d-4zbrv   1/1     Running   0          4m5s\nhigh-memory-deployment-85785c87d-5xtvq   0/1     Pending   0          4m5s\nhigh-memory-deployment-85785c87d-6ds4c   1/1     Running   0          4m5s\nhigh-memory-deployment-85785c87d-qhclc   1/1     Running   0          4m5s\nhigh-memory-deployment-85785c87d-qxlf2   1/1     Running   0          4m5s\nhigh-memory-deployment-85785c87d-rk8sb   1/1     Running   0          5m8s"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#test-persistent-volumes",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#test-persistent-volumes",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Test Persistent Volumes",
    "text": "Test Persistent Volumes\nMagnum comes with a default storage class for persistent volumes which relies on OpenStack Cinder. We can test with a simple pod:\nkubectl create -f ../alpine-persistent-volume.yaml\nkubectl describe pod alpine"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#install-nginx-controller",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#install-nginx-controller",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Install NGINX Controller",
    "text": "Install NGINX Controller\nIn principle, we do not need an Ingress because the OpenStack Load Balancer can directly route traffic to JupyterHub. However, there is no way of getting an HTTPS certificate without an Ingress.\nInstall the NGINX Ingress controller with:\nhelm upgrade --install ingress-nginx ingress-nginx \\\n             --repo https://kubernetes.github.io/ingress-nginx \\\n             --namespace ingress-nginx --create-namespace\n\nUse a Fixed IP Address\nNotice that deploying the NGINX ingress controller will create a new OpenStack Load Balancer, which will be assigned a random IP address. This address will be lost when the NGINX ingress controller is deleted or the cluster is deleted.\nTo use a fixed IP address, first create a floating IP in OpenStack:\nopenstack floating ip create public\nexport IP=&lt;FLOATING_IP&gt;\nFind the ID of the NGINX Load Balancer:\nopenstack loadbalancer list\nexport LB=&lt;LOAD_BALANCER_ID&gt;\nFind the ID of the VIP port:\nLB_VIP_PORT_ID=$(openstack loadbalancer show $LB -c vip_port_id -f value)\nFind the existing IP:\nEXISTING_FIP=$(openstack floating ip list --port $LB_VIP_PORT_ID -c ID -f value)\nRemove it:\nopenstack floating ip unset --port $EXISTING_FIP\nAnd associate the new floating IP:\nopenstack floating ip set --port $LB_VIP_PORT_ID $IP"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#configure-a-subdomain",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#configure-a-subdomain",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Configure a Subdomain",
    "text": "Configure a Subdomain\nIn case you do not have access to a custom domain, you can use the Jetstream subdomain for your project. The subdomain is available to all projects on Jetstream, and it is a good way to test your deployment without having to configure a custom domain.\nJetstream provides subdomains to each project as:\nxxxxxx.$PROJ.projects.jetstream-cloud.org\nwhere PROJ is the ID of your Jetstream 2 allocation (all lowercase):\nexport PROJ=\"xxx000000\"\nFirst, get the public IP of the NGINX ingress controller:\nexport IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\nIf you have a custom subdomain, you can configure an A record that points to the EXTERNAL-IP of the service. Otherwise, use OpenStack to create a record:\nexport SUBDOMAIN=\"k8s\"\nopenstack recordset create  $PROJ.projects.jetstream-cloud.org. $SUBDOMAIN --type A --record $IP --ttl 3600\nAccess JupyterHub at https://k8s.$PROJ.projects.jetstream-cloud.org."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#test-the-nginx-ingress-controller",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#test-the-nginx-ingress-controller",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Test the NGINX Ingress Controller",
    "text": "Test the NGINX Ingress Controller\nWe have a deployment of a simple toy application to test that Kubernetes, the NGINX ingress, and the domain are working correctly.\nFirst, associate a subdomain to the IP address of the NGINX ingress controller:\nexport SUBDOMAIN=\"testpage\"\nopenstack recordset create $PROJ.projects.jetstream-cloud.org. $SUBDOMAIN --type A --record $IP --ttl 3600\nkubectl create -f echo-test.yaml\nNow you should be able to connect to:\nhttp://testpage.$PROJ.projects.jetstream-cloud.org\nIf everything is working properly, you should see “Testing NGINX Ingress on Jetstream!” in the browser."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#install-jupyterhub",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Install JupyterHub",
    "text": "Install JupyterHub\nFinally, we can go back to the root of the repository and install JupyterHub. First, create the secrets file:\nbash create_secrets.sh\nThe default secrets.yaml file assumes you are deploying on a projects.jetstream-cloud.org subdomain. If that is not the case, edit the file with your own domain.\nbash configure_helm_jupyterhub.sh\nbash install_jhub.sh"
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#setup-https",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#setup-https",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Setup HTTPS",
    "text": "Setup HTTPS\nFinally, to get a valid certificate, deploy cert-manager with:\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.2/cert-manager.yaml\nand a Cluster Issuer:\nkubectl create -f ../setup_https/https_cluster_issuer.yml\nNow your deployment should have a valid HTTPS certificate."
  },
  {
    "objectID": "posts/2024-12-11-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "href": "posts/2024-12-11-jetstream_kubernetes_magnum.html#issues-and-feedback",
    "title": "Deploy Kubernetes and JupyterHub on Jetstream with Magnum and Cluster API",
    "section": "Issues and Feedback",
    "text": "Issues and Feedback\nPlease open an issue on the repository to report any issue or give feedback."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a computational scientist the San Diego Supercomputer Center, California, USA.\nI mostly work on distributed data processing in Python of Astrophysics/Cosmology datasets. I simulate and analyze data from Cosmic Microwave Background experiments, I got my Ph.D. at the University of Milano, Italy, working on the Planck satellite (European Space Agency). See my publications on Google Scholar or Arxiv.\nCurrently I also work on cloud deployments of interactive computing platforms based on JupyterHub on top of Kubernetes.\nI am available for consulting, code-review, software infrastructure design.\nContact me via Twitter or email (andrea on the domain andreazonca.com).\n\nThis website is powered by quarto."
  }
]